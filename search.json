[
  {
    "objectID": "application_procedure.html",
    "href": "application_procedure.html",
    "title": "Application Procedure",
    "section": "",
    "text": "We are currently finalising the application procedure for 2026 entry into the CDT. Applications are expected to open mid November 2025."
  },
  {
    "objectID": "blog/blog_list.html",
    "href": "blog/blog_list.html",
    "title": "CCMI Blog",
    "section": "",
    "text": "Posts\n\n\n        \n            \n            \n                Nobel Prizes in Chemistry and Physics for AI research\n                \n                The 2024 nobel prizes in Physics and Chemistry were given out to breakthrough research in AI from members of the UCL community.\n            \n                \n        \n\n        \n            \n            \n                CCMI is about to start\n                \n                The first CCMI cohort is about to arrive.\n            \n                \n        \n\n        \n            \n            \n                CCMI - How we got here\n                \n                How CCMI got from a vague idea to an existing centre.\n            \n                \n        \n\n        \n            \n            \n                To be or not to be in the office\n                \n                A few thoughts on remote vs in-office working in a CDT.\n            \n                \n        \n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/ccmi_the_history.html",
    "href": "blog/posts/ccmi_the_history.html",
    "title": "CCMI - How we got here",
    "section": "",
    "text": "It is finally done. Our CDT has started. I’d like to explain a little bit of the history of this CDT.\nMy first attempt to bring a CDT to UCL was in 2018 on a Scientific Computing focused CDT. The proposal went all the way to the final interviews. Yet, at the end it was not funded. A typical CDT application is about a year of work. So any rejection is a major disappointment. But on the other hand it was a very good learning experience. In long post mortems I realised why we were not funded and I pledged to myself to try again.\nIn 2022 Colin Cotter from Imperial and I started talking about the next upcoming application round for Doctoral Training Centres, and we decided that we would have a much stronger case by submitting a joint UCL/Imperial proposal.\nBut what would the unique selling point of this CDT be? Both, Colin and I are passionate about research software. At UCL we also have with ARC one of the UK’s foremost centres for Research Software Engineering. This was pivotal. It would give us the credibility and resources to build a CDT proposal with world leading research software engineering at its heart.\nBefore a CDT proposal can go to council there is a lot of work. We had to go through tough internal UCL competition before we were even allowed to apply to EPSRC. Application numbers per university are limited and competition is fierce.\nAfter surviving the internal selection procedure we had to write an outline application for EPSRC. While formally this is just a short outline, partners already need to be contacted and costing be done in the outline stage. Moreover, the core idea of the CDT needs to be there and presented.\nAgain, the competition is tough in the outline stage and a lot of excellent proposals do not make it on. Finally, once we made it to the main stage the big proposal needed to be written, costings finalised, details of the training programme worked out and partner letters organised. This took several months of work.\nBut this is not the end. After receiving external referee reports, applicants are invited to an interview. One day in autumn 2023 myself, Colin, and our Co-Director Ruth Misener from Imperial were invited to come to Bristol for an in-depth interview.\nThis was it, the culmination of over a year of work. Our nerves were bare. But we managed to keep ourselves calm enough and somehow everything went well.\nFinally, in early 2024 the results were officially announced to the world. While we originally thought we would start in Autumn 2024 it turned out that we were one of a number of CDTs that were asked by EPSRC to delay start by one year.\nInitially, this seemed disappointing. After all this work we were keen to see students arriving. But it turned out a blessing in disguise. We had time to hire excellent CDT Managers (Laura Beer at UCL and Lydia Noha at Imperial). We could carefully plan the training programme and did not have to rush recruitment.\nNow it is October 2025 and it finally happened. Our students have arrived and CCMI has officially started. It was a long journey but probably the single most satisfying one in my career so far.\nSuch a journey cannot be made alone. A CDT is a huge team effort. I have already mentioned Colin, Ruth, Laura, and Lydia. But we are a much larger team behind this CDT, and I would like to take the opportunity to thank all of them for the great work to get CCMI from a vague idea to an actually existing centre.\nThanks again to Colin Cotter, Dante Kalise, Ruth Misener, Vahid Shahrezaei and Lydia Noah at Imperial and Marta Betcke, Serge Guillas, Hao Ni, Laura Beer, Matthew Scroggs and the ARC Operations Team at UCL. We have made it.\nFinally, to conclude this post I would also especially like to thank all our new students who have given us their trust to spend the next four years doing research and training with us at UCL and Imperial. Over the next few weeks we will build up homepage profiles for each one of them and introduce these remarkable researchers in separate posts."
  },
  {
    "objectID": "blog/posts/nobel_prizes.html",
    "href": "blog/posts/nobel_prizes.html",
    "title": "Nobel Prizes in Chemistry and Physics for AI research",
    "section": "",
    "text": "London is the world center in AI research. This has been impressively demonstrated this year by the award of two Nobel Prizes to members of the UCL Community. Professor Geoffrey Hinton, founder of the Gatsby Computational Neuroscience Unit at UCL, was awarded the Physics Nobel Prize alongside John J Hopfield for breakthroughs in AI research.\nSir Demis Hassabis the Co-Founder and CEO of Google Deepmind, who completed his PhD at UCL and was a postdoc in the Gatsby Neuroscience Unit, was awarded the Nobel Prize in Chemistry along with John Jumper and Professor David Baker for their work on computational protein design and protein structure prediction.\nRead the UCL News Article for the full story of these remarkable researchers."
  },
  {
    "objectID": "apply.html",
    "href": "apply.html",
    "title": "How to apply",
    "section": "",
    "text": "UCL Japanese Garden\n\n\nWe are currently updating our application system for 2026 entry. Applications will open around mid November together with detailed information on how to apply for the academic year 2026/2027."
  },
  {
    "objectID": "phd_projects/entries/Parpas_moleculardynamics.html",
    "href": "phd_projects/entries/Parpas_moleculardynamics.html",
    "title": "Efficient Computation of Transition States in Molecular Dynamics",
    "section": "",
    "text": "Transition states govern the outcomes of molecular reactions. Therefore, determining the transition states of molecules is a fundamental challenge encountered across diverse fields, including chemical physics, biology, material science, and chemical engineering. However, current numerical methods fall short of what is needed in practice because transition states are rare events within a high-dimensional stochastic dynamical system. As a result, the simulation of rare events is generally intractable and requires domain-specific knowledge of the underlying molecular system or extensive and time consuming trial-and-error computations.\n\n\n\nThe theoretical foundations of our methodology are provided by the remarkable fact that, under appropriate technical conditions, the eigenforms of a particular type of operator, known as the Witten Laplacian, concentrate on transition states. However, there is a catch: solving the associated Witten PDE in high dimensions is currently impractical. The objective of this project is to take of advantage of recent developments that showed how to represent the Witten PDE as a system of stochastic differential equations.[1] This opens the door to new numerical methods utilizing state-of-the-art stochastic simulation and machine learning methods. These advancements are especially promising for harnessing the power of massively parallel computing.\nThere is a deep connection between the stochastic dynamics in used to understand saddle points and gradient flows. Associated with the stochastic representation in [1] there is a partial differential equation, the Fokker-Planck equation, that describes the evolution of the probability density of the stochastic dynamics. In a seminal paper, the authors in [2] made a connection between gradient flows and the Fokker-Planck equation. Moreover, their proof is constructive and based on an implicit discretization scheme that is known as the JKO scheme. Recent works have shown how to leverage the connection between the Fokker-Planck equation and gradient flows to develop numerical methods to solve for the probability density function of the stochastic representation directly. Unfortunately, the implementation of the JKO scheme requires the solution of a large optimization problem at each iteration. To address this challenge, you will study advanced distributed optimization methods that can scale to large dimensions. A more speculative direction of research is to explore whether the Witten PDE can also be written as a gradient flow in an appropriate space. If successful, this direction will lead to a new class of algorithms for computing transition states.\n[1] T. Lelièvre, P. Parpas. Using Witten Laplacians to locate index-1 saddle points SIAM Journal on Scientific Computing, to appear, 2023. [2] Jordan R, Kinderlehrer D, Otto F. The variational formulation of the Fokker–Planck equation. SIAM journal on mathematical analysis. 1998 Jan;29(1):1-7.\n\n\n\nIt is expected that you will develop an open-source software library that can be used by the wider community. Initially, the focus will be on benchmark problems but the hope is that you will eventually be able to simulate realistic systems. You will build on an initial implementation available here: https://github.com/pp500/Stochastic-Saddle-Point-Dynamics"
  },
  {
    "objectID": "phd_projects/entries/Parpas_moleculardynamics.html#project-description",
    "href": "phd_projects/entries/Parpas_moleculardynamics.html#project-description",
    "title": "Efficient Computation of Transition States in Molecular Dynamics",
    "section": "",
    "text": "Transition states govern the outcomes of molecular reactions. Therefore, determining the transition states of molecules is a fundamental challenge encountered across diverse fields, including chemical physics, biology, material science, and chemical engineering. However, current numerical methods fall short of what is needed in practice because transition states are rare events within a high-dimensional stochastic dynamical system. As a result, the simulation of rare events is generally intractable and requires domain-specific knowledge of the underlying molecular system or extensive and time consuming trial-and-error computations.\n\n\n\nThe theoretical foundations of our methodology are provided by the remarkable fact that, under appropriate technical conditions, the eigenforms of a particular type of operator, known as the Witten Laplacian, concentrate on transition states. However, there is a catch: solving the associated Witten PDE in high dimensions is currently impractical. The objective of this project is to take of advantage of recent developments that showed how to represent the Witten PDE as a system of stochastic differential equations.[1] This opens the door to new numerical methods utilizing state-of-the-art stochastic simulation and machine learning methods. These advancements are especially promising for harnessing the power of massively parallel computing.\nThere is a deep connection between the stochastic dynamics in used to understand saddle points and gradient flows. Associated with the stochastic representation in [1] there is a partial differential equation, the Fokker-Planck equation, that describes the evolution of the probability density of the stochastic dynamics. In a seminal paper, the authors in [2] made a connection between gradient flows and the Fokker-Planck equation. Moreover, their proof is constructive and based on an implicit discretization scheme that is known as the JKO scheme. Recent works have shown how to leverage the connection between the Fokker-Planck equation and gradient flows to develop numerical methods to solve for the probability density function of the stochastic representation directly. Unfortunately, the implementation of the JKO scheme requires the solution of a large optimization problem at each iteration. To address this challenge, you will study advanced distributed optimization methods that can scale to large dimensions. A more speculative direction of research is to explore whether the Witten PDE can also be written as a gradient flow in an appropriate space. If successful, this direction will lead to a new class of algorithms for computing transition states.\n[1] T. Lelièvre, P. Parpas. Using Witten Laplacians to locate index-1 saddle points SIAM Journal on Scientific Computing, to appear, 2023. [2] Jordan R, Kinderlehrer D, Otto F. The variational formulation of the Fokker–Planck equation. SIAM journal on mathematical analysis. 1998 Jan;29(1):1-7.\n\n\n\nIt is expected that you will develop an open-source software library that can be used by the wider community. Initially, the focus will be on benchmark problems but the hope is that you will eventually be able to simulate realistic systems. You will build on an initial implementation available here: https://github.com/pp500/Stochastic-Saddle-Point-Dynamics"
  },
  {
    "objectID": "phd_projects/entries/casale_lupu.html",
    "href": "phd_projects/entries/casale_lupu.html",
    "title": "Mathematical and Computational Modeling of Resilience in Multilayer Networks",
    "section": "",
    "text": "The investigators work both in the Department of Computing at Imperial College London and have extensive expertise in the research topics that underpin the project. They have a running collaboration in the area of resilience, which intersects with their expertise in modelling enterprise systems and networks. Recent works relevant to the project include modelling and simulation of enterprise networks under cyber attacks (https://ieeexplore.ieee.org/document/9713988); mean-field ODE models for modelling networks evolving in randomly environments (https://ieeexplore.ieee.org/abstract/document/7843645); variational inference methods for inference of parameters in routing models (https://www.cambridge.org/core/journals/advances-in-applied-probability/article/abs/variational-inference-for-markovian-queueing-networks/D35E7DB62BE78D883730A04E617C3DB3); game theoretic analysis of stochastic agents modelling urban mobility (https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2462.pdf; a journal version is under submission). We have also built a software toolchain for multi-layer networks that will provide a software basis to this project.\n\n\n\nThe PhD project will develop advanced mathematical and computational models for studying the resilience of networks of networks. Specifically, the project will focus on the multilayer network formalism (MLN), which is used to describe networks consisting of multiple interconnected layers, each representing different types of interactions or relationships between nodes. MLNs are used to model systems where entities participate in various types of networks simultaneously, such as communication networks, social networks, and transportation systems, allowing for the analysis of interactions across different dimensions and their collective impact on the system’s dynamics. Applications of MLNs include understanding information diffusion, analyzing infrastructure resilience, optimizing network traffic, and studying the spread of diseases or cyber threats across interconnected systems.\nThis project focuses on the mathematical foundations of MLNs, exploring their dynamic properties and resilience to perturbations using advanced techniques such as Neural Ordinary Differential Equations (Neural ODEs) to model diffusion processes across network layers. A key aspect of the research will involve creating efficient analytical frameworks to capture the behavior of multimodal information flows and cascades within these networks. The aim is to understand how multilayered structures behave under various stresses, particularly in scenarios involving abrupt changes to the network structure (e.g., sudden failures, cyberattacks, etc.), developing new resilience metrics to quantify robustness. Although the project has a modelling focus, applications to mobility systems and network security (eg network segmentation) will be explored as use cases.\n\n\n\nSeveral public data sources are available to concretely assess the toolchain, we presently work with mobility datasets from Oslo and Austin and security data from enterprise networks. The focus of the deliverables will therefore be on open source software tools:\n\nD1: Large-scale MLN simulation tool, Year 1\nD2: Diffusion models and resilience metrics for MLN analysis, Year 2\nD3: Use cases software applications in collaboration with external partners and companies, Year 3"
  },
  {
    "objectID": "phd_projects/entries/casale_lupu.html#project-description",
    "href": "phd_projects/entries/casale_lupu.html#project-description",
    "title": "Mathematical and Computational Modeling of Resilience in Multilayer Networks",
    "section": "",
    "text": "The investigators work both in the Department of Computing at Imperial College London and have extensive expertise in the research topics that underpin the project. They have a running collaboration in the area of resilience, which intersects with their expertise in modelling enterprise systems and networks. Recent works relevant to the project include modelling and simulation of enterprise networks under cyber attacks (https://ieeexplore.ieee.org/document/9713988); mean-field ODE models for modelling networks evolving in randomly environments (https://ieeexplore.ieee.org/abstract/document/7843645); variational inference methods for inference of parameters in routing models (https://www.cambridge.org/core/journals/advances-in-applied-probability/article/abs/variational-inference-for-markovian-queueing-networks/D35E7DB62BE78D883730A04E617C3DB3); game theoretic analysis of stochastic agents modelling urban mobility (https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2462.pdf; a journal version is under submission). We have also built a software toolchain for multi-layer networks that will provide a software basis to this project.\n\n\n\nThe PhD project will develop advanced mathematical and computational models for studying the resilience of networks of networks. Specifically, the project will focus on the multilayer network formalism (MLN), which is used to describe networks consisting of multiple interconnected layers, each representing different types of interactions or relationships between nodes. MLNs are used to model systems where entities participate in various types of networks simultaneously, such as communication networks, social networks, and transportation systems, allowing for the analysis of interactions across different dimensions and their collective impact on the system’s dynamics. Applications of MLNs include understanding information diffusion, analyzing infrastructure resilience, optimizing network traffic, and studying the spread of diseases or cyber threats across interconnected systems.\nThis project focuses on the mathematical foundations of MLNs, exploring their dynamic properties and resilience to perturbations using advanced techniques such as Neural Ordinary Differential Equations (Neural ODEs) to model diffusion processes across network layers. A key aspect of the research will involve creating efficient analytical frameworks to capture the behavior of multimodal information flows and cascades within these networks. The aim is to understand how multilayered structures behave under various stresses, particularly in scenarios involving abrupt changes to the network structure (e.g., sudden failures, cyberattacks, etc.), developing new resilience metrics to quantify robustness. Although the project has a modelling focus, applications to mobility systems and network security (eg network segmentation) will be explored as use cases.\n\n\n\nSeveral public data sources are available to concretely assess the toolchain, we presently work with mobility datasets from Oslo and Austin and security data from enterprise networks. The focus of the deliverables will therefore be on open source software tools:\n\nD1: Large-scale MLN simulation tool, Year 1\nD2: Diffusion models and resilience metrics for MLN analysis, Year 2\nD3: Use cases software applications in collaboration with external partners and companies, Year 3"
  },
  {
    "objectID": "phd_projects/entries/Boulle_Koopman.html",
    "href": "phd_projects/entries/Boulle_Koopman.html",
    "title": "Koopman operator learning of nonlinear PDEs",
    "section": "",
    "text": "The aim of this project is to learn nonlinear partial differential equations (PDEs) from data using techniques based on Koopman operators and dynamic mode decomposition.\n\n\nPDE learning, which involves discovering the underlying partial differential equations governing complex physical systems from data, has become an increasingly active research area in scientific machine learning, primarily relying on neural network approximation algorithms. These methods leverage the representational flexibility of neural networks to approximate complex functions and have shown promising results in various applications [1]. However, extracting physical insights from these learned models and establishing rigorous theoretical guarantees remains challenging due to the highly nonlinear nature of neural networks. A major open problem in this field is the development of tools and theories for nonlinear PDEs.\nOn the other hand, Koopman operator theory presents an alternative approach to analysing nonlinear dynamical systems. The Koopman operator provides a global, linear representation of a nonlinear system by acting on an infinite-dimensional space of observables. This linearization makes it possible to analyse nonlinear systems using tools traditionally reserved for linear systems, such as spectral analysis and operator theory techniques [2,3]. The Koopman framework has shown significant potential in applications such as control theory, fluid dynamics, and signal processing.\nHowever, the Koopman operator framework has been developed predominantly for discrete-time dynamical systems and ordinary differential equations (ODEs). Its extension to continuous-time systems, and in particular to nonlinear PDEs, remains an open and underexplored area of research. The challenge lies in bridging the gap between the theoretical foundation of Koopman operators and their practical application to the highly complex dynamics of PDEs. Addressing this gap will unlock new possibilities for analysing, controlling, and predicting the behaviour of nonlinear PDE systems through a data-driven, operator-theoretic lens.\n[1] N. Boulle and A. Townsend, “A Mathematical Guide to Operator Learning”, https://arxiv.org/abs/2312.14688. [2] M. J. Colbrook, “The Multiverse of Dynamic Mode Decomposition Algorithms”, https://arxiv.org/abs/2312.00137. [3] S. L. Brunton, M. Budisic, E. Kaiser, J. N. Kutz, “Modern Koopman Theory for Dynamical Systems”, https://doi.org/10.1137/21M1401243=\n\n\n\nThis project aims to achieve the following objectives: 1. Generalizing the Koopman operator framework to infinitely large system of ODEs and nonlinear PDEs 2. Developing the first theory for nonlinear PDE learning based on Koopman operators 3. Implementing new algorithms for approximating Koopman operators and applying them to fluid dynamics problems\n\n\n\nA large component of this project will involve the implementation of new data-driven algorithms for learning and computing spectral properties associated with PDEs. The resulting software will be made publicly available in the form of a package with a detailed documentation to facilitate its adoption among the scientific machine learning community."
  },
  {
    "objectID": "phd_projects/entries/Boulle_Koopman.html#project-description",
    "href": "phd_projects/entries/Boulle_Koopman.html#project-description",
    "title": "Koopman operator learning of nonlinear PDEs",
    "section": "",
    "text": "The aim of this project is to learn nonlinear partial differential equations (PDEs) from data using techniques based on Koopman operators and dynamic mode decomposition.\n\n\nPDE learning, which involves discovering the underlying partial differential equations governing complex physical systems from data, has become an increasingly active research area in scientific machine learning, primarily relying on neural network approximation algorithms. These methods leverage the representational flexibility of neural networks to approximate complex functions and have shown promising results in various applications [1]. However, extracting physical insights from these learned models and establishing rigorous theoretical guarantees remains challenging due to the highly nonlinear nature of neural networks. A major open problem in this field is the development of tools and theories for nonlinear PDEs.\nOn the other hand, Koopman operator theory presents an alternative approach to analysing nonlinear dynamical systems. The Koopman operator provides a global, linear representation of a nonlinear system by acting on an infinite-dimensional space of observables. This linearization makes it possible to analyse nonlinear systems using tools traditionally reserved for linear systems, such as spectral analysis and operator theory techniques [2,3]. The Koopman framework has shown significant potential in applications such as control theory, fluid dynamics, and signal processing.\nHowever, the Koopman operator framework has been developed predominantly for discrete-time dynamical systems and ordinary differential equations (ODEs). Its extension to continuous-time systems, and in particular to nonlinear PDEs, remains an open and underexplored area of research. The challenge lies in bridging the gap between the theoretical foundation of Koopman operators and their practical application to the highly complex dynamics of PDEs. Addressing this gap will unlock new possibilities for analysing, controlling, and predicting the behaviour of nonlinear PDE systems through a data-driven, operator-theoretic lens.\n[1] N. Boulle and A. Townsend, “A Mathematical Guide to Operator Learning”, https://arxiv.org/abs/2312.14688. [2] M. J. Colbrook, “The Multiverse of Dynamic Mode Decomposition Algorithms”, https://arxiv.org/abs/2312.00137. [3] S. L. Brunton, M. Budisic, E. Kaiser, J. N. Kutz, “Modern Koopman Theory for Dynamical Systems”, https://doi.org/10.1137/21M1401243=\n\n\n\nThis project aims to achieve the following objectives: 1. Generalizing the Koopman operator framework to infinitely large system of ODEs and nonlinear PDEs 2. Developing the first theory for nonlinear PDE learning based on Koopman operators 3. Implementing new algorithms for approximating Koopman operators and applying them to fluid dynamics problems\n\n\n\nA large component of this project will involve the implementation of new data-driven algorithms for learning and computing spectral properties associated with PDEs. The resulting software will be made publicly available in the form of a package with a detailed documentation to facilitate its adoption among the scientific machine learning community."
  },
  {
    "objectID": "phd_projects/entries/Pearce_bacterialbiofilms.html",
    "href": "phd_projects/entries/Pearce_bacterialbiofilms.html",
    "title": "Data-driven multi-scale modelling of bacterial biofilms",
    "section": "",
    "text": "Bacterial biofilms are communities of bacterial cells embedded in extracellular matrix, and are the most common form of bacterial life. Numerous software packages exist for performing cell-based simulation of bacterial biofilms and synthetic bacterial communities (e.g. iDynoMiCS, BSim, gro). However, such software does not typically allow for flexibility in the properties of extracellular matrix proteins, which we have found in previous work to provide both structural integrity (Hartmann et al., Nat. Phys., 2019; Pearce et al., Phys. Rev. Lett. 2019) and protection from antibiotics (Böhning et al., Nat. Comm., 2023) in bacterial biofilms. There is therefore a crucial gap in the software landscape, which limits our ability to understand and test fundamentally how bacterial communities respond to antibiotic treatment in a range of conditions. Recent experimental data generated by our collaborators has revealed the structure, chemical properties and spatial arrangement of the molecules that make up the extracellular matrix in biofilms, providing an opportunity to close this gap through data-driven coarse-grained models and codes for biomolecular dynamics within bacterial biofilms.\n\n\n\nIn this project, we will develop a computational workflow to take in molecular-level detail about matrix proteins and output coarse-grained models of bacterial biofilms that account for matrix properties. We will realise the following objectives:\n\nDevelop a workflow to take in experimental data on the molecular structure and properties of matrix proteins and output coarse-grained molecular properties for use in cell-based simulations, validated where possible by mesoscopic experimental data such as phase diagrams.\nPerform simulations of bacterial biofilms including such extracellular matrix proteins and explore their effect on antitbiotic treatment, validated by data on molecular organisation and antibiotic transport in biofilms.\nIntegrate the workflow and code into new and/or existing software so that molecular data can be quickly converted into new coarse-grained particles in cell-based models.\n\n\n\n\n\nSoftware to convert molecular data to coarse-grained models.\nSoftware to include the microscopic properties of matrix proteins in cell-based models."
  },
  {
    "objectID": "phd_projects/entries/Pearce_bacterialbiofilms.html#project-description",
    "href": "phd_projects/entries/Pearce_bacterialbiofilms.html#project-description",
    "title": "Data-driven multi-scale modelling of bacterial biofilms",
    "section": "",
    "text": "Bacterial biofilms are communities of bacterial cells embedded in extracellular matrix, and are the most common form of bacterial life. Numerous software packages exist for performing cell-based simulation of bacterial biofilms and synthetic bacterial communities (e.g. iDynoMiCS, BSim, gro). However, such software does not typically allow for flexibility in the properties of extracellular matrix proteins, which we have found in previous work to provide both structural integrity (Hartmann et al., Nat. Phys., 2019; Pearce et al., Phys. Rev. Lett. 2019) and protection from antibiotics (Böhning et al., Nat. Comm., 2023) in bacterial biofilms. There is therefore a crucial gap in the software landscape, which limits our ability to understand and test fundamentally how bacterial communities respond to antibiotic treatment in a range of conditions. Recent experimental data generated by our collaborators has revealed the structure, chemical properties and spatial arrangement of the molecules that make up the extracellular matrix in biofilms, providing an opportunity to close this gap through data-driven coarse-grained models and codes for biomolecular dynamics within bacterial biofilms.\n\n\n\nIn this project, we will develop a computational workflow to take in molecular-level detail about matrix proteins and output coarse-grained models of bacterial biofilms that account for matrix properties. We will realise the following objectives:\n\nDevelop a workflow to take in experimental data on the molecular structure and properties of matrix proteins and output coarse-grained molecular properties for use in cell-based simulations, validated where possible by mesoscopic experimental data such as phase diagrams.\nPerform simulations of bacterial biofilms including such extracellular matrix proteins and explore their effect on antitbiotic treatment, validated by data on molecular organisation and antibiotic transport in biofilms.\nIntegrate the workflow and code into new and/or existing software so that molecular data can be quickly converted into new coarse-grained particles in cell-based models.\n\n\n\n\n\nSoftware to convert molecular data to coarse-grained models.\nSoftware to include the microscopic properties of matrix proteins in cell-based models."
  },
  {
    "objectID": "phd_projects/entries/Burman_heterogeneouscoupling.html",
    "href": "phd_projects/entries/Burman_heterogeneouscoupling.html",
    "title": "A computational framework for heterogeneous coupling in large scale computations",
    "section": "",
    "text": "With the advent of high performance computing and new computer architectures there is renewed interest in the efficient coupling of different partial differential equation based models and their associated solvers. Indeed, it is often the case that for a given application or computational methods there exists a highly optimized solver. However in complex applications many different models are connected and it is then attractive to connect such optimized solvers for the subproblem to obtain a solution of the global problem. The common approach in this framework is to discretize the global continuous problem into a, possibly huge, algebraic system, and then split the latter as a system of coupled algebraic subsystems. In this case the coupling condition often consists in the identification of unknowns in the subsystems corresponding to the same global unknown. This approach forms the basis of the dominating coupling softwares available today (see for example [3,4]).\nAn alternative approach, which is lately gaining increasing interest, is, instead, to decompose the original problem already at the continuous/infinite dimensional level, thus obtaining a system of coupled infinite dimensional subproblems, to be successively discretized. The advantage of the latter approach compared to the former is that it allows for a detailed mathematical analysis of the resulting couplings under precise conditions on the local solvers. Moreover, it allows for the design of optimal preconditioners of the coupling method. An abstract framework for heterogeneous coupling including stability and error analysis as well as the design of preconditioners was recently proposed by the principal supervisor in [1]. It is shown that a variety of different couplings enter the framework including interface coupling, volume coupling, FEM-BEM coupling, bulk-surface pde systems, or networks of fractures. This theoretical work was inspired by the principal supervisors extensive work on coupling methods for heterogeneous interface problems [3], fluid-structure or fluid-fluid interaction [4,5], or FEM- BEM coupling [2,6].\n\n\n\nDescribe the main objectives of the project.\nThe main objective of the project is to further develop the ideas of [1] along the following lines:\n\nTheory. The examples of [1] are restricted to scalar problems. Here the objective of the thesis would be to show how the framework can be applied to systems, with special focus on the linearized Navier-Stokes’ equations of fluid mechanics and the Maxwell’s equations of electro magnetics. For these cases a complete analysis will be derived including stability and error analysis. Preconditioners for the coupling will also be designed. If time allows more heterogeneous systems will then be considered, such as for example free flow coupled to fractured porous media or magneto-hydrodynamics. Another more adventurous potential research strand is to explore using network approximation to learn optimal coupling spaces satisfying the conditions of [1].\nComputational aspects. The framework will then be realised in a computational software where the merits of different coupling conditions or formulations can be assessed. Here special care will be taken to make the computational software agnostic (to the furthest possible extent) to the solvers of the subproblems in the spirit of [1].\n\n\n\n\nThe software deliverable should form a significant component of the PhD project. It must be a substantial piece of software that can disseminated to the wider community. For the details of the software deliverable please describe existing software (if available), community need for the software, licensing and technical aspects (programming language, target platforms, etc.). The license by default should be an open-source license. For other types of licenses please discuss with the CDT leadership team.\nThe objective is to design and implement a software library allowing for heterogeneous coupling of different partial differential equations and their associated methods. This library would allow for interface coupling, volume coupling, mixed dimensional coupling. Assuming that the different solvers of the subproblems are optimized the library will handle the preconditioning of the coupling using the general FETI type preconditioner designed and analysed in [1]. Contrary to the libraries of [7,8] this library would have a solid mathematical foundation that will allow for a highly optimized and reliable computational software.\n[1] Bertoluzza, Silvia ; Burman, Erik. An abstract framework for heterogeneous coupling: stability, approximation and applications. arXiv:2312.11733, 2023. [2] Betcke, Timo ; Bosy, Michał ; Burman, Erik . Hybrid coupling of finite element and boundary element methods using Nitsche’s method and the Calderon projection. Numer. Algorithms 91 (2022), no. 3, 997–1019. [3] Burman, Erik ; Elfverson, Daniel ; Hansbo, Peter ; Larson, Mats G. ; Larsson, Karl . Hybridized CutFEM for elliptic interface problems. SIAM J. Sci. Comput. 41 (2019), no. 5, A3354–A3380. [4] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel A. ; Guzmán, Johnny . Fully discrete loosely coupled Robin-Robin scheme for incompressible fluid-structure interaction: stability and error analysis. Numer. Math. 151 (2022), no. 4, 807–840. [5] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel ; Guzmán, Johnny . Loosely coupled, non- iterative time-splitting scheme based on Robin-Robin coupling: unified analysis for parabolic/parabolic and parabolic/hyperbolic problems. J. Numer. Math. 31 (2023), no. 1, 59–77. [6] Bosy, Michal; Scroggs, Matthew W.; Betcke, Timo; Burman, Erik ; Cooper, Christopher D. Coupling finite and boundary element methods to solve the Poisson–Boltzmann equation for electrostatics in molecular solvation. Journal of Computational Chemistry. https://doi.org/10.1002/jcc.27262, 2023. [7] Bungartz, Hans-Joachim ; Lindner, Florian ; Gatzhammer, Bernhard ; Mehl, Miriam ; Scheufele, Klaudius ; Shukaev, Alexander ; Uekermann, Benjamin . preCICE—a fully parallel library for multi- physics surface coupling. Comput. & Fluids 141 (2016), 250–258. [8] Tang, Yu-Hang ; Kudo, Shuhei ; Bian, Xin ; Li, Zhen ; Karniadakis, George Em . Multiscale universal interface: a concurrent framework for coupling heterogeneous solvers. J. Comput. Phys. 297 (2015), 13–31."
  },
  {
    "objectID": "phd_projects/entries/Burman_heterogeneouscoupling.html#project-description",
    "href": "phd_projects/entries/Burman_heterogeneouscoupling.html#project-description",
    "title": "A computational framework for heterogeneous coupling in large scale computations",
    "section": "",
    "text": "With the advent of high performance computing and new computer architectures there is renewed interest in the efficient coupling of different partial differential equation based models and their associated solvers. Indeed, it is often the case that for a given application or computational methods there exists a highly optimized solver. However in complex applications many different models are connected and it is then attractive to connect such optimized solvers for the subproblem to obtain a solution of the global problem. The common approach in this framework is to discretize the global continuous problem into a, possibly huge, algebraic system, and then split the latter as a system of coupled algebraic subsystems. In this case the coupling condition often consists in the identification of unknowns in the subsystems corresponding to the same global unknown. This approach forms the basis of the dominating coupling softwares available today (see for example [3,4]).\nAn alternative approach, which is lately gaining increasing interest, is, instead, to decompose the original problem already at the continuous/infinite dimensional level, thus obtaining a system of coupled infinite dimensional subproblems, to be successively discretized. The advantage of the latter approach compared to the former is that it allows for a detailed mathematical analysis of the resulting couplings under precise conditions on the local solvers. Moreover, it allows for the design of optimal preconditioners of the coupling method. An abstract framework for heterogeneous coupling including stability and error analysis as well as the design of preconditioners was recently proposed by the principal supervisor in [1]. It is shown that a variety of different couplings enter the framework including interface coupling, volume coupling, FEM-BEM coupling, bulk-surface pde systems, or networks of fractures. This theoretical work was inspired by the principal supervisors extensive work on coupling methods for heterogeneous interface problems [3], fluid-structure or fluid-fluid interaction [4,5], or FEM- BEM coupling [2,6].\n\n\n\nDescribe the main objectives of the project.\nThe main objective of the project is to further develop the ideas of [1] along the following lines:\n\nTheory. The examples of [1] are restricted to scalar problems. Here the objective of the thesis would be to show how the framework can be applied to systems, with special focus on the linearized Navier-Stokes’ equations of fluid mechanics and the Maxwell’s equations of electro magnetics. For these cases a complete analysis will be derived including stability and error analysis. Preconditioners for the coupling will also be designed. If time allows more heterogeneous systems will then be considered, such as for example free flow coupled to fractured porous media or magneto-hydrodynamics. Another more adventurous potential research strand is to explore using network approximation to learn optimal coupling spaces satisfying the conditions of [1].\nComputational aspects. The framework will then be realised in a computational software where the merits of different coupling conditions or formulations can be assessed. Here special care will be taken to make the computational software agnostic (to the furthest possible extent) to the solvers of the subproblems in the spirit of [1].\n\n\n\n\nThe software deliverable should form a significant component of the PhD project. It must be a substantial piece of software that can disseminated to the wider community. For the details of the software deliverable please describe existing software (if available), community need for the software, licensing and technical aspects (programming language, target platforms, etc.). The license by default should be an open-source license. For other types of licenses please discuss with the CDT leadership team.\nThe objective is to design and implement a software library allowing for heterogeneous coupling of different partial differential equations and their associated methods. This library would allow for interface coupling, volume coupling, mixed dimensional coupling. Assuming that the different solvers of the subproblems are optimized the library will handle the preconditioning of the coupling using the general FETI type preconditioner designed and analysed in [1]. Contrary to the libraries of [7,8] this library would have a solid mathematical foundation that will allow for a highly optimized and reliable computational software.\n[1] Bertoluzza, Silvia ; Burman, Erik. An abstract framework for heterogeneous coupling: stability, approximation and applications. arXiv:2312.11733, 2023. [2] Betcke, Timo ; Bosy, Michał ; Burman, Erik . Hybrid coupling of finite element and boundary element methods using Nitsche’s method and the Calderon projection. Numer. Algorithms 91 (2022), no. 3, 997–1019. [3] Burman, Erik ; Elfverson, Daniel ; Hansbo, Peter ; Larson, Mats G. ; Larsson, Karl . Hybridized CutFEM for elliptic interface problems. SIAM J. Sci. Comput. 41 (2019), no. 5, A3354–A3380. [4] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel A. ; Guzmán, Johnny . Fully discrete loosely coupled Robin-Robin scheme for incompressible fluid-structure interaction: stability and error analysis. Numer. Math. 151 (2022), no. 4, 807–840. [5] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel ; Guzmán, Johnny . Loosely coupled, non- iterative time-splitting scheme based on Robin-Robin coupling: unified analysis for parabolic/parabolic and parabolic/hyperbolic problems. J. Numer. Math. 31 (2023), no. 1, 59–77. [6] Bosy, Michal; Scroggs, Matthew W.; Betcke, Timo; Burman, Erik ; Cooper, Christopher D. Coupling finite and boundary element methods to solve the Poisson–Boltzmann equation for electrostatics in molecular solvation. Journal of Computational Chemistry. https://doi.org/10.1002/jcc.27262, 2023. [7] Bungartz, Hans-Joachim ; Lindner, Florian ; Gatzhammer, Bernhard ; Mehl, Miriam ; Scheufele, Klaudius ; Shukaev, Alexander ; Uekermann, Benjamin . preCICE—a fully parallel library for multi- physics surface coupling. Comput. & Fluids 141 (2016), 250–258. [8] Tang, Yu-Hang ; Kudo, Shuhei ; Bian, Xin ; Li, Zhen ; Karniadakis, George Em . Multiscale universal interface: a concurrent framework for coupling heterogeneous solvers. J. Comput. Phys. 297 (2015), 13–31."
  },
  {
    "objectID": "phd_projects/entries/cooper_tobar.html",
    "href": "phd_projects/entries/cooper_tobar.html",
    "title": "Optimising Experimental Design for Nuclear Materials Research Using Foundation Models",
    "section": "",
    "text": "The development of advanced materials for nuclear energy systems requires complex experimental campaigns involving significant time and resource investments. Current approaches rely heavily on manual planning by experts, limiting scalability and adaptability. This project aims to revolutionize experimental design in nuclear materials research by leveraging foundation models, particularly large language models (LLMs), to guide and optimize the scientific process.\n\nMathematical Foundations The project will develop a computational framework grounded in mathematical optimization techniques such as Bayesian inference, active learning, and probabilistic modelling. These methods will enable efficient experiment design by integrating prior scientific knowledge, evaluating uncertainties, and recommending optimal experimental strategies. Mathematical rigor will ensure the scalability and robustness of the approach.\nComputational Modelling The project will explore cutting-edge AI and computational modelling approaches, including deep learning architectures specialized for scientific data. Foundation models will extract relevant information from large datasets, generate hypotheses, and simulate experimental scenarios. Integration with nuclear materials models will allow for the realistic prediction of material behaviours under extreme conditions such as radiation exposure.\nReal-World Applications Close collaboration with the UK Atomic Energy Agency will align the project with nuclear materials research challenges, focusing on irradiation-resistant materials, fusion reactor components, and other mission-critical materials. The framework will be validated using experimental datasets from real-world research projects, ensuring that AI-driven experimental designs translate into tangible advances in nuclear materials science.\n\nThis interdisciplinary project bridges artificial intelligence, materials science, and nuclear research. The successful PhD candidate will gain expertise in advanced AI methods, materials characterization, and scientific data analysis, preparing them for leading roles in academia or industry. By advancing experimental design optimization, the project will accelerate discovery while reducing the time and cost associated with nuclear materials research.\n\n\nThe design of experiments in nuclear materials research is challenging due to the high costs, safety-critical constraints, and the complexity of data-driven insights required. Traditional methods rely on expert-driven approaches supported by computational models, but these are often resource-intensive and difficult to scale.\nRecent advances in AI, particularly large language models, offer promising solutions for optimizing experimental campaigns. These models excel at processing scientific literature, proposing hypotheses, and guiding experiment design. Sam Cooper’s group has demonstrated the potential of AI for materials science in automating microstructure optimization using generative models [1]. However, integrating such models into experiment design for nuclear research remains underexplored.\nFelipe Tobar’s group has made key contributions in Bayesian model selection [2] and Gaussian processes, which are essential for managing uncertainty in experimental design from the lens of Bayesian optimisation. Combining these methods with LLM-driven scientific reasoning could create a powerful system for optimizing nuclear materials experiments, particularly in designing irradiation-resistant materials and fusion reactor components.\nThis project aims to merge AI-driven language models, probabilistic inference, and computational materials simulations into a unified framework for experiment planning. By leveraging expertise from both groups and collaborating with the UK Atomic Energy Agency, the project seeks to accelerate nuclear materials research through scalable, data-driven experimental design.\n[1] Lei, G., Docherty, R. and Cooper, S.J., 2024. Materials science in the era of large language models: a perspective. Digital Discovery.\n[2] Backhoff-Veraguas, J., Fontbona, J., Rios, G. and Tobar, F., 2022. Bayesian learning with Wasserstein barycenters.ÊESAIM: Probability and Statistics, 26, pp.436-472.\n\n\n\n\nDevelop an AI-Driven Experimental Design Framework: Create a computational framework that integrates large language models (LLMs) with probabilistic modelling techniques such as Bayesian optimization and active learning. Enable automated extraction of scientific knowledge from literature, hypothesis generation, and experiment proposal.\nIncorporate Nuclear Materials Research Needs: Tailor the framework to address challenges in nuclear materials research, including irradiation-resistant materials and fusion reactor components. Ensure compatibility with computational materials models such as density functional theory (DFT) and molecular dynamics simulations.\nIntegrate Mathematical Rigor: Use advanced mathematical methods to handle uncertainties, optimize multi-objective functions, and adaptively refine experimental strategies. Ensure model transparency, interpretability, and decision-making reliability.\nValidate Through Real-World Applications: Collaborate with the UK Atomic Energy Agency to test the framework using real-world experimental datasets. Demonstrate the systemÕs ability to improve experimental efficiency, reduce costs, and accelerate scientific discovery.\nAdvance Scientific Reproducibility and Automation: Develop methods to enhance scientific reproducibility by creating transparent, traceable experiment planning processes. Publish open-source tools, models, and findings to advance the broader scientific community.\n\nThese objectives aim to create a cutting-edge, AI-powered system that transforms the way experiments are designed in nuclear materials science, accelerating innovation while ensuring scientific reliability and safety.\n\n\n\n\nAI-Powered Experiment Design Platform: A modular software platform integrating large language models , Bayesian optimization, and active learning algorithms. Features include literature extraction, hypothesis generation, experiment planning, and adaptive campaign management.\nMaterials-Specific Model Integration: Interfaces connecting the AI system with established computational materials tools such as DFT and molecular dynamics packages. Custom modules for nuclear materials applications (e.g., predicting irradiation damage).\nUser Dashboard and Visualization Tools: An interactive dashboard for planning, tracking, and analysing experimental campaigns. Visualization tools for data-driven insights, experimental recommendations, and campaign summaries.\nCodebase and APIs: Open-source code repositories hosted on platforms like GitHub. Well-documented APIs for easy integration into existing scientific workflows.\n\nData Deliverables\n\nCurated Nuclear Materials Dataset: A dataset of relevant nuclear materials experiments, including published and simulated data, formatted for AI model training.\nExperiment Design Case Studies: Fully documented experimental campaigns used as validation benchmarks. Reproducible case studies demonstrating the system’s capabilities in real-world scenarios.\nSynthetic Experiment Datasets: AI-generated experimental datasets for benchmarking and evaluation of optimization performance."
  },
  {
    "objectID": "phd_projects/entries/cooper_tobar.html#project-description",
    "href": "phd_projects/entries/cooper_tobar.html#project-description",
    "title": "Optimising Experimental Design for Nuclear Materials Research Using Foundation Models",
    "section": "",
    "text": "The development of advanced materials for nuclear energy systems requires complex experimental campaigns involving significant time and resource investments. Current approaches rely heavily on manual planning by experts, limiting scalability and adaptability. This project aims to revolutionize experimental design in nuclear materials research by leveraging foundation models, particularly large language models (LLMs), to guide and optimize the scientific process.\n\nMathematical Foundations The project will develop a computational framework grounded in mathematical optimization techniques such as Bayesian inference, active learning, and probabilistic modelling. These methods will enable efficient experiment design by integrating prior scientific knowledge, evaluating uncertainties, and recommending optimal experimental strategies. Mathematical rigor will ensure the scalability and robustness of the approach.\nComputational Modelling The project will explore cutting-edge AI and computational modelling approaches, including deep learning architectures specialized for scientific data. Foundation models will extract relevant information from large datasets, generate hypotheses, and simulate experimental scenarios. Integration with nuclear materials models will allow for the realistic prediction of material behaviours under extreme conditions such as radiation exposure.\nReal-World Applications Close collaboration with the UK Atomic Energy Agency will align the project with nuclear materials research challenges, focusing on irradiation-resistant materials, fusion reactor components, and other mission-critical materials. The framework will be validated using experimental datasets from real-world research projects, ensuring that AI-driven experimental designs translate into tangible advances in nuclear materials science.\n\nThis interdisciplinary project bridges artificial intelligence, materials science, and nuclear research. The successful PhD candidate will gain expertise in advanced AI methods, materials characterization, and scientific data analysis, preparing them for leading roles in academia or industry. By advancing experimental design optimization, the project will accelerate discovery while reducing the time and cost associated with nuclear materials research.\n\n\nThe design of experiments in nuclear materials research is challenging due to the high costs, safety-critical constraints, and the complexity of data-driven insights required. Traditional methods rely on expert-driven approaches supported by computational models, but these are often resource-intensive and difficult to scale.\nRecent advances in AI, particularly large language models, offer promising solutions for optimizing experimental campaigns. These models excel at processing scientific literature, proposing hypotheses, and guiding experiment design. Sam Cooper’s group has demonstrated the potential of AI for materials science in automating microstructure optimization using generative models [1]. However, integrating such models into experiment design for nuclear research remains underexplored.\nFelipe Tobar’s group has made key contributions in Bayesian model selection [2] and Gaussian processes, which are essential for managing uncertainty in experimental design from the lens of Bayesian optimisation. Combining these methods with LLM-driven scientific reasoning could create a powerful system for optimizing nuclear materials experiments, particularly in designing irradiation-resistant materials and fusion reactor components.\nThis project aims to merge AI-driven language models, probabilistic inference, and computational materials simulations into a unified framework for experiment planning. By leveraging expertise from both groups and collaborating with the UK Atomic Energy Agency, the project seeks to accelerate nuclear materials research through scalable, data-driven experimental design.\n[1] Lei, G., Docherty, R. and Cooper, S.J., 2024. Materials science in the era of large language models: a perspective. Digital Discovery.\n[2] Backhoff-Veraguas, J., Fontbona, J., Rios, G. and Tobar, F., 2022. Bayesian learning with Wasserstein barycenters.ÊESAIM: Probability and Statistics, 26, pp.436-472.\n\n\n\n\nDevelop an AI-Driven Experimental Design Framework: Create a computational framework that integrates large language models (LLMs) with probabilistic modelling techniques such as Bayesian optimization and active learning. Enable automated extraction of scientific knowledge from literature, hypothesis generation, and experiment proposal.\nIncorporate Nuclear Materials Research Needs: Tailor the framework to address challenges in nuclear materials research, including irradiation-resistant materials and fusion reactor components. Ensure compatibility with computational materials models such as density functional theory (DFT) and molecular dynamics simulations.\nIntegrate Mathematical Rigor: Use advanced mathematical methods to handle uncertainties, optimize multi-objective functions, and adaptively refine experimental strategies. Ensure model transparency, interpretability, and decision-making reliability.\nValidate Through Real-World Applications: Collaborate with the UK Atomic Energy Agency to test the framework using real-world experimental datasets. Demonstrate the systemÕs ability to improve experimental efficiency, reduce costs, and accelerate scientific discovery.\nAdvance Scientific Reproducibility and Automation: Develop methods to enhance scientific reproducibility by creating transparent, traceable experiment planning processes. Publish open-source tools, models, and findings to advance the broader scientific community.\n\nThese objectives aim to create a cutting-edge, AI-powered system that transforms the way experiments are designed in nuclear materials science, accelerating innovation while ensuring scientific reliability and safety.\n\n\n\n\nAI-Powered Experiment Design Platform: A modular software platform integrating large language models , Bayesian optimization, and active learning algorithms. Features include literature extraction, hypothesis generation, experiment planning, and adaptive campaign management.\nMaterials-Specific Model Integration: Interfaces connecting the AI system with established computational materials tools such as DFT and molecular dynamics packages. Custom modules for nuclear materials applications (e.g., predicting irradiation damage).\nUser Dashboard and Visualization Tools: An interactive dashboard for planning, tracking, and analysing experimental campaigns. Visualization tools for data-driven insights, experimental recommendations, and campaign summaries.\nCodebase and APIs: Open-source code repositories hosted on platforms like GitHub. Well-documented APIs for easy integration into existing scientific workflows.\n\nData Deliverables\n\nCurated Nuclear Materials Dataset: A dataset of relevant nuclear materials experiments, including published and simulated data, formatted for AI model training.\nExperiment Design Case Studies: Fully documented experimental campaigns used as validation benchmarks. Reproducible case studies demonstrating the system’s capabilities in real-world scenarios.\nSynthetic Experiment Datasets: AI-generated experimental datasets for benchmarking and evaluation of optimization performance."
  },
  {
    "objectID": "phd_projects/entries/Benning_Paralift.html",
    "href": "phd_projects/entries/Benning_Paralift.html",
    "title": "PARALIFT – A PARAllel framework for LIFTed training of neural networks",
    "section": "",
    "text": "This project is founded on innovative advancements in distributed neural network training. The cornerstone of this work, as proposed in [1], is a framework for the distributed training of deep neural networks innovatively replaces the traditional neural network consistency constraint with quadratic penalisations of that constraint. These quadratic penalisations have the advantage that they remove the need for back-propagation and pave the way for distributed optimisation algorithms that can distribute the training of individual layers across different workers. At the same time, the framework requires the introduction of auxiliary variables, which lifts the parameter search space into a higher dimensional space. This framework has subsequently been augmented by constraining the auxiliary variables to convex sets [2], and its quadratic penalties have been replaced with more bespoke penalty functions [3, 4]. In [4], the penalty functions have been replaced with tailored Bregman distance functions that, in combination with suitable optimisation algorithms, bypass the requirement of differentiating activation functions in the process. Building on this foundation, the lifted Bregman framework has been effectively applied to address the inverse problem in neural networks [5]. The objective here is to deduce the input of a pre-trained network from its output. In [6], the proposed framework has also been shown to be an effective tool for the computation of source condition elements, which are important quantities in the context of inverse problems. Furthermore, the method therein has been adapted to efficiently compute optimal sampling patterns for magnetic resonance imaging.\n\n\n\nWhile the background work has showcased the versatility of the quadratic penalisation and lifted Bregman methods in smaller-scale applications, their full potential in distributed neural network training remains largely untapped. This project aims to tap this potential by democratising access to the distributed lifted Bregman framework for the broader research community. The main objective is to develop a user-friendly software toolbox, designed to simplify the implementation of deep neural network training for large-scale datasets using the lifted Bregman framework. This toolbox will be as straightforward to use as existing toolboxes like PyTorch or JAX. A key advantage of this new toolbox is its capacity for massive parallelisation, enabling the distribution of parameter updates across various workers for individual layers, while concurrently avoiding the need for differentiating non-differentiable operations.\nThe specific main objectives of this PhD project include (but are not limited to): - Developing a Comprehensive Software Toolbox: Creating an efficient and effective toolbox for implementing the lifted Bregman framework, tailored to both ease of use and high performance. - Advancing Medical Imaging Techniques: Applying this innovative framework to medical imaging applications, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), for large-scale datasets. - Enhancing Variational Regularisations: Focusing on the training of variational regularisations with optimal source condition elements as a novel use-case. - Inverting Deep Neural Networks: Employing the lifted Bregman inversion framework for the challenging task of inverting deep neural networks, particularly in contexts involving large-scale datasets.\n\n\n\nThis project is designed to propel the use of the lifted Bregman framework from its current state with limited use cases to mass adoption. To do so, the project involves the development a Python library or Julia package (depending on the candidate’s prior programming experience and skillset) for distributed optimisation of the lifted Bregman objectives. The goal of this library or package is to make it as easy setting up complex use cases, e.g. the training of very deep transformer-based neural network architectures for large-scale datasets, as it currently is in popular automatic differentiation/deep learning frameworks such as PyTorch and JAX, but to efficiently utilise distributed training routines tailored to the lifted Bregman framework without users having to implement any of this functionality manually. In particular, the library or package will - allow users to set up a wide range of different and versatile architectures and optimisation problems. - support popular training techniques, such as batch normalisation. - feature a variety of different distributed, non-smooth optimisation routines, including distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - utilise distributed hardware architectures with little to no user input to enable both data and layer parallelism.\nThis project aims to elevate the lifted Bregman framework from its current state of limited use-cases to widespread use in the research community. Central to this objective is the development of either a Python library or a Julia package, depending on the candidate’s programming expertise and skills. This tool will facilitate distributed optimisation of lifted Bregman objectives, making the setup of complex scenarios as effortless as using established automatic differentiation and deep learning frameworks like PyTorch and JAX. However, it uniquely focuses on the efficient employment of distributed training routines specific to the lifted Bregman framework, all without requiring users having to engage with the intricate details of manual implementation. Key features of this library or package include (but are not limited to): - Versatile Architecture and Optimisation Setup: Enabling users to effortlessly configure a broad spectrum of deep neural network architectures, loss functions, constraints, and optimisation problems. - Support for Established Training Techniques: Incorporating widely used training methodologies, including batch normalisation, to ensure compatibility with current best practices. - Advanced Distributed Optimisation Routines: Offering an array of sophisticated distributed, non-smooth optimisation techniques, such as distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - Optimized Utilisation of Distributed Hardware: Designed to leverage distributed hardware architectures with minimal user intervention, thereby facilitating both data and layer parallelism for enhanced computational efficiency.\nReferences - [1] Miguel Carreira-Perpinan, and Weiran Wang. “Distributed optimization of deeply nested systems.” PMLR, 2014. - [2] Armin Askari, et al. “Lifted neural networks.” arXiv preprint arXiv:1805.01532 (2018). - [3] Fangda Gu, Armin Askari, and Laurent El Ghaoui. “Fenchel lifted networks: A Lagrange relaxation of neural network training.” PMLR, 2020. - [4] Xiaoyu Wang, and Martin Benning. “Lifted Bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [5] Xiaoyu Wang, and Martin Benning. “A lifted Bregman formulation for the inversion of deep neural networks.” Frontiers in Applied Mathematics and Statistics 9 (2023): 1176850. - [6] Martin Benning, Tatiana A. Bubba, Luca Ratti, and Danilo Riccio. “Trust your source: quantifying source condition elements for variational regularisation methods.” arXiv preprint arXiv:2303.00696 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Benning_Paralift.html#project-description",
    "href": "phd_projects/entries/Benning_Paralift.html#project-description",
    "title": "PARALIFT – A PARAllel framework for LIFTed training of neural networks",
    "section": "",
    "text": "This project is founded on innovative advancements in distributed neural network training. The cornerstone of this work, as proposed in [1], is a framework for the distributed training of deep neural networks innovatively replaces the traditional neural network consistency constraint with quadratic penalisations of that constraint. These quadratic penalisations have the advantage that they remove the need for back-propagation and pave the way for distributed optimisation algorithms that can distribute the training of individual layers across different workers. At the same time, the framework requires the introduction of auxiliary variables, which lifts the parameter search space into a higher dimensional space. This framework has subsequently been augmented by constraining the auxiliary variables to convex sets [2], and its quadratic penalties have been replaced with more bespoke penalty functions [3, 4]. In [4], the penalty functions have been replaced with tailored Bregman distance functions that, in combination with suitable optimisation algorithms, bypass the requirement of differentiating activation functions in the process. Building on this foundation, the lifted Bregman framework has been effectively applied to address the inverse problem in neural networks [5]. The objective here is to deduce the input of a pre-trained network from its output. In [6], the proposed framework has also been shown to be an effective tool for the computation of source condition elements, which are important quantities in the context of inverse problems. Furthermore, the method therein has been adapted to efficiently compute optimal sampling patterns for magnetic resonance imaging.\n\n\n\nWhile the background work has showcased the versatility of the quadratic penalisation and lifted Bregman methods in smaller-scale applications, their full potential in distributed neural network training remains largely untapped. This project aims to tap this potential by democratising access to the distributed lifted Bregman framework for the broader research community. The main objective is to develop a user-friendly software toolbox, designed to simplify the implementation of deep neural network training for large-scale datasets using the lifted Bregman framework. This toolbox will be as straightforward to use as existing toolboxes like PyTorch or JAX. A key advantage of this new toolbox is its capacity for massive parallelisation, enabling the distribution of parameter updates across various workers for individual layers, while concurrently avoiding the need for differentiating non-differentiable operations.\nThe specific main objectives of this PhD project include (but are not limited to): - Developing a Comprehensive Software Toolbox: Creating an efficient and effective toolbox for implementing the lifted Bregman framework, tailored to both ease of use and high performance. - Advancing Medical Imaging Techniques: Applying this innovative framework to medical imaging applications, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), for large-scale datasets. - Enhancing Variational Regularisations: Focusing on the training of variational regularisations with optimal source condition elements as a novel use-case. - Inverting Deep Neural Networks: Employing the lifted Bregman inversion framework for the challenging task of inverting deep neural networks, particularly in contexts involving large-scale datasets.\n\n\n\nThis project is designed to propel the use of the lifted Bregman framework from its current state with limited use cases to mass adoption. To do so, the project involves the development a Python library or Julia package (depending on the candidate’s prior programming experience and skillset) for distributed optimisation of the lifted Bregman objectives. The goal of this library or package is to make it as easy setting up complex use cases, e.g. the training of very deep transformer-based neural network architectures for large-scale datasets, as it currently is in popular automatic differentiation/deep learning frameworks such as PyTorch and JAX, but to efficiently utilise distributed training routines tailored to the lifted Bregman framework without users having to implement any of this functionality manually. In particular, the library or package will - allow users to set up a wide range of different and versatile architectures and optimisation problems. - support popular training techniques, such as batch normalisation. - feature a variety of different distributed, non-smooth optimisation routines, including distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - utilise distributed hardware architectures with little to no user input to enable both data and layer parallelism.\nThis project aims to elevate the lifted Bregman framework from its current state of limited use-cases to widespread use in the research community. Central to this objective is the development of either a Python library or a Julia package, depending on the candidate’s programming expertise and skills. This tool will facilitate distributed optimisation of lifted Bregman objectives, making the setup of complex scenarios as effortless as using established automatic differentiation and deep learning frameworks like PyTorch and JAX. However, it uniquely focuses on the efficient employment of distributed training routines specific to the lifted Bregman framework, all without requiring users having to engage with the intricate details of manual implementation. Key features of this library or package include (but are not limited to): - Versatile Architecture and Optimisation Setup: Enabling users to effortlessly configure a broad spectrum of deep neural network architectures, loss functions, constraints, and optimisation problems. - Support for Established Training Techniques: Incorporating widely used training methodologies, including batch normalisation, to ensure compatibility with current best practices. - Advanced Distributed Optimisation Routines: Offering an array of sophisticated distributed, non-smooth optimisation techniques, such as distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - Optimized Utilisation of Distributed Hardware: Designed to leverage distributed hardware architectures with minimal user intervention, thereby facilitating both data and layer parallelism for enhanced computational efficiency.\nReferences - [1] Miguel Carreira-Perpinan, and Weiran Wang. “Distributed optimization of deeply nested systems.” PMLR, 2014. - [2] Armin Askari, et al. “Lifted neural networks.” arXiv preprint arXiv:1805.01532 (2018). - [3] Fangda Gu, Armin Askari, and Laurent El Ghaoui. “Fenchel lifted networks: A Lagrange relaxation of neural network training.” PMLR, 2020. - [4] Xiaoyu Wang, and Martin Benning. “Lifted Bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [5] Xiaoyu Wang, and Martin Benning. “A lifted Bregman formulation for the inversion of deep neural networks.” Frontiers in Applied Mathematics and Statistics 9 (2023): 1176850. - [6] Martin Benning, Tatiana A. Bubba, Luca Ratti, and Danilo Riccio. “Trust your source: quantifying source condition elements for variational regularisation methods.” arXiv preprint arXiv:2303.00696 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "",
    "text": "Partial differential equations (PDEs) in non-divergence form are fundamental in a wide range of scientific and technological fields, playing a pivotal role in optimal control and game theory through the Hamilton-Jacobi-Bellman and Isaacs PDEs. Despite recent advancements in numerical methods for these PDEs (see the unified a priori analysis in [1]), their application remains confined to specialists. This limitation arises because there are no widely accessible software packages that offer the user-friendliness and computational power of state-of-the-art libraries for divergence-form equations, such as Fenics or Firedrake.\nFenics and Firedrake have succeeded primarily due to the development of the Unified Form Language, which allows users to define a wide range of PDEs in divergence form succinctly and in a manner that aligns with their mathematical structure. This project seeks to extend the Unified Form Language to accommodate PDEs in non-divergence form and implement this extension in Firedrake.\nA proof of concept has already been demonstrated in [2], where numerical experiments for the fully nonlinear Hamilton-Jacobi-Bellman/Monge-Ampère equation were conducted using integral components of the Fenics library in a non-divergence form setting (noting that Firedrake and Fenics share the same Unified Form Language)."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html#project-description",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html#project-description",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "",
    "text": "Partial differential equations (PDEs) in non-divergence form are fundamental in a wide range of scientific and technological fields, playing a pivotal role in optimal control and game theory through the Hamilton-Jacobi-Bellman and Isaacs PDEs. Despite recent advancements in numerical methods for these PDEs (see the unified a priori analysis in [1]), their application remains confined to specialists. This limitation arises because there are no widely accessible software packages that offer the user-friendliness and computational power of state-of-the-art libraries for divergence-form equations, such as Fenics or Firedrake.\nFenics and Firedrake have succeeded primarily due to the development of the Unified Form Language, which allows users to define a wide range of PDEs in divergence form succinctly and in a manner that aligns with their mathematical structure. This project seeks to extend the Unified Form Language to accommodate PDEs in non-divergence form and implement this extension in Firedrake.\nA proof of concept has already been demonstrated in [2], where numerical experiments for the fully nonlinear Hamilton-Jacobi-Bellman/Monge-Ampère equation were conducted using integral components of the Fenics library in a non-divergence form setting (noting that Firedrake and Fenics share the same Unified Form Language)."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html#details-of-softwaredata-deliverables",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe project will develop and implement an extension of the Unified Form Language to non-divergence form equations within Firedrake, focusing on semi-Lagrangian methods and incorporating mixed boundary conditions [3]. The implementation will leverage existing Firedrake functionality to solve systems of equations that combine terms in non-divergence and divergence forms. An example of such a system is mean-field games. The project license will likely be a GNU Lesser General Public License, with Python as the primary programming language. Initially, the target platforms are Ubuntu and macOS (via Homebrew).\n\n[1] Debrabant, K. & Jakobsen, E. R. Semi-Lagrangian schemes for linear and fully nonlinear diffusion equations. Mathematics of Computation 82, 1433-1462 (2012).\n[2] Feng, X. & Jensen, M. Convergent Semi-Lagrangian Methods for the Monge–Ampère Equation on Unstructured Grids. SIAM Journal on Numerical Analysis 55, 691-712 (2017).\n[3] Jaroszkowski, B. & Jensen, M. Finite element approximation of Hamilton–Jacobi–Bellman equations with nonlinear mixed boundary conditions. IMA Journal of Numerical Analysis (2023)."
  },
  {
    "objectID": "phd_projects/entries/Graham_multilevelbayesian.html",
    "href": "phd_projects/entries/Graham_multilevelbayesian.html",
    "title": "Automated Bayesian inference in stochastic differential equation models",
    "section": "",
    "text": "Diffusion processes specified by systems of stochastic differential equations are used to model phenomena in a wide range of settings. Inferring the posterior distribution on the unknown parameters of such models given, potentially partial and noisy, observations of the process is often a computationally demanding task, with even forward simulation of the model typically intractable, necessitating use of numerical integration schemes. A common approach in this setting is to use data augmentation, whereby the paths of the time discretised diffusion process are jointly inferred with the model parameters. A plethora of approximate inference schemes have been proposed in this setting, with key challenges from a computational statistics standpoint being the high-dimension of the resulting latent space when a fine time-discretization is used, and the complex, typically non-linear, dependencies between the latent variables, resulting in a challenging to approximate posterior distribution geometry. The appropriate choice of numerical integration scheme will often also be model dependent, with properties such as hypoellipticity whereby the diffusion process includes a mix of rough and smooth components, requiring careful treatment to ensure the time-discretized process retains key properties of the underlying continuous time system.\nA promising recently proposed inference approach in this setting is to consider the joint configurations of the time discretized diffusion process and model parameters which are consistent with the observations as forming an implicitly defined manifold in the latent space, with a constrained Hamiltonian Monte Carlo algorithm then used to sample from the resulting manifold supported joint posterior distribution (Graham, Thiery and Beskos, 2022). In contrast with other approaches proposed in the literature, this methodology can be applied alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. In particular, full flexibility is available in the choice of numerical integration scheme, allowing straightforward use of higher-order integrators without a tractable transition density such as those proposed in a hypo-elliptic setting (Iguchi, Beskos and Graham, 2023).\n\nGraham, M. M., Thiery, A. H., & Beskos, A. (2022). Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4), 1229-1256.\nIguchi, Y., Beskos, A., & Graham, M. (2023). Parameter Inference for Degenerate Diffusion Processes. arXiv preprint arXiv:2307.16485.\n\n\n\n\nWhile there has been much work on designing increasingly efficient, but also complex, approximate inference and numerical integration schemes for diffusion processes, there is lack of corresponding robust general-purpose software implementations which abstract details of the inference algorithm and numerical discretization away from practitioners. Probabilistic programming frameworks such as BUGS, Stan, PyMC and Turing, which combine domain specific languages to specify probabilistic generative models, with general purpose inference algorithms that can be used to estimate the unknown variables in the model, have been very successful in encouraging adoption of Bayesian methodology and are widely used in practice in a range of fields. However, while it is possible to use existing frameworks to perform inference in diffusion models, typically this will still require the user to manually implement the time discretisation of the process. Further, the default inference algorithms used in these frameworks will typically scale poorly to high-dimensional and complex posteriors characteristic of diffusion models.\nIn this project we propose to produce a dedicated open-source software framework for performing inference in stochastic differential equation models. This will combine a simple interface for practitioners to specify their model of interest, automatic detection of key system properties such as hypoellipticity and selection of appropriate discretization schemes, and general purpose implementations of a range of numerical integration schemes and inference algorithms. As well developing the underlying software framework, there will also be the opportunity to develop novel statistical algorithms for performing inference in diffusion models to integrate in to the framework and to design automated approaches for tuning and adapting the control parameters of existing methodology, to minimize the need for user-intervention."
  },
  {
    "objectID": "phd_projects/entries/Graham_multilevelbayesian.html#project-description",
    "href": "phd_projects/entries/Graham_multilevelbayesian.html#project-description",
    "title": "Automated Bayesian inference in stochastic differential equation models",
    "section": "",
    "text": "Diffusion processes specified by systems of stochastic differential equations are used to model phenomena in a wide range of settings. Inferring the posterior distribution on the unknown parameters of such models given, potentially partial and noisy, observations of the process is often a computationally demanding task, with even forward simulation of the model typically intractable, necessitating use of numerical integration schemes. A common approach in this setting is to use data augmentation, whereby the paths of the time discretised diffusion process are jointly inferred with the model parameters. A plethora of approximate inference schemes have been proposed in this setting, with key challenges from a computational statistics standpoint being the high-dimension of the resulting latent space when a fine time-discretization is used, and the complex, typically non-linear, dependencies between the latent variables, resulting in a challenging to approximate posterior distribution geometry. The appropriate choice of numerical integration scheme will often also be model dependent, with properties such as hypoellipticity whereby the diffusion process includes a mix of rough and smooth components, requiring careful treatment to ensure the time-discretized process retains key properties of the underlying continuous time system.\nA promising recently proposed inference approach in this setting is to consider the joint configurations of the time discretized diffusion process and model parameters which are consistent with the observations as forming an implicitly defined manifold in the latent space, with a constrained Hamiltonian Monte Carlo algorithm then used to sample from the resulting manifold supported joint posterior distribution (Graham, Thiery and Beskos, 2022). In contrast with other approaches proposed in the literature, this methodology can be applied alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. In particular, full flexibility is available in the choice of numerical integration scheme, allowing straightforward use of higher-order integrators without a tractable transition density such as those proposed in a hypo-elliptic setting (Iguchi, Beskos and Graham, 2023).\n\nGraham, M. M., Thiery, A. H., & Beskos, A. (2022). Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4), 1229-1256.\nIguchi, Y., Beskos, A., & Graham, M. (2023). Parameter Inference for Degenerate Diffusion Processes. arXiv preprint arXiv:2307.16485.\n\n\n\n\nWhile there has been much work on designing increasingly efficient, but also complex, approximate inference and numerical integration schemes for diffusion processes, there is lack of corresponding robust general-purpose software implementations which abstract details of the inference algorithm and numerical discretization away from practitioners. Probabilistic programming frameworks such as BUGS, Stan, PyMC and Turing, which combine domain specific languages to specify probabilistic generative models, with general purpose inference algorithms that can be used to estimate the unknown variables in the model, have been very successful in encouraging adoption of Bayesian methodology and are widely used in practice in a range of fields. However, while it is possible to use existing frameworks to perform inference in diffusion models, typically this will still require the user to manually implement the time discretisation of the process. Further, the default inference algorithms used in these frameworks will typically scale poorly to high-dimensional and complex posteriors characteristic of diffusion models.\nIn this project we propose to produce a dedicated open-source software framework for performing inference in stochastic differential equation models. This will combine a simple interface for practitioners to specify their model of interest, automatic detection of key system properties such as hypoellipticity and selection of appropriate discretization schemes, and general purpose implementations of a range of numerical integration schemes and inference algorithms. As well developing the underlying software framework, there will also be the opportunity to develop novel statistical algorithms for performing inference in diffusion models to integrate in to the framework and to design automated approaches for tuning and adapting the control parameters of existing methodology, to minimize the need for user-intervention."
  },
  {
    "objectID": "phd_projects/entries/xue_distribution_detection.html",
    "href": "phd_projects/entries/xue_distribution_detection.html",
    "title": "Out-of-distribution detection with long-tailed learning and vision-language models",
    "section": "",
    "text": "Most deep neural networks were developed on an assumption that the classes of test data have been known and seen in the training data. However, after the deployment of these models, they often encounter the data from unseen or even unknown classes, and it is often very important in practice for them to detect and handle such a situation, for example, in the cases of autonomous driving and unseen disease diagnosis. Hence, deep learning for out-of-distribution detection emerges as a crucial task to address. However, this task is challenging due to the unavailability of out-of-distribution samples for the network training. Even worse, in the available training data the seen classes often have clear class imbalance, leading to a long-tailed class distribution and the scarcity of samples from tail classes. Consequently, existing out-of-distribution detectors are typically confused between scarce tail samples and unseen out-of-distribution samples.\n\n\n\nThis project aims to tackle the challenge of out-of-distribution detection with long-tailed in-distribution samples. To achieve this aim, the student is expected to accomplish the following workplan: (1) to explore the state of the art of deep learning for out-of-distribution detection, long-tailed learning, and vision-language models; (2) to develop two innovative and promising approaches to out-of-distribution detection with long-tailed in-distribution samples, without and with the assistance of pre-trained vision-language models, respectively; (3) to design and implement comprehensive experiments to evaluate the methods developed in this project, and compare them with relevant state-of-the-art methods, on various types of datasets, from publicly-available benchmark datasets for out-of-distribution detection to healthcare datasets for rare-disease diagnosis; and (4) to provide some theoretical justification (in mathematics, statistics, or optimisation) for the proposed methods.\n\n\n\nThe software deliverables will include a Python toolbox on PyTorch for the deep learning methods developed in this project, with code to be released in a repository publicly available to the scientific community, for reproducibility and further development."
  },
  {
    "objectID": "phd_projects/entries/xue_distribution_detection.html#project-description",
    "href": "phd_projects/entries/xue_distribution_detection.html#project-description",
    "title": "Out-of-distribution detection with long-tailed learning and vision-language models",
    "section": "",
    "text": "Most deep neural networks were developed on an assumption that the classes of test data have been known and seen in the training data. However, after the deployment of these models, they often encounter the data from unseen or even unknown classes, and it is often very important in practice for them to detect and handle such a situation, for example, in the cases of autonomous driving and unseen disease diagnosis. Hence, deep learning for out-of-distribution detection emerges as a crucial task to address. However, this task is challenging due to the unavailability of out-of-distribution samples for the network training. Even worse, in the available training data the seen classes often have clear class imbalance, leading to a long-tailed class distribution and the scarcity of samples from tail classes. Consequently, existing out-of-distribution detectors are typically confused between scarce tail samples and unseen out-of-distribution samples.\n\n\n\nThis project aims to tackle the challenge of out-of-distribution detection with long-tailed in-distribution samples. To achieve this aim, the student is expected to accomplish the following workplan: (1) to explore the state of the art of deep learning for out-of-distribution detection, long-tailed learning, and vision-language models; (2) to develop two innovative and promising approaches to out-of-distribution detection with long-tailed in-distribution samples, without and with the assistance of pre-trained vision-language models, respectively; (3) to design and implement comprehensive experiments to evaluate the methods developed in this project, and compare them with relevant state-of-the-art methods, on various types of datasets, from publicly-available benchmark datasets for out-of-distribution detection to healthcare datasets for rare-disease diagnosis; and (4) to provide some theoretical justification (in mathematics, statistics, or optimisation) for the proposed methods.\n\n\n\nThe software deliverables will include a Python toolbox on PyTorch for the deep learning methods developed in this project, with code to be released in a repository publicly available to the scientific community, for reproducibility and further development."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "",
    "text": "The researcher on this project will develop new modelling capabilities in the differentiable programming paradigm that will enable gradient-based optimisation to be employed in complex systems whose models combine partial differential equations (PDEs), differential-algebraic equations (DAEs), and machine learning (ML).\nAcross physics-based and data-driven modelling, the need to optimise the modelled system for an objective is pervasive. Optimisation objectives can include the fit to observed data, minimisation of resource consumption such as energy or materials, or performance criteria such as speed or weight. Optimising these objectives requires the entire mathematical model to be differentiated with respect to its inputs. In many circumstances, the only tractable way to do this is using reverse mode algorithmic differentiation, known in the ML community as backpropagation. In short, the model code is differentiated and run backwards.\nThe most realistic and challenging model systems combine components represented in different ways. A complex, energy-intensive and polluting industrial process might contain mechanical components modelled by DAEs, fluid processes described by PDEs and have components for which no model is known but for which a neural network can be trained from data. Optimising the design of such systems has enormous environmental and economic benefits. This is the big prize.\nTraditional algorithmic differentiation tools started with a model written in a language such as Fortran or C and differentiated at the level of the primitive options. The much more modern approach is to design a programming model to be differentiated. This enables much more efficient differentiation of higher level mathematical structures. This approach was termed “differentiable programming” in the ML community (Meijer, ESEC/FSE 2018), but similar approaches exist in PDE-based modelling (Farrell et al. 2013) and for DAE models (`-Hart 2011).\nDifferentiable programming treats computational models as the mathematical composition of differentiable components, and applies the chain rule to differentiate across multiple components. By matching the mathematical and software abstractions of differentiable programming tools in ML, PDEs, and DAEs, differentiable models of the most complex coupled systems will be made possible. Initial work in this field has coupled ML and PDEs (Bouziani & Ham, 2023) and ML and DAEs (Ceccon et al., 2022). This project will build the bridge between PDEs and DAEs using the software tools in those works and hence complete the triangle, delivering seamless differentiable modelling and optimisation between all three domains."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#project-description",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#project-description",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "",
    "text": "The researcher on this project will develop new modelling capabilities in the differentiable programming paradigm that will enable gradient-based optimisation to be employed in complex systems whose models combine partial differential equations (PDEs), differential-algebraic equations (DAEs), and machine learning (ML).\nAcross physics-based and data-driven modelling, the need to optimise the modelled system for an objective is pervasive. Optimisation objectives can include the fit to observed data, minimisation of resource consumption such as energy or materials, or performance criteria such as speed or weight. Optimising these objectives requires the entire mathematical model to be differentiated with respect to its inputs. In many circumstances, the only tractable way to do this is using reverse mode algorithmic differentiation, known in the ML community as backpropagation. In short, the model code is differentiated and run backwards.\nThe most realistic and challenging model systems combine components represented in different ways. A complex, energy-intensive and polluting industrial process might contain mechanical components modelled by DAEs, fluid processes described by PDEs and have components for which no model is known but for which a neural network can be trained from data. Optimising the design of such systems has enormous environmental and economic benefits. This is the big prize.\nTraditional algorithmic differentiation tools started with a model written in a language such as Fortran or C and differentiated at the level of the primitive options. The much more modern approach is to design a programming model to be differentiated. This enables much more efficient differentiation of higher level mathematical structures. This approach was termed “differentiable programming” in the ML community (Meijer, ESEC/FSE 2018), but similar approaches exist in PDE-based modelling (Farrell et al. 2013) and for DAE models (`-Hart 2011).\nDifferentiable programming treats computational models as the mathematical composition of differentiable components, and applies the chain rule to differentiate across multiple components. By matching the mathematical and software abstractions of differentiable programming tools in ML, PDEs, and DAEs, differentiable models of the most complex coupled systems will be made possible. Initial work in this field has coupled ML and PDEs (Bouziani & Ham, 2023) and ML and DAEs (Ceccon et al., 2022). This project will build the bridge between PDEs and DAEs using the software tools in those works and hence complete the triangle, delivering seamless differentiable modelling and optimisation between all three domains."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#justification-of-how-it-fits-the-scope-of-the-programme",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#justification-of-how-it-fits-the-scope-of-the-programme",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "Justification of how it fits the scope of the programme",
    "text": "Justification of how it fits the scope of the programme\nThis is a software-centred project spanning physics-based and data-driven modelling approaches. It is at the core of the CDT scope."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#details-of-softwaredata-deliverables",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe new simulation capability will be delivered as extensions to the Firedrake (https://www.firedrakeproject.org/), OMLT project (https://github.com/cog-imperial/OMLT), Pyadjoint (https://pyadjoint.org/) and/or Pyomo (http://www.pyomo.org/)."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#references",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#references",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "References",
    "text": "References\nBouziani, Nacime and Ham, David A. Physics-driven machine learning models coupling PyTorch and Firedrake ICLR 2023 Workshop on Physics for Machine Learning, 2023 https://doi.org/10.48550/arXiv.2303.06871 Francesco Ceccon, Jordan Jalving, Joshua Haddad, Alexander Thebelt, Calvin Tsay, Carl D Laird, and Ruth Misener. 2022. OMLT: optimization & machine learning toolkit. J. Mach. Learn. Res. 23, 1, Article 349 P. E. Farrell, D. A. Ham, S. W. Funke, and M. E. Rognes, Automated Derivation of the Adjoint of High-Level Transient Finite Element Programs, SIAM Journal on Scientific Computing 2013 35:4, C369-C393 Hart, William E., Jean-Paul Watson, and David L. Woodruff. “Pyomo: modeling and solving mathematical programs in Python.” Mathematical Programming Computation 3(3) (2011): 219-260. Meijer, Erik. “Behind every great deep learning framework is an even greater programming languages concept (keynote).” Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2018."
  },
  {
    "objectID": "phd_projects/entries/guedj.html",
    "href": "phd_projects/entries/guedj.html",
    "title": "Transfer Learning and Few-Shot Learning for Science: Learning with Limited Data and Compute",
    "section": "",
    "text": "Modern scientific problems often require machine learning models that can perform effectively despite limited access to large-scale, labelled datasets. This challenge is particularly acute in domains such as biology, environmental science, medicine, or materials science, where data acquisition can be expensive, time-consuming, or constrained by ethical considerations.\nTransfer learning has emerged as a powerful paradigm, allowing pre-trained models to adapt to new tasks by leveraging knowledge from related domains. Techniques such as fine-tuning, feature extraction, and domain adaptation have demonstrated significant success in reducing data requirements. Concurrently, few-shot learning aims to push the boundaries further by enabling models to generalise from a handful of labelled examples, using methods such as meta-learning and metric-based learning.\nWhile these approaches have shown promise in domains such as computer vision and natural language processing, their application to scientific datasets presents unique challenges. Scientific data often exhibit complex, domain-specific properties, and are constrained by limited compute resources, particularly in research environments with restricted budgets. Existing work has made progress in domain adaptation and few-shot learning; however, the practical integration of these techniques into scientific workflows remains underexplored.\n\n\n\nThe primary goal of this project is to advance the use of transfer learning and few-shot learning techniques to address the challenges posed by limited data and computational resources in scientific domains. One key topic of the PhD will be to leverage recent advances from learning theory and specifically PAC-Bayesian learning to develop new algorithms. The specific objectives include:\n\nDeveloping Task-Specific Transfer Learning Frameworks: Design algorithms that can effectively transfer knowledge from large, general-purpose datasets to domain-specific tasks in science, ensuring adaptability to the nuances of scientific data.\nExploring Few-Shot Learning Paradigms for Scientific Applications: Investigate and extend few-shot learning techniques tailored for structured, high-dimensional, and often noisy scientific datasets.\nOptimising for Compute Efficiency: Propose novel methods to reduce the computational cost of transfer learning and few-shot learning approaches, including lightweight model architectures and parameter-efficient tuning strategies.\nValidating Methods Across Diverse Scientific Domains: Conduct empirical studies to assess the performance and generalisability of the proposed methods on datasets from fields such as environmental monitoring, molecular modelling, and healthcare.\nPromoting Responsible AI in Science: Address ethical and practical concerns, ensuring that the developed methods are robust, interpretable, and accessible to the scientific community.\n\n\n\n\nTo facilitate reproducibility and adoption, the project will produce the following deliverables:\n\nOpen-Source Software Tools:\n\nPython-based libraries implementing the developed transfer learning and few-shot learning methods.\nPre-configured pipelines for adapting these methods to specific scientific datasets, including user-friendly interfaces for non-experts.\n\nBenchmark Datasets:\n\nCurated datasets from a range of scientific domains, annotated and pre-processed for transfer and few-shot learning tasks.\nSynthetic datasets simulating real-world scientific scenarios to evaluate the robustness and scalability of the proposed methods.\n\nScientific publications:\n\nResearch articles introducing new methodology and theoretical analysis\nDetailed case studies demonstrating the practical utility of the methods in solving real-world scientific problems."
  },
  {
    "objectID": "phd_projects/entries/guedj.html#project-description",
    "href": "phd_projects/entries/guedj.html#project-description",
    "title": "Transfer Learning and Few-Shot Learning for Science: Learning with Limited Data and Compute",
    "section": "",
    "text": "Modern scientific problems often require machine learning models that can perform effectively despite limited access to large-scale, labelled datasets. This challenge is particularly acute in domains such as biology, environmental science, medicine, or materials science, where data acquisition can be expensive, time-consuming, or constrained by ethical considerations.\nTransfer learning has emerged as a powerful paradigm, allowing pre-trained models to adapt to new tasks by leveraging knowledge from related domains. Techniques such as fine-tuning, feature extraction, and domain adaptation have demonstrated significant success in reducing data requirements. Concurrently, few-shot learning aims to push the boundaries further by enabling models to generalise from a handful of labelled examples, using methods such as meta-learning and metric-based learning.\nWhile these approaches have shown promise in domains such as computer vision and natural language processing, their application to scientific datasets presents unique challenges. Scientific data often exhibit complex, domain-specific properties, and are constrained by limited compute resources, particularly in research environments with restricted budgets. Existing work has made progress in domain adaptation and few-shot learning; however, the practical integration of these techniques into scientific workflows remains underexplored.\n\n\n\nThe primary goal of this project is to advance the use of transfer learning and few-shot learning techniques to address the challenges posed by limited data and computational resources in scientific domains. One key topic of the PhD will be to leverage recent advances from learning theory and specifically PAC-Bayesian learning to develop new algorithms. The specific objectives include:\n\nDeveloping Task-Specific Transfer Learning Frameworks: Design algorithms that can effectively transfer knowledge from large, general-purpose datasets to domain-specific tasks in science, ensuring adaptability to the nuances of scientific data.\nExploring Few-Shot Learning Paradigms for Scientific Applications: Investigate and extend few-shot learning techniques tailored for structured, high-dimensional, and often noisy scientific datasets.\nOptimising for Compute Efficiency: Propose novel methods to reduce the computational cost of transfer learning and few-shot learning approaches, including lightweight model architectures and parameter-efficient tuning strategies.\nValidating Methods Across Diverse Scientific Domains: Conduct empirical studies to assess the performance and generalisability of the proposed methods on datasets from fields such as environmental monitoring, molecular modelling, and healthcare.\nPromoting Responsible AI in Science: Address ethical and practical concerns, ensuring that the developed methods are robust, interpretable, and accessible to the scientific community.\n\n\n\n\nTo facilitate reproducibility and adoption, the project will produce the following deliverables:\n\nOpen-Source Software Tools:\n\nPython-based libraries implementing the developed transfer learning and few-shot learning methods.\nPre-configured pipelines for adapting these methods to specific scientific datasets, including user-friendly interfaces for non-experts.\n\nBenchmark Datasets:\n\nCurated datasets from a range of scientific domains, annotated and pre-processed for transfer and few-shot learning tasks.\nSynthetic datasets simulating real-world scientific scenarios to evaluate the robustness and scalability of the proposed methods.\n\nScientific publications:\n\nResearch articles introducing new methodology and theoretical analysis\nDetailed case studies demonstrating the practical utility of the methods in solving real-world scientific problems."
  },
  {
    "objectID": "phd_projects/entries/duncan.html",
    "href": "phd_projects/entries/duncan.html",
    "title": "Smart Image-Based Sensor Optimisation for Fusion Simulation Validation",
    "section": "",
    "text": "Fusion components need to sustain continuous extreme loads including: Steady state heat fluxes up to 20 MW/m2; strong local magnetic fields on the order of several Tesla imparting large Mega Newton body forces; and neutron bombardment leading to significant irradiation damage. The combination of extreme multi-physics conditions means that validation experiments are time and resource intensive, often yielding sparse data. Given that fusion component tests have a high cost in time and resources, a means of designing targeted and data-rich experiments that can be directly compared with engineering / physics simulations is highly desirable.\nCombining advances in image-based diagnostics and machine learning to optimise their use is a route to reducing the number of experiments required for fusion component qualification. Large scale component validation experiments on UKAEA’s CHIMERA and LIBRTI facilities will cost on the order of £M’s. This cost means it is not possible to run the number of experiments required for uncertainty quantification but it is possible to simulate millions of experiments and use this information to run a targeted experimental campaign. Additionally, the cost in time and resources means that gaining high-fidelity data from an optimised array of imaging sensors has the potential to greatly reduce the required experimental effort for simulation validation. To enable this, new simulation tools are required that can accurately model image-based sensors applied to a multi-physics simulation. Furthermore, while optimisation algorithms exist for points sensors applied to single physics applications there are currently no methods for optimising arrays of image-based sensors applied to a realistic multi-physics experiment.\n\n\nThis project will leverage the expertise of Dr Andrew Duncan in machine learning methods for model calibration, uncertainty quantification and sensor placement optimization for expensive computational simulations subject to budget constraints. The project will be co-supervised by Dr Dante Kalise, who has expertise in optimisation and optimal control of PDE driven systems. The student will be based at Imperial College London but will spend up to a month each year working on-site at either Culham or FTF-Yorkshire to enhance interaction with UKAEA staff.\n\n\n\nThe aim of this project is to develop a series of robust optimisation methods that can be used to design optimal image-based sensor arrays for multi-physics validation experiments. These optimisations will use a multi-physics simulation as input and output an array of image-based diagnostics and their positions along with predicted measurement uncertainties. The first objective of this project will be to enable multi-physics sensor placement optimisation with a focus on ray-tracing algorithms for simulating camera sensors combined with other sensors. The second objective is to incorporate computationally efficient uncertainty propagation using machine learning and ensembles of multi-fidelity models. The third objective will be to develop explainable approaches to sensor placement, leveraging multi-objective optimization methods obtain provide a justification of what aspects of an experiment will be informed by a particular sensor, and the associated tradeoffs against competing objectives. The final objective of this project is to apply the developed optimization methods to a fusion-relevant test case such as the ‘Simple Test Case’ dataset being generated as part of UKAEA’s Key Challenge 4 on Digital Qualification.\n\n\n\nThe final objective of this project is to develop a suite of computational optimization methods to be applied to a fusion-relevant test case such as the ‘Simple Test Case’ dataset being generated as part of UKAEA’s Key Challenge 4 on Digital Qualification."
  },
  {
    "objectID": "phd_projects/entries/duncan.html#project-description",
    "href": "phd_projects/entries/duncan.html#project-description",
    "title": "Smart Image-Based Sensor Optimisation for Fusion Simulation Validation",
    "section": "",
    "text": "Fusion components need to sustain continuous extreme loads including: Steady state heat fluxes up to 20 MW/m2; strong local magnetic fields on the order of several Tesla imparting large Mega Newton body forces; and neutron bombardment leading to significant irradiation damage. The combination of extreme multi-physics conditions means that validation experiments are time and resource intensive, often yielding sparse data. Given that fusion component tests have a high cost in time and resources, a means of designing targeted and data-rich experiments that can be directly compared with engineering / physics simulations is highly desirable.\nCombining advances in image-based diagnostics and machine learning to optimise their use is a route to reducing the number of experiments required for fusion component qualification. Large scale component validation experiments on UKAEA’s CHIMERA and LIBRTI facilities will cost on the order of £M’s. This cost means it is not possible to run the number of experiments required for uncertainty quantification but it is possible to simulate millions of experiments and use this information to run a targeted experimental campaign. Additionally, the cost in time and resources means that gaining high-fidelity data from an optimised array of imaging sensors has the potential to greatly reduce the required experimental effort for simulation validation. To enable this, new simulation tools are required that can accurately model image-based sensors applied to a multi-physics simulation. Furthermore, while optimisation algorithms exist for points sensors applied to single physics applications there are currently no methods for optimising arrays of image-based sensors applied to a realistic multi-physics experiment.\n\n\nThis project will leverage the expertise of Dr Andrew Duncan in machine learning methods for model calibration, uncertainty quantification and sensor placement optimization for expensive computational simulations subject to budget constraints. The project will be co-supervised by Dr Dante Kalise, who has expertise in optimisation and optimal control of PDE driven systems. The student will be based at Imperial College London but will spend up to a month each year working on-site at either Culham or FTF-Yorkshire to enhance interaction with UKAEA staff.\n\n\n\nThe aim of this project is to develop a series of robust optimisation methods that can be used to design optimal image-based sensor arrays for multi-physics validation experiments. These optimisations will use a multi-physics simulation as input and output an array of image-based diagnostics and their positions along with predicted measurement uncertainties. The first objective of this project will be to enable multi-physics sensor placement optimisation with a focus on ray-tracing algorithms for simulating camera sensors combined with other sensors. The second objective is to incorporate computationally efficient uncertainty propagation using machine learning and ensembles of multi-fidelity models. The third objective will be to develop explainable approaches to sensor placement, leveraging multi-objective optimization methods obtain provide a justification of what aspects of an experiment will be informed by a particular sensor, and the associated tradeoffs against competing objectives. The final objective of this project is to apply the developed optimization methods to a fusion-relevant test case such as the ‘Simple Test Case’ dataset being generated as part of UKAEA’s Key Challenge 4 on Digital Qualification.\n\n\n\nThe final objective of this project is to develop a suite of computational optimization methods to be applied to a fusion-relevant test case such as the ‘Simple Test Case’ dataset being generated as part of UKAEA’s Key Challenge 4 on Digital Qualification."
  },
  {
    "objectID": "phd_projects/entries/joergensson.html",
    "href": "phd_projects/entries/joergensson.html",
    "title": "Computational modelling for educational research",
    "section": "",
    "text": "We would like to offer an interdisciplinary PhD project at the Department of Mathematics and the Centre for Higher Education Research and Scholarship. Ultimately, the research conducted by the student will address complex topics in education while leveraging mathematical methodologies to offer insights with real-world implications for policy and practice. We will thus focus on understanding what drives inequities in STEM and related careers, including gender equity. We will address the topic using mixed approaches with a strong quantitative outlook. This will include mathematical modelling to explore fundamental aspects of learning or decision-making through agent-based models. The agent-based models will be complemented and informed by simpler mathematical models and by using Bayesian statistics and machine-learning methods applied to real-world data. This will include existing large national datasets, such as the Administrative Data Research UK (ADR UK, https://www.adruk.org/data-access/data-catalogue/) catalogue and flagship datasets (https://www.adruk.org/data-access/flagship-datasets/). Beyond a literature search, an exploration of existing data will thus be an essential starting point for tailoring the modelling approach.\n\n\nMathematical models of learning and cognition are widespread in computational psychology and educational research. Thus, agent-based models have been applied to simulate and understand the underlying behavioural dynamics for a wide range of phenomena in social science (e.g., https://www.biorxiv.org/content/10.1101/2024.06.16.599026v1, https://onlinelibrary.wiley.com/doi/full/10.1002/andp.202100277), including gender equity (https://royalsocietypublishing.org/doi/10.1098/rsos.221346), and recently agent-based models using large language models have emerged as a new and fruitful avenue (e.g., https://arxiv.org/abs/2304.03442). Agent-based models are thus the ideal tool to explore how the decisions of individuals lead to emergent properties on a societal level. The behaviour of larger populations, on the other hand, can be modelled, e.g., using diffusion decision models or other game-theoretical concepts.\nIn general, quantitative methods, including advanced statistical analyses, play a fundamental role in educational research. Integrating quantitative approaches with qualitative methods thus offers a robust methodology in the field and a more comprehensive understanding of educational phenomena.\nDr Andreas Joergensen has a strong background in developing and applying mechanistic models, such as agent-based models, across a wide range of fields, including astrophysics, biology, and environmental conservation. Moreover, he has combined these models with Bayesian inference and machine-learning frameworks to successfully use them to shed light on real-world data. His interdisciplinary expertise will support the application of these techniques to educational research. Additionally, Professor Camille Kandiko Howson brings extensive expertise in educational research, including the development and analysis of national student surveys and measures of educational quality and learning gain; large-scale research into educational inequalities in Physics and wider Physics Education Research; and research on applied methods for learning analytics. Together, they have previously explored synergies between mathematics and education, particularly through a project proposal on gender equity at Imperial, which will flow into the PhD project.\n\n\n\nThis project aims to apply quantitative and computational methods to address complex challenges in education research, with a specific focus on inequities in STEM fields. The goal is to derive actionable insights that can influence educational policies and practices. While large-scale patterns of engagement and inequality in education are often tracked through national datasets, many researchers lack the computational expertise to analyse this data or to model the underlying dynamics. This project, therefore, seeks to bridge that gap by developing computational methodologies that connect and analyse mathematical models, addressing previously unexplored aspects of existing data.\nThe agent-based models in this project will be tailored to inform policies and practices, leveraging an understanding of the current data landscape. This includes identifying suitable proxies for inequity, such as degree progression, career outcomes, and academic performance in higher education. Additionally, useful summary statistics and underlying mechanisms will be incorporated to ensure the model’s relevance to real-world scenarios.\nThe agent-based models will be built on Bayesian principles, allowing them to capture how individuals interact with social information and navigate complex environments. By exploring the impact of different model parameters, we aim to uncover the mechanisms driving emergent social phenomena. Furthermore, surrogate models, such as emulators or simplifying approximations, will be introduced to enable comparisons with real-world data using Bayesian inference.\n\n\n\nThe student will be responsible for developing the code necessary for the project. This will include mathematical models to simulate educational processes, but it will also entail the code needed for incorporating and dealing with real-world data. All code developed during the project should adhere to best practices in open science and be made publicly available upon publication to ensure transparency and facilitate future research. This will allow the broader scientific community to benefit from the tools and methodologies created during the PhD project."
  },
  {
    "objectID": "phd_projects/entries/joergensson.html#project-description",
    "href": "phd_projects/entries/joergensson.html#project-description",
    "title": "Computational modelling for educational research",
    "section": "",
    "text": "We would like to offer an interdisciplinary PhD project at the Department of Mathematics and the Centre for Higher Education Research and Scholarship. Ultimately, the research conducted by the student will address complex topics in education while leveraging mathematical methodologies to offer insights with real-world implications for policy and practice. We will thus focus on understanding what drives inequities in STEM and related careers, including gender equity. We will address the topic using mixed approaches with a strong quantitative outlook. This will include mathematical modelling to explore fundamental aspects of learning or decision-making through agent-based models. The agent-based models will be complemented and informed by simpler mathematical models and by using Bayesian statistics and machine-learning methods applied to real-world data. This will include existing large national datasets, such as the Administrative Data Research UK (ADR UK, https://www.adruk.org/data-access/data-catalogue/) catalogue and flagship datasets (https://www.adruk.org/data-access/flagship-datasets/). Beyond a literature search, an exploration of existing data will thus be an essential starting point for tailoring the modelling approach.\n\n\nMathematical models of learning and cognition are widespread in computational psychology and educational research. Thus, agent-based models have been applied to simulate and understand the underlying behavioural dynamics for a wide range of phenomena in social science (e.g., https://www.biorxiv.org/content/10.1101/2024.06.16.599026v1, https://onlinelibrary.wiley.com/doi/full/10.1002/andp.202100277), including gender equity (https://royalsocietypublishing.org/doi/10.1098/rsos.221346), and recently agent-based models using large language models have emerged as a new and fruitful avenue (e.g., https://arxiv.org/abs/2304.03442). Agent-based models are thus the ideal tool to explore how the decisions of individuals lead to emergent properties on a societal level. The behaviour of larger populations, on the other hand, can be modelled, e.g., using diffusion decision models or other game-theoretical concepts.\nIn general, quantitative methods, including advanced statistical analyses, play a fundamental role in educational research. Integrating quantitative approaches with qualitative methods thus offers a robust methodology in the field and a more comprehensive understanding of educational phenomena.\nDr Andreas Joergensen has a strong background in developing and applying mechanistic models, such as agent-based models, across a wide range of fields, including astrophysics, biology, and environmental conservation. Moreover, he has combined these models with Bayesian inference and machine-learning frameworks to successfully use them to shed light on real-world data. His interdisciplinary expertise will support the application of these techniques to educational research. Additionally, Professor Camille Kandiko Howson brings extensive expertise in educational research, including the development and analysis of national student surveys and measures of educational quality and learning gain; large-scale research into educational inequalities in Physics and wider Physics Education Research; and research on applied methods for learning analytics. Together, they have previously explored synergies between mathematics and education, particularly through a project proposal on gender equity at Imperial, which will flow into the PhD project.\n\n\n\nThis project aims to apply quantitative and computational methods to address complex challenges in education research, with a specific focus on inequities in STEM fields. The goal is to derive actionable insights that can influence educational policies and practices. While large-scale patterns of engagement and inequality in education are often tracked through national datasets, many researchers lack the computational expertise to analyse this data or to model the underlying dynamics. This project, therefore, seeks to bridge that gap by developing computational methodologies that connect and analyse mathematical models, addressing previously unexplored aspects of existing data.\nThe agent-based models in this project will be tailored to inform policies and practices, leveraging an understanding of the current data landscape. This includes identifying suitable proxies for inequity, such as degree progression, career outcomes, and academic performance in higher education. Additionally, useful summary statistics and underlying mechanisms will be incorporated to ensure the model’s relevance to real-world scenarios.\nThe agent-based models will be built on Bayesian principles, allowing them to capture how individuals interact with social information and navigate complex environments. By exploring the impact of different model parameters, we aim to uncover the mechanisms driving emergent social phenomena. Furthermore, surrogate models, such as emulators or simplifying approximations, will be introduced to enable comparisons with real-world data using Bayesian inference.\n\n\n\nThe student will be responsible for developing the code necessary for the project. This will include mathematical models to simulate educational processes, but it will also entail the code needed for incorporating and dealing with real-world data. All code developed during the project should adhere to best practices in open science and be made publicly available upon publication to ensure transparency and facilitate future research. This will allow the broader scientific community to benefit from the tools and methodologies created during the PhD project."
  },
  {
    "objectID": "phd_projects/entries/bertrand_rg_nn.html",
    "href": "phd_projects/entries/bertrand_rg_nn.html",
    "title": "Renormalisation Group Perspective on Neural Networks",
    "section": "",
    "text": "While machine learning models have by now invaded our daily lives, we are still crucially lacking a good understanding of the reason why they prove to be so powerful. A case in point is that of artificial neural networks which were originally built to mimic neuronal organization in living systems but have since taken a life of their own, so to speak.\nTo mathematical physicists, some of the key concepts in artificial neural networks are reminiscent of the concepts of the renormalisation group that has revolutionised the thinking in statistical physics in the 1970s. Neural networks turn a complex input in the form of data into coarse grained information, say, data classification. In the renormalisation group, fast, fine, detailed degrees of freedom are integrated out to be absorbed into the couplings in an effective theory governing a given system on a coarser scale.\nWithin the statistical mechanics community the relationship between machine learning models (more particularly neural networks) and the renormalisation group has been studied from different perspectives [1]: (1) for example using neural networks to extract the relevant physics and calculate quantities normally derived in a renormalisation procedure [2,3] but also (2) in order to follow the process of information coarse-graining in machine learning [4,5].\n[1] C Beny (2013), Deep learning and the renormalization group, https://arxiv.org/abs/1301.3124\n[2] Z Li, M Luo and X Wan (2019), Extracting critical exponents by finite-size scaling with convolutional neural networks, Phys Rev B 99 075418, https://journals.aps.org/prb/pdf/10.1103/PhysRevB.99.075418\n[3] M Koch-Janusz and Z Ringel (2018), Mutual information, neural networks and the renormalization group, Nat Phys 14 578, https://www.nature.com/articles/s41567-018-0081-4\n[4] A Nguyen and K Howe (2019), Learning Renormalization with a Convolutional Neural Network, Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada, https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_148.pdf\n[5] J Erdmenger, K T Grosvenor and R Jefferson (2022), Towards quantifying information flows: Relative entropy in deep neural networks and the renormalization group, SciPost Phys. 12, 041, https://scipost.org/SciPostPhys.12.1.041/pdf\n\n\n\n\n\nIn the present project, we will explore the intimate link between the renormalisation group and neural networks (including convolutional and deep neural networks). We will study how information flows under the renormalisation group procedure and the application of deep neural networks. Here, rather than applying machine learning techniques to extract potentially relevant information from models in statistical mechanics, we will use the mathematical framework behind the renormalisation group theory to gain a deeper understanding of neural networks, even though we expect our approach may draw on the former perspective. We will use canonical models in statistical mechanics and observe how convolutional neural network extract information, following the information flow through the hidden layers. Armed with this newfound understanding, we will consider how neural networks can in turn help us predict and study phase transitions and criticality in statistical mechanics models which are not easily amenable to renormalisation group methods.\n\n\n\nThe success of this project will rely on the development of:\n\nnumerical algorithms for a large-scale computational exploration of a variety of systems in statistical mechanics;\nanalytical and computational renormalisation group procedures for a variety of canonical models in statistical mechanics;\npurpose-built, scalable and adaptable software implementing a number of neural network architectures which may involve both CPU and GPU implementations."
  },
  {
    "objectID": "phd_projects/entries/bertrand_rg_nn.html#project-description",
    "href": "phd_projects/entries/bertrand_rg_nn.html#project-description",
    "title": "Renormalisation Group Perspective on Neural Networks",
    "section": "",
    "text": "While machine learning models have by now invaded our daily lives, we are still crucially lacking a good understanding of the reason why they prove to be so powerful. A case in point is that of artificial neural networks which were originally built to mimic neuronal organization in living systems but have since taken a life of their own, so to speak.\nTo mathematical physicists, some of the key concepts in artificial neural networks are reminiscent of the concepts of the renormalisation group that has revolutionised the thinking in statistical physics in the 1970s. Neural networks turn a complex input in the form of data into coarse grained information, say, data classification. In the renormalisation group, fast, fine, detailed degrees of freedom are integrated out to be absorbed into the couplings in an effective theory governing a given system on a coarser scale.\nWithin the statistical mechanics community the relationship between machine learning models (more particularly neural networks) and the renormalisation group has been studied from different perspectives [1]: (1) for example using neural networks to extract the relevant physics and calculate quantities normally derived in a renormalisation procedure [2,3] but also (2) in order to follow the process of information coarse-graining in machine learning [4,5].\n[1] C Beny (2013), Deep learning and the renormalization group, https://arxiv.org/abs/1301.3124\n[2] Z Li, M Luo and X Wan (2019), Extracting critical exponents by finite-size scaling with convolutional neural networks, Phys Rev B 99 075418, https://journals.aps.org/prb/pdf/10.1103/PhysRevB.99.075418\n[3] M Koch-Janusz and Z Ringel (2018), Mutual information, neural networks and the renormalization group, Nat Phys 14 578, https://www.nature.com/articles/s41567-018-0081-4\n[4] A Nguyen and K Howe (2019), Learning Renormalization with a Convolutional Neural Network, Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada, https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_148.pdf\n[5] J Erdmenger, K T Grosvenor and R Jefferson (2022), Towards quantifying information flows: Relative entropy in deep neural networks and the renormalization group, SciPost Phys. 12, 041, https://scipost.org/SciPostPhys.12.1.041/pdf\n\n\n\n\n\nIn the present project, we will explore the intimate link between the renormalisation group and neural networks (including convolutional and deep neural networks). We will study how information flows under the renormalisation group procedure and the application of deep neural networks. Here, rather than applying machine learning techniques to extract potentially relevant information from models in statistical mechanics, we will use the mathematical framework behind the renormalisation group theory to gain a deeper understanding of neural networks, even though we expect our approach may draw on the former perspective. We will use canonical models in statistical mechanics and observe how convolutional neural network extract information, following the information flow through the hidden layers. Armed with this newfound understanding, we will consider how neural networks can in turn help us predict and study phase transitions and criticality in statistical mechanics models which are not easily amenable to renormalisation group methods.\n\n\n\nThe success of this project will rely on the development of:\n\nnumerical algorithms for a large-scale computational exploration of a variety of systems in statistical mechanics;\nanalytical and computational renormalisation group procedures for a variety of canonical models in statistical mechanics;\npurpose-built, scalable and adaptable software implementing a number of neural network architectures which may involve both CPU and GPU implementations."
  },
  {
    "objectID": "phd_projects/entries/ukaea_particles.html",
    "href": "phd_projects/entries/ukaea_particles.html",
    "title": "Particles for Fusion",
    "section": "",
    "text": "One of the grand challenge problems in fusion energy is modelling plasma in the edge region of a donut-shaped fusion reactor called a tokamak. In this edge region, hot plasma interfaces with the reactor wall and cold neutral gas resulting in a computationally expensive multi-scale problem that is an exascale computing challenge. This edge region exists between the core plasma, one of the hottest places in the solar system, and a container wall, which we would like to not melt. Hot plasma and cold neutral particles are both considered to be collisionless and are modelled using a “kinetic” discretisation comprising a large number of moving particles. Additional quantities, such as electromagnetic fields and cooler collisional fluid plasma, maybe modelled using finite element representations.\nA simulation of plasma may couple a turbulent fluid model for certain plasma species with a particle model for other species within the same computational domain. The resulting simulation is very computationally expensive and requires algorithms to be efficiently implemented on modern HPC hardware. However the exact equations and discretisations which model the edge region plasma are themselves an active area of research. What is required is the ability to formulate highly sophisticated coupled particle-mesh simulation software, capable of fully exploiting high performance computing facilities but retaining the flexibility to revise fundamental mathematical decisions without massive recoding. This project will achieve this by extending the Firedrake automated finite element system to mathematically and computationally support massive swarms of rapidly moving particles interacting with field variables stored on a finite element mesh. The work will span numerical mathematics, scientific computing and algorithmic differentiation (AD) to contribute a simulation capability unmatched in any current system.\n\n\nTraditional numerical PDE solvers are based on low-level code written in compiled languages such as C, C++, or Fortran. The code is far from the high level mathematical description of the problem that mathematicians, scientists and engineers formulate. Every change to the equations, discretisations, or implementation requires painstaking work in large code bases, with mathematics, performance engineering and parallelism all exposed at the same level. Progress is slow, error-prone and tedious. In contrast, the Firedrake automated finite element system (https://firedrakeproject.org) takes a radically different approach. Users write high level mathematical code closely reflecting the differential equations they are solving in Python. The high performance parallel implementation is be automatically generated as the code runs. The effect is that high performance simulations can be written in tens to hundreds of lines of code, in place of tens to hundreds of thousands of lines. Sophisticated mathematical approaches which are so complex as to be intractable for many researchers become straightforward because the user is freed from having to implement them themselves.\n\n\n\nTo extend Firedrake to performantly deal with particles coupled to finite element simulations on high performance computing systems.\nTo demonstrate and evaluate this performance using challenging test problems relevant to the simulation of fusion power generation systems.\n\n\n\nA key deliverable of this project will be a high performance Lagrangian particle capability in the Firedrake automated finite element system. The particle capability will be fully integrated in Firedrake’s high level mathematical abstraction, enabling users to formulate complex PDE-particle coupled systems with minimal code. Both particle trajectories and interpolation operators between the particles and PDE fields will be differentiable, thereby enabling the solution of inverse problems such as data assimilation and optimal design. The code will be released in Firedrake and will become available to Firedrake’s thousands of users worldwide. This project combines the development of novel computational mathematics with the opportunity to work in a professional research software engineering context and to acquire those professional skills yourself."
  },
  {
    "objectID": "phd_projects/entries/ukaea_particles.html#project-description",
    "href": "phd_projects/entries/ukaea_particles.html#project-description",
    "title": "Particles for Fusion",
    "section": "",
    "text": "One of the grand challenge problems in fusion energy is modelling plasma in the edge region of a donut-shaped fusion reactor called a tokamak. In this edge region, hot plasma interfaces with the reactor wall and cold neutral gas resulting in a computationally expensive multi-scale problem that is an exascale computing challenge. This edge region exists between the core plasma, one of the hottest places in the solar system, and a container wall, which we would like to not melt. Hot plasma and cold neutral particles are both considered to be collisionless and are modelled using a “kinetic” discretisation comprising a large number of moving particles. Additional quantities, such as electromagnetic fields and cooler collisional fluid plasma, maybe modelled using finite element representations.\nA simulation of plasma may couple a turbulent fluid model for certain plasma species with a particle model for other species within the same computational domain. The resulting simulation is very computationally expensive and requires algorithms to be efficiently implemented on modern HPC hardware. However the exact equations and discretisations which model the edge region plasma are themselves an active area of research. What is required is the ability to formulate highly sophisticated coupled particle-mesh simulation software, capable of fully exploiting high performance computing facilities but retaining the flexibility to revise fundamental mathematical decisions without massive recoding. This project will achieve this by extending the Firedrake automated finite element system to mathematically and computationally support massive swarms of rapidly moving particles interacting with field variables stored on a finite element mesh. The work will span numerical mathematics, scientific computing and algorithmic differentiation (AD) to contribute a simulation capability unmatched in any current system.\n\n\nTraditional numerical PDE solvers are based on low-level code written in compiled languages such as C, C++, or Fortran. The code is far from the high level mathematical description of the problem that mathematicians, scientists and engineers formulate. Every change to the equations, discretisations, or implementation requires painstaking work in large code bases, with mathematics, performance engineering and parallelism all exposed at the same level. Progress is slow, error-prone and tedious. In contrast, the Firedrake automated finite element system (https://firedrakeproject.org) takes a radically different approach. Users write high level mathematical code closely reflecting the differential equations they are solving in Python. The high performance parallel implementation is be automatically generated as the code runs. The effect is that high performance simulations can be written in tens to hundreds of lines of code, in place of tens to hundreds of thousands of lines. Sophisticated mathematical approaches which are so complex as to be intractable for many researchers become straightforward because the user is freed from having to implement them themselves.\n\n\n\nTo extend Firedrake to performantly deal with particles coupled to finite element simulations on high performance computing systems.\nTo demonstrate and evaluate this performance using challenging test problems relevant to the simulation of fusion power generation systems.\n\n\n\nA key deliverable of this project will be a high performance Lagrangian particle capability in the Firedrake automated finite element system. The particle capability will be fully integrated in Firedrake’s high level mathematical abstraction, enabling users to formulate complex PDE-particle coupled systems with minimal code. Both particle trajectories and interpolation operators between the particles and PDE fields will be differentiable, thereby enabling the solution of inverse problems such as data assimilation and optimal design. The code will be released in Firedrake and will become available to Firedrake’s thousands of users worldwide. This project combines the development of novel computational mathematics with the opportunity to work in a professional research software engineering context and to acquire those professional skills yourself."
  },
  {
    "objectID": "phd_projects/entries/lamb_deep_learning copy.html",
    "href": "phd_projects/entries/lamb_deep_learning copy.html",
    "title": "Deep learning with symmetry",
    "section": "",
    "text": "Project Description\nThis project will study the equivariance of deep learning. The large majority of equivariant deep learning approaches is ad-hoc, and the direction of the PhD project is to exploit the algebraic theory of invariant and equivariant functions in a systematic way to develop a versatile and general approach to learning in the presence of symmetry. A few important tasks that can be achieved with such general theory are learning symmetry group orbits (e.g. recognizing objects, irrespective of their position or orientation), learning symmetry coordinates (e.g. the position and orientation of objects), and learning symmetries and near-symmetries (invariances and near-invariances).\n\n\nExisting background work\nEquivariant deep learning is a subfield of deep learning that deals explicitly with symmetry in datasets and models. One of the most well-known and successful example is Convolutional Neural Networks (CNNs), used for the translation invariant machine learning of images. Equivariance is considered by many leading machine learners as one of the essential ingredients, explaining the remarkable leap in efficiency of deep learning models [1]. It is also an important ingredient in physics informed modelling. Our group has a long track record of understanding symmetry in dynamical systems and we are now translating this to machine learning.\n[1] M. Bronstein et al. Geometric deep learning. https://geometricdeeplearning.com\n\n\nMain objectives of the project\nThe project will develop a systematic approach to develop algorithms that merge (computer) algebra with optimisation strategies in deep learning settings. There are many areas of applications where equivariant deep learning is the intrinsic objective. The PhD project aims to develop algorithms and software to demonstrate the effectiveness of the mathematical methodology in a concrete applied setting.\n\n\nDetails of Software/Data Deliverables\nThe project will provide open source software enabling the implementation of equivariant learning strategies compatible with widely used deep learning frameworks."
  },
  {
    "objectID": "phd_projects/entries/Benning_Hearing.html",
    "href": "phd_projects/entries/Benning_Hearing.html",
    "title": "Improving impaired hearing through sound reconstruction from neural activity patterns",
    "section": "",
    "text": "This PhD project centres on the intersection of computer science and hearing research, with a primary focus on unravelling the complexities of the auditory system and its implications for perception and hearing loss. While decades of hearing research have yielded many valuable insights, the difference between normal and impaired perception relies on intricacies of the auditory system that remain poorly understood. Traditionally, the field has relied on simplifying assumptions, which limit the applicability of research findings to real-world scenarios. In [1], authors from the co-supervisor’s lab have studied auditory processing deficits in individuals with hearing loss using brain recordings, revealing that hearing loss distorts the low-dimensional neural encoding of speech, primarily affecting spectral processing and cross-frequency interactions, leading to hypersensitivity to background noise even after hearing aid amplification, and highlights the potential of deep neural networks for central brain structure research. In [2, 3], authors of the primary supervisor’s lab have introduced a novel regularisation framework for inverting deep neural networks that utilises auxiliary variables and tailored Bregman distances to lift the network parameter space into higher dimensions.\n\n\n\nThis project aims to leverage modern deep learning techniques to formulate and address the forward problem of mapping sounds onto neural activity as well as the inverse problem of reconstructing sounds from neural activity, a pivotal aspect in hearing research. By inverting neural networks to reconstruct sounds from simulated auditory nerve activity and real brain activity after various forms of cochlear damage, this project seeks to answer crucial questions about the degradation of acoustic information after hearing loss. Furthermore, it explores the extent to which information loss can be compensated when the inverse problem in the presence of cochlear damage is solved with neural activity that is recorded from an undamaged cochlea.\nThe project aims at combining and extending the described background research, with the primary goal of developing and implementing invertible deep neural networks that are expressive enough to convert sounds into neural activity, for which the inverse problem can be solved in a stable fashion, and that can be applied to real-world data.\nThis research has the potential to contribute significantly to our understanding of auditory processing, with implications for the design of assistive listening technologies. The project benefits from a substantial database of neural recordings from the inferior colliculus, a central hub in the auditory pathway, for its initial phases.\n\n\n\nHigh-quality software development is at the core of the proposed PhD project. Coding developments will include but are not limited to: - Software for Auditory Processing: Development of a comprehensive software suite, including user-friendly, well-documented programming codes for implementing and training invertible neural network models. - Integration with Existing Frameworks: Ensuring compatibility and ease of integration with established Python libraries like PyTorch or JAX, enhancing the utility of the developed software in various research and practical applications. - Open Access to Software: Making all developed software tools publicly available, ensuring they are accessible and well-documented for use by the wider research community. - Publication of Research Findings: Releasing open access publications detailing the research findings, methodologies, and applications of the developed software, contributing to the advancement of the field.\nThe applicant should have a background in Computer Science, Mathematics, or a related subject. The ideal applicant has programming experience in Python and particularly with advanced automatic differentiation and deep learning libraries such as PyTorch or JAX. A strong background in inverse problems is desirable but not mandatory.\nReferences - [1] Shievanie Sabesan, Andreas Fragner, Ciaran Bench, Fotios Drakopoulos, Nicholas A Lesica (2023) Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss. eLife 12. doi: 10.7554/eLife.85108 - [2] Xiaoyu Wang, and Martin Benning. “Lifted bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [3] Xiaoyu Wang, and Martin Benning (2023) A lifted Bregman formulation for the inversion of deep neural networks. Front. Appl. Math. Stat. 9:1176850. doi: 10.3389/fams.2023.1176850"
  },
  {
    "objectID": "phd_projects/entries/Benning_Hearing.html#project-description",
    "href": "phd_projects/entries/Benning_Hearing.html#project-description",
    "title": "Improving impaired hearing through sound reconstruction from neural activity patterns",
    "section": "",
    "text": "This PhD project centres on the intersection of computer science and hearing research, with a primary focus on unravelling the complexities of the auditory system and its implications for perception and hearing loss. While decades of hearing research have yielded many valuable insights, the difference between normal and impaired perception relies on intricacies of the auditory system that remain poorly understood. Traditionally, the field has relied on simplifying assumptions, which limit the applicability of research findings to real-world scenarios. In [1], authors from the co-supervisor’s lab have studied auditory processing deficits in individuals with hearing loss using brain recordings, revealing that hearing loss distorts the low-dimensional neural encoding of speech, primarily affecting spectral processing and cross-frequency interactions, leading to hypersensitivity to background noise even after hearing aid amplification, and highlights the potential of deep neural networks for central brain structure research. In [2, 3], authors of the primary supervisor’s lab have introduced a novel regularisation framework for inverting deep neural networks that utilises auxiliary variables and tailored Bregman distances to lift the network parameter space into higher dimensions.\n\n\n\nThis project aims to leverage modern deep learning techniques to formulate and address the forward problem of mapping sounds onto neural activity as well as the inverse problem of reconstructing sounds from neural activity, a pivotal aspect in hearing research. By inverting neural networks to reconstruct sounds from simulated auditory nerve activity and real brain activity after various forms of cochlear damage, this project seeks to answer crucial questions about the degradation of acoustic information after hearing loss. Furthermore, it explores the extent to which information loss can be compensated when the inverse problem in the presence of cochlear damage is solved with neural activity that is recorded from an undamaged cochlea.\nThe project aims at combining and extending the described background research, with the primary goal of developing and implementing invertible deep neural networks that are expressive enough to convert sounds into neural activity, for which the inverse problem can be solved in a stable fashion, and that can be applied to real-world data.\nThis research has the potential to contribute significantly to our understanding of auditory processing, with implications for the design of assistive listening technologies. The project benefits from a substantial database of neural recordings from the inferior colliculus, a central hub in the auditory pathway, for its initial phases.\n\n\n\nHigh-quality software development is at the core of the proposed PhD project. Coding developments will include but are not limited to: - Software for Auditory Processing: Development of a comprehensive software suite, including user-friendly, well-documented programming codes for implementing and training invertible neural network models. - Integration with Existing Frameworks: Ensuring compatibility and ease of integration with established Python libraries like PyTorch or JAX, enhancing the utility of the developed software in various research and practical applications. - Open Access to Software: Making all developed software tools publicly available, ensuring they are accessible and well-documented for use by the wider research community. - Publication of Research Findings: Releasing open access publications detailing the research findings, methodologies, and applications of the developed software, contributing to the advancement of the field.\nThe applicant should have a background in Computer Science, Mathematics, or a related subject. The ideal applicant has programming experience in Python and particularly with advanced automatic differentiation and deep learning libraries such as PyTorch or JAX. A strong background in inverse problems is desirable but not mandatory.\nReferences - [1] Shievanie Sabesan, Andreas Fragner, Ciaran Bench, Fotios Drakopoulos, Nicholas A Lesica (2023) Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss. eLife 12. doi: 10.7554/eLife.85108 - [2] Xiaoyu Wang, and Martin Benning. “Lifted bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [3] Xiaoyu Wang, and Martin Benning (2023) A lifted Bregman formulation for the inversion of deep neural networks. Front. Appl. Math. Stat. 9:1176850. doi: 10.3389/fams.2023.1176850"
  },
  {
    "objectID": "phd_projects/entries/haqshenas.html",
    "href": "phd_projects/entries/haqshenas.html",
    "title": "High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney, as well as osteoid osteoma. OptimUS featured as part of an international software benchmarking exercise for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic affiliations worldwide as well as a pedagogical tool for undergraduate and postgraduate students. The current BEM formulations in OptimUS assume that domains exhibit fluid-like properties. This gives us accurate prediction of ultrasound propagation in soft tissues (e.g. the liver and kidney). However, hard tissues like bones and many dense cancer tumours demonstrate solid-like (elastic) behaviour. This creates a gap in accurately modelling ultrasound propagation in domains with a mix of soft and hard tissues and underscores the need for BEM formulations that represent the physics more accurately.\nOptimUS leverages the BEMpp kernel developed by Prof Betcke in UCL. Prof Chaillat (from ENSTA, Paris, France) is an expert in modelling wave propagation in elastic domains using BEM. She is the main developer of COFFEE, which is a code for solving elastic BEM in separated domains. The supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art medical imaging and computational facilities (high-performance computing workstations as well as the UCL Research Computing Platforms Service) required for the successful delivery of this project.\n\n\n\nThis project aims to develop an efficient BEM formulation for solving ultrasound wave propagation in mixed domains of soft and hard tissues (i.e., both fluid and elastic domains). The BEM approach uses Green’s functions, which are inherently complex in elastic problems. Also, the equations become ill-conditioned at high frequencies, substantially deteriorating the performance of the solver. To address these challenges, a novel formulation based on the Helmholtz transform will be introduced, building on the team’s previous works in solving the Helmholtz and Maxwell equations. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from medical images (ultrasound/MRI/CT scans). The successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as shear wave elastography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its adoption in clinical settings, where it could be used for personalised treatment, ultimately improving patient outcomes.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of the new BEM formulation for solving ultrasound waves in complex viscous and elastic domains. The student will be actively involved in software development using Python and employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in clinics."
  },
  {
    "objectID": "phd_projects/entries/haqshenas.html#project-description",
    "href": "phd_projects/entries/haqshenas.html#project-description",
    "title": "High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney, as well as osteoid osteoma. OptimUS featured as part of an international software benchmarking exercise for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic affiliations worldwide as well as a pedagogical tool for undergraduate and postgraduate students. The current BEM formulations in OptimUS assume that domains exhibit fluid-like properties. This gives us accurate prediction of ultrasound propagation in soft tissues (e.g. the liver and kidney). However, hard tissues like bones and many dense cancer tumours demonstrate solid-like (elastic) behaviour. This creates a gap in accurately modelling ultrasound propagation in domains with a mix of soft and hard tissues and underscores the need for BEM formulations that represent the physics more accurately.\nOptimUS leverages the BEMpp kernel developed by Prof Betcke in UCL. Prof Chaillat (from ENSTA, Paris, France) is an expert in modelling wave propagation in elastic domains using BEM. She is the main developer of COFFEE, which is a code for solving elastic BEM in separated domains. The supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art medical imaging and computational facilities (high-performance computing workstations as well as the UCL Research Computing Platforms Service) required for the successful delivery of this project.\n\n\n\nThis project aims to develop an efficient BEM formulation for solving ultrasound wave propagation in mixed domains of soft and hard tissues (i.e., both fluid and elastic domains). The BEM approach uses Green’s functions, which are inherently complex in elastic problems. Also, the equations become ill-conditioned at high frequencies, substantially deteriorating the performance of the solver. To address these challenges, a novel formulation based on the Helmholtz transform will be introduced, building on the team’s previous works in solving the Helmholtz and Maxwell equations. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from medical images (ultrasound/MRI/CT scans). The successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as shear wave elastography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its adoption in clinical settings, where it could be used for personalised treatment, ultimately improving patient outcomes.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of the new BEM formulation for solving ultrasound waves in complex viscous and elastic domains. The student will be actively involved in software development using Python and employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in clinics."
  },
  {
    "objectID": "phd_projects/entries/Briol_Montecarlo.html",
    "href": "phd_projects/entries/Briol_Montecarlo.html",
    "title": "Transfer Learning for Monte Carlo",
    "section": "",
    "text": "The problem of computing quantities of interest taking the form of intractable expectations is widespread in computational statistics, machine learning, and more broadly in the computational sciences. In practice we often need to tackle several integrals which are similar; e.g. related through time or space. However, the vast majority of existing research focuses on refining approximations of a single integral and completely ignores the fact that these are part of a set of related integrals. This key piece of information, if used appropriately, could allow algorithms to share computation across integration tasks, leading to much more accurate estimates. Although a small number of such methods have been proposed, their use is currently very limited due to their high computational cost and the lack of widely available software.\n\n\n\nThe objectives of this project are two-fold: 1) Developing novel algorithms for the transfer of information across integration tasks. Most existing algorithms have a cost which increases cubically with the number of tasks and/or with the number of samples per task. This means that they are not worthwhile in all but the most challenging integration tasks. In this project, the student will explore approximation methods to reduce this from cubic to linear cost. This work will build on the literatures on Monte Carlo methods, kernel methods and Gaussian processes. 2) Developing software for the widespread use of transfer learning for numerical integration. The ProbNum package in Python already includes a basic algorithm to do this, but there is very little support in terms of hyper parameter optimisation and/or advice for practitioners on how to use the methods. Many other methods are also missing. The second objective of this project will therefore be the development of this package and implementation of any novel algorithms developed in the first part of the project. The broader aim will be to make these algorithms broadly available to allow users beyond statistics/machine learning to make use of these methods.\n\n\n\nThe PhD student will contribute to the development of the “ProbNum” python package; see https://probnum.readthedocs.io/en/latest/. Specifically, they will focus on the implementation of existing, and novel, methods for numerical integration, with a specific focus on methods based on transfer learning. This will be done in collaboration with colleagues and other PhD students at the University of Tuebingen, which are the leads for this package."
  },
  {
    "objectID": "phd_projects/entries/Briol_Montecarlo.html#project-description",
    "href": "phd_projects/entries/Briol_Montecarlo.html#project-description",
    "title": "Transfer Learning for Monte Carlo",
    "section": "",
    "text": "The problem of computing quantities of interest taking the form of intractable expectations is widespread in computational statistics, machine learning, and more broadly in the computational sciences. In practice we often need to tackle several integrals which are similar; e.g. related through time or space. However, the vast majority of existing research focuses on refining approximations of a single integral and completely ignores the fact that these are part of a set of related integrals. This key piece of information, if used appropriately, could allow algorithms to share computation across integration tasks, leading to much more accurate estimates. Although a small number of such methods have been proposed, their use is currently very limited due to their high computational cost and the lack of widely available software.\n\n\n\nThe objectives of this project are two-fold: 1) Developing novel algorithms for the transfer of information across integration tasks. Most existing algorithms have a cost which increases cubically with the number of tasks and/or with the number of samples per task. This means that they are not worthwhile in all but the most challenging integration tasks. In this project, the student will explore approximation methods to reduce this from cubic to linear cost. This work will build on the literatures on Monte Carlo methods, kernel methods and Gaussian processes. 2) Developing software for the widespread use of transfer learning for numerical integration. The ProbNum package in Python already includes a basic algorithm to do this, but there is very little support in terms of hyper parameter optimisation and/or advice for practitioners on how to use the methods. Many other methods are also missing. The second objective of this project will therefore be the development of this package and implementation of any novel algorithms developed in the first part of the project. The broader aim will be to make these algorithms broadly available to allow users beyond statistics/machine learning to make use of these methods.\n\n\n\nThe PhD student will contribute to the development of the “ProbNum” python package; see https://probnum.readthedocs.io/en/latest/. Specifically, they will focus on the implementation of existing, and novel, methods for numerical integration, with a specific focus on methods based on transfer learning. This will be done in collaboration with colleagues and other PhD students at the University of Tuebingen, which are the leads for this package."
  },
  {
    "objectID": "phd_projects/entries/Cox_kwave.html",
    "href": "phd_projects/entries/Cox_kwave.html",
    "title": "Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)",
    "section": "",
    "text": "Pseudospectral time domain models of wave propagation, in which wave equations are solved using spectral methods for computing spatial gradients and corrected-finite-difference schemes for the temporal integration, are increasingly widely used in acoustics, eg. in therapeutic and diagnostic biomedical ultrasound (imaging and therapy), and in large-scale underwater acoustics. This has been driven largely by the success of our software k-Wave, a toolbox for modelling linear and nonlinear wave propagation, written initially in Matlab but with alternative implementations now available. There are currently &gt;16,000 registered k-Wave users from &gt;70 countries who have downloaded at least one version of k-Wave, with &gt;4000 users downloading the latest release (a proxy for active users). This makes k-Wave one of the most widely used open-source tools in acoustics. The two main journal articles describing k-Wave have been cited over 2500 times. Recently, we began working on a new version of the software, k-Wave 2.0, which involves re-engineering the code base to leverage object orientated programming and differentiable functions to facilitate its use in deep learning and coupled physics problems. In addition, we are taking the opportunity to extend the algorithms underlying k-Wave so that the inputs (sources, sensors, medium properties, boundary conditions) can be defined arbitrarily in space and are not dependent on the underlying grid used for the computations. Our vision is for k-Wave 2.0 to be largely grid-independent, as far as the user is concerned. (This is guided in part by the philosophy that drives k-Wave development, that of lowering the barrier to entry for potential users as much as possible.) In the current version of k-Wave it is already possible to use ‘off-grid’ sources and sensors. This has been achieved by exploiting the fact that the bandlimited interpolant implicit in the numerical scheme is known to a high degree of accuracy [REF]. Furthermore, we have shown that reflecting boundary conditions can, in a restricted set of cases, be implemented by exploiting the symmetries in discrete sine and cosine transforms to automatically compute image source, but this approach is limited to planar boundaries and short duration simulations.\n\n\n\nThe two main limitations of k-Wave are (1) the staircasing effect that heterogeneous medium properties are subject to, because they have to be defined at the grid points and are undefined in-between, and (2) the fact that only boundary condition currently implemented is an absorbing boundary condition (perfectly matched layer) mimicking free-space propagation. The objectives of this project are to develop and code numerical schemes that overcome these limitations.\nFirst, we propose to tackle the problem of defining the medium properties independently of the grid by rearranging the governing equations so that terms containing heterogeneous medium properties are reformulated as equivalent scattering terms, and exploiting and extending the off-grid approach currently used to implement sources and sensors. A preliminary simulation with a toy model and a sound speed heterogeneity gives us confidence this approach will work, but its extension to other heterogeneities, eg. density, absorption, nonlinearity parameter has not been explored.\nSecond, we aim to implement arbitrary boundary conditions by computing, at each time-step, the distributed image source that would be required to mimic the effect of the boundary. This strategy, which will also employ the off-grid machinery, extends the conventional idea of an image source in a planar reflector to that of an arbitrarily-shaped boundary. Preliminary work has demonstrated this idea in the simple scenario of a perfectly-reflecting circular boundary, but the extent to which it can be extended to impedance boundaries with arbitrary geometries remains to be seen.\nFor both these areas of research, as well as the question of what the optimal algorithms and numerical schemes are, there remain questions as to how they are best implemented in k-Wave 2.0, both from the perspective of computational efficiency as well as the user experience.\n\n\n\nThe algorithms and code developed as part of the project will be directly contributing to k-Wave 2.0. k-Wave 2.0 is being developed within a GitHub repository with the help of UCL’s Advanced Research Computing group, and following a rigorous software development protocol."
  },
  {
    "objectID": "phd_projects/entries/Cox_kwave.html#project-description",
    "href": "phd_projects/entries/Cox_kwave.html#project-description",
    "title": "Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)",
    "section": "",
    "text": "Pseudospectral time domain models of wave propagation, in which wave equations are solved using spectral methods for computing spatial gradients and corrected-finite-difference schemes for the temporal integration, are increasingly widely used in acoustics, eg. in therapeutic and diagnostic biomedical ultrasound (imaging and therapy), and in large-scale underwater acoustics. This has been driven largely by the success of our software k-Wave, a toolbox for modelling linear and nonlinear wave propagation, written initially in Matlab but with alternative implementations now available. There are currently &gt;16,000 registered k-Wave users from &gt;70 countries who have downloaded at least one version of k-Wave, with &gt;4000 users downloading the latest release (a proxy for active users). This makes k-Wave one of the most widely used open-source tools in acoustics. The two main journal articles describing k-Wave have been cited over 2500 times. Recently, we began working on a new version of the software, k-Wave 2.0, which involves re-engineering the code base to leverage object orientated programming and differentiable functions to facilitate its use in deep learning and coupled physics problems. In addition, we are taking the opportunity to extend the algorithms underlying k-Wave so that the inputs (sources, sensors, medium properties, boundary conditions) can be defined arbitrarily in space and are not dependent on the underlying grid used for the computations. Our vision is for k-Wave 2.0 to be largely grid-independent, as far as the user is concerned. (This is guided in part by the philosophy that drives k-Wave development, that of lowering the barrier to entry for potential users as much as possible.) In the current version of k-Wave it is already possible to use ‘off-grid’ sources and sensors. This has been achieved by exploiting the fact that the bandlimited interpolant implicit in the numerical scheme is known to a high degree of accuracy [REF]. Furthermore, we have shown that reflecting boundary conditions can, in a restricted set of cases, be implemented by exploiting the symmetries in discrete sine and cosine transforms to automatically compute image source, but this approach is limited to planar boundaries and short duration simulations.\n\n\n\nThe two main limitations of k-Wave are (1) the staircasing effect that heterogeneous medium properties are subject to, because they have to be defined at the grid points and are undefined in-between, and (2) the fact that only boundary condition currently implemented is an absorbing boundary condition (perfectly matched layer) mimicking free-space propagation. The objectives of this project are to develop and code numerical schemes that overcome these limitations.\nFirst, we propose to tackle the problem of defining the medium properties independently of the grid by rearranging the governing equations so that terms containing heterogeneous medium properties are reformulated as equivalent scattering terms, and exploiting and extending the off-grid approach currently used to implement sources and sensors. A preliminary simulation with a toy model and a sound speed heterogeneity gives us confidence this approach will work, but its extension to other heterogeneities, eg. density, absorption, nonlinearity parameter has not been explored.\nSecond, we aim to implement arbitrary boundary conditions by computing, at each time-step, the distributed image source that would be required to mimic the effect of the boundary. This strategy, which will also employ the off-grid machinery, extends the conventional idea of an image source in a planar reflector to that of an arbitrarily-shaped boundary. Preliminary work has demonstrated this idea in the simple scenario of a perfectly-reflecting circular boundary, but the extent to which it can be extended to impedance boundaries with arbitrary geometries remains to be seen.\nFor both these areas of research, as well as the question of what the optimal algorithms and numerical schemes are, there remain questions as to how they are best implemented in k-Wave 2.0, both from the perspective of computational efficiency as well as the user experience.\n\n\n\nThe algorithms and code developed as part of the project will be directly contributing to k-Wave 2.0. k-Wave 2.0 is being developed within a GitHub repository with the help of UCL’s Advanced Research Computing group, and following a rigorous software development protocol."
  },
  {
    "objectID": "phd_projects/entries/Arridge_neuralODE.html",
    "href": "phd_projects/entries/Arridge_neuralODE.html",
    "title": "Time Reversal Imaging and Learned Physics with Neural ODEs",
    "section": "",
    "text": "Many imaging problems take the form of recovering the coefficients of a partial differential equation (PDE). Examples include Electrical Impedance Tomography (EIT), Diffuse Optical Tomography (DOT) and Magnetic Induction Tomography (MIT). These are categorised as non-linear. Ill-posed inverse problems and a classical approach to their solution is to iteratively solve a forward and adjoint problem by solving the PDE system explicitly. A particular challenge occurs for time-dependent problems such as Ultrasound Computed Tomography (USCT) or PhotoAcoustic Tomography (PAT) because of the extra time and memory complexity of the extra time dimension. In these problems the adjoint solution to the governing PDE has the physical interpretation of time-reversal, and has led to practical solutions for problems in several million unknowns with computation time of several hours. In the machine learning community there is an increasing interest in using deep learning techniques for solving forward and inverse problems involving PDEs, as well as for “discovering Physics” by finding appropriate polynomial combinations of differential operators to fit the observed behaviour of physical systems. For time dependent problems a key technology is the Neural ODE concept, wherein the time-dependent part of a model is evaluated by classical methods and a neural network is used at each time point to express the spatial derivatives of the operator. Prior work at UCL involving the supervisor team has developed the world leading k-Wave software for modelling acoustic propagation in a variety of setting, including heterogeneous sound speed, variable absorption. K-Wave .currently has 15.000 users from 70 countries. Recently several advances have been made in leveraging deep learning techniques including a learned Helmholtz [1], a differentiable wave simulator (j-wave) [2], and a learned Born series method [3].\n\n[1] A Stanziola, SR Arridge, BT Cox, BE Treeby, “A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound”, Journal of Computational Physics 441, 110430\n[2] A. Stanziola, S.R. Arridge, B.T. Cox, B.E. Treeby, “j-Wave: An open-source differentiable wave simulator”, SoftwareX 22, 101338\n[3] A Stanziola, S Arridge, BT Cox, BE Treeby, “A learned Born series for highly-scattering media”, JASA Express Letters 3 (5)\n\n\n\n\nIn this project we want to exploit the Neural ODE concept for inverse problems in ultrasound, including USCT and PAT. We will first tackle some established problems such as the initial pressure reconstruction in PAT and sound speed recovery in USCT. In these problems the spatial operator is known and we want to compare the efficiency and accuracy of Neural ODE forward adjoint solvers methods to existing solutions. Next we will use a convolutional neural net (CNN) in place of an explicit spatial operator to learn a non-linear PDE model for ultrasound, including extension to acoustic attenuation. Finally we will use methods of symbolic regression to learn back an interpretable physical model from the CNN as a method of discovery novel representations for non-linear acoustic propagation. Developed methods will be tested in close collaboration with the Biomedical Ultrasound Group at UCL dept Medical Physics and Biomedical Engineering, firstly on phantoms with controlled parameters and subsequently on in-vivo biological samples and human volunteers\n\n\n\nSoftware to be used :\n\nk-wave : kwave.org\njaxdf: https://github.com/ucl-bug/jaxdf\nTorchDiffEq: https://github.com/rtqichen/torchdiffeq/\nDiffrax: https://docs.kidger.site/diffrax\n\nMilestones 1. Year 1 : Integrate k-Wave as forward model within TorchDiff/Diffrax and train to reconstruct sound speed from a model 2D USCT problem 2. Year 2 : Implement a CNN to learn a non-linear spatial derivative operator for variable sound speed and acoustic attenuation 3. Year 3 : Develop symbolic regression methods to learn novel interpretable PDE models for acoustic propagation. Deliverables : New methods will be beta-released on Github and introduced in short courses/Summer schools where acoustic modelling is routinely presented. Results will be presented in conferences such as Acoustic Society of America (ASA) and Medical Ultrasound Tomography (MUST). Stable implementations will be disseminated with existing k-Wave/j-Wave platforms under rolling releases."
  },
  {
    "objectID": "phd_projects/entries/Arridge_neuralODE.html#project-description",
    "href": "phd_projects/entries/Arridge_neuralODE.html#project-description",
    "title": "Time Reversal Imaging and Learned Physics with Neural ODEs",
    "section": "",
    "text": "Many imaging problems take the form of recovering the coefficients of a partial differential equation (PDE). Examples include Electrical Impedance Tomography (EIT), Diffuse Optical Tomography (DOT) and Magnetic Induction Tomography (MIT). These are categorised as non-linear. Ill-posed inverse problems and a classical approach to their solution is to iteratively solve a forward and adjoint problem by solving the PDE system explicitly. A particular challenge occurs for time-dependent problems such as Ultrasound Computed Tomography (USCT) or PhotoAcoustic Tomography (PAT) because of the extra time and memory complexity of the extra time dimension. In these problems the adjoint solution to the governing PDE has the physical interpretation of time-reversal, and has led to practical solutions for problems in several million unknowns with computation time of several hours. In the machine learning community there is an increasing interest in using deep learning techniques for solving forward and inverse problems involving PDEs, as well as for “discovering Physics” by finding appropriate polynomial combinations of differential operators to fit the observed behaviour of physical systems. For time dependent problems a key technology is the Neural ODE concept, wherein the time-dependent part of a model is evaluated by classical methods and a neural network is used at each time point to express the spatial derivatives of the operator. Prior work at UCL involving the supervisor team has developed the world leading k-Wave software for modelling acoustic propagation in a variety of setting, including heterogeneous sound speed, variable absorption. K-Wave .currently has 15.000 users from 70 countries. Recently several advances have been made in leveraging deep learning techniques including a learned Helmholtz [1], a differentiable wave simulator (j-wave) [2], and a learned Born series method [3].\n\n[1] A Stanziola, SR Arridge, BT Cox, BE Treeby, “A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound”, Journal of Computational Physics 441, 110430\n[2] A. Stanziola, S.R. Arridge, B.T. Cox, B.E. Treeby, “j-Wave: An open-source differentiable wave simulator”, SoftwareX 22, 101338\n[3] A Stanziola, S Arridge, BT Cox, BE Treeby, “A learned Born series for highly-scattering media”, JASA Express Letters 3 (5)\n\n\n\n\nIn this project we want to exploit the Neural ODE concept for inverse problems in ultrasound, including USCT and PAT. We will first tackle some established problems such as the initial pressure reconstruction in PAT and sound speed recovery in USCT. In these problems the spatial operator is known and we want to compare the efficiency and accuracy of Neural ODE forward adjoint solvers methods to existing solutions. Next we will use a convolutional neural net (CNN) in place of an explicit spatial operator to learn a non-linear PDE model for ultrasound, including extension to acoustic attenuation. Finally we will use methods of symbolic regression to learn back an interpretable physical model from the CNN as a method of discovery novel representations for non-linear acoustic propagation. Developed methods will be tested in close collaboration with the Biomedical Ultrasound Group at UCL dept Medical Physics and Biomedical Engineering, firstly on phantoms with controlled parameters and subsequently on in-vivo biological samples and human volunteers\n\n\n\nSoftware to be used :\n\nk-wave : kwave.org\njaxdf: https://github.com/ucl-bug/jaxdf\nTorchDiffEq: https://github.com/rtqichen/torchdiffeq/\nDiffrax: https://docs.kidger.site/diffrax\n\nMilestones 1. Year 1 : Integrate k-Wave as forward model within TorchDiff/Diffrax and train to reconstruct sound speed from a model 2D USCT problem 2. Year 2 : Implement a CNN to learn a non-linear spatial derivative operator for variable sound speed and acoustic attenuation 3. Year 3 : Develop symbolic regression methods to learn novel interpretable PDE models for acoustic propagation. Deliverables : New methods will be beta-released on Github and introduced in short courses/Summer schools where acoustic modelling is routinely presented. Results will be presented in conferences such as Acoustic Society of America (ASA) and Medical Ultrasound Tomography (MUST). Stable implementations will be disseminated with existing k-Wave/j-Wave platforms under rolling releases."
  },
  {
    "objectID": "phd_projects/entries/burman_inverse.html",
    "href": "phd_projects/entries/burman_inverse.html",
    "title": "Computational solution of inverse problems using large datasets of low rank",
    "section": "",
    "text": "A major challenge in computational science is how to use information from large datasets to improve computations of the large scale severly ill-posed problems that appear in inverse problems and data assimilation. Here we will explore how large datasets characterising different aspects of the solution can be used to improve approximation. The approach is to first use machine learning techniques to find a finite dimensional manifold characterising the dataset. Then we can apply recent stability results for inverse problems with solution in a finite dimensional space, and design finite element methods for which we prove rigorous error estimates up to perturbations of the data and the accuracy of the approximate manifold. Finally, we train neural networks to map from the manifold into the finite element solution space to obtain a reduced order model that can be used for the efficient iterative solution of the inverse problem. The project will give rise to three pieces of software that can be applied together or individually."
  },
  {
    "objectID": "phd_projects/entries/burman_inverse.html#project-description",
    "href": "phd_projects/entries/burman_inverse.html#project-description",
    "title": "Computational solution of inverse problems using large datasets of low rank",
    "section": "",
    "text": "A major challenge in computational science is how to use information from large datasets to improve computations of the large scale severly ill-posed problems that appear in inverse problems and data assimilation. Here we will explore how large datasets characterising different aspects of the solution can be used to improve approximation. The approach is to first use machine learning techniques to find a finite dimensional manifold characterising the dataset. Then we can apply recent stability results for inverse problems with solution in a finite dimensional space, and design finite element methods for which we prove rigorous error estimates up to perturbations of the data and the accuracy of the approximate manifold. Finally, we train neural networks to map from the manifold into the finite element solution space to obtain a reduced order model that can be used for the efficient iterative solution of the inverse problem. The project will give rise to three pieces of software that can be applied together or individually."
  },
  {
    "objectID": "phd_projects/entries/Kovalchuk_healthcare.html",
    "href": "phd_projects/entries/Kovalchuk_healthcare.html",
    "title": "Drift detection in graph streams and its applications in healthcare",
    "section": "",
    "text": "Graphs have become a useful tool for representing information in many application domains. Social, computer, sensor and transport networks; molecular structures and business processes – all can be represented as attributed graphs. One of the characteristics of such graphs is dynamism – the graph structure, as well as the attributes of nodes and edges can change over time. The accuracy of predictive and inference models built over dynamic graphs depends on the ability of the models to adapt to these changes. This project will propose novel methods for detecting changes in graphs over time (also known as drifts) and demonstrate their usefulness in downstream machine learning and process mining tasks performed over dynamic graphs. The work will build upon the methods recently proposed by the principal supervisor based on graphs and deep learning for process mining https://doi.org/10.1109/ACCESS.2020.3025999 and drift detection in business processes https://doi.org/10.3390/e24070910. The PhD student will take this previous work as a basis to both advance the theoretical approach to drift detection in graph streams and demonstrate its generalizability by applying to a new domain, namely, discovering disease trajectories. Disease trajectories represented as graphs can help predict disease progression, risk of developing comorbid disorders and patient outcomes more accurately Kusuma et al. Existing solutions for discovering disease trajectories are based on statistical analysis https://doi.org/10.1038/ncomms5022 and knowledge graphs https://doi.org/10.1186/s13326-020-00228-8, thus computationally expensive and not scalable. Furthermore, these solutions are not capable of adapting to changes over time (e.g. changes in disease progression due to events such as the coronavirus pandemic or introducing a new drug/vaccination). Finally, there is currently no solution exists based on the UK population data. The methods built in this project will be applied to Hospital Episode Statistics (HES) data, thus revealing the healthcare picture of the entire UK population.\n\n\n\n\nDevelop novel methods for drift detection in graph streams to outperform the state-of-the-art methods according to at least one metric: accuracy, scalability, computational time.\nDemonstrate the generalizability of the developed methods by applying them to several domains such as detecting drifts in business processes and disease trajectories, aiming to outperform the state-of-the-art methods in respective areas according to at least one metric: accuracy, scalability, computational time.\nDevelop software allowing non-domain experts query process graphs to answer a range of research/business questions.\n\n\n\n\nThis project will deliver a disease trajectory browser, similar to the one proposed in https://doi.org/10.1038/s41467-020-18682-4 but based on more advanced process discovery and drift detection in graph streams methods developed as part of this project and leveraging UK population data. The proposed process discovery and drift detection methods will be developed in Python. The discovered graphs will be sorted in a graph database such as Neo4j. The frontend will be built using a range of JavaScript libraries, including Cytoscape.js and Dagre.js to represent graphs. The browser will be released as an open-source software under the MIT license."
  },
  {
    "objectID": "phd_projects/entries/Kovalchuk_healthcare.html#project-description",
    "href": "phd_projects/entries/Kovalchuk_healthcare.html#project-description",
    "title": "Drift detection in graph streams and its applications in healthcare",
    "section": "",
    "text": "Graphs have become a useful tool for representing information in many application domains. Social, computer, sensor and transport networks; molecular structures and business processes – all can be represented as attributed graphs. One of the characteristics of such graphs is dynamism – the graph structure, as well as the attributes of nodes and edges can change over time. The accuracy of predictive and inference models built over dynamic graphs depends on the ability of the models to adapt to these changes. This project will propose novel methods for detecting changes in graphs over time (also known as drifts) and demonstrate their usefulness in downstream machine learning and process mining tasks performed over dynamic graphs. The work will build upon the methods recently proposed by the principal supervisor based on graphs and deep learning for process mining https://doi.org/10.1109/ACCESS.2020.3025999 and drift detection in business processes https://doi.org/10.3390/e24070910. The PhD student will take this previous work as a basis to both advance the theoretical approach to drift detection in graph streams and demonstrate its generalizability by applying to a new domain, namely, discovering disease trajectories. Disease trajectories represented as graphs can help predict disease progression, risk of developing comorbid disorders and patient outcomes more accurately Kusuma et al. Existing solutions for discovering disease trajectories are based on statistical analysis https://doi.org/10.1038/ncomms5022 and knowledge graphs https://doi.org/10.1186/s13326-020-00228-8, thus computationally expensive and not scalable. Furthermore, these solutions are not capable of adapting to changes over time (e.g. changes in disease progression due to events such as the coronavirus pandemic or introducing a new drug/vaccination). Finally, there is currently no solution exists based on the UK population data. The methods built in this project will be applied to Hospital Episode Statistics (HES) data, thus revealing the healthcare picture of the entire UK population.\n\n\n\n\nDevelop novel methods for drift detection in graph streams to outperform the state-of-the-art methods according to at least one metric: accuracy, scalability, computational time.\nDemonstrate the generalizability of the developed methods by applying them to several domains such as detecting drifts in business processes and disease trajectories, aiming to outperform the state-of-the-art methods in respective areas according to at least one metric: accuracy, scalability, computational time.\nDevelop software allowing non-domain experts query process graphs to answer a range of research/business questions.\n\n\n\n\nThis project will deliver a disease trajectory browser, similar to the one proposed in https://doi.org/10.1038/s41467-020-18682-4 but based on more advanced process discovery and drift detection in graph streams methods developed as part of this project and leveraging UK population data. The proposed process discovery and drift detection methods will be developed in Python. The discovered graphs will be sorted in a graph database such as Neo4j. The frontend will be built using a range of JavaScript libraries, including Cytoscape.js and Dagre.js to represent graphs. The browser will be released as an open-source software under the MIT license."
  },
  {
    "objectID": "phd_projects/entries/hetherington1.html",
    "href": "phd_projects/entries/hetherington1.html",
    "title": "Model correlation and the version control tree",
    "section": "",
    "text": "When we consider the behaviour of numerical models of complex systems, it is relatively common practice to treat the predictions of different models of the same phenomena as independent and uncorrelated. This approach was, for example, taken during the Covid-19 pandemic to uncertainty quantification for the national R number, based on the outputs of an ensemble of models taken from a variety of UK universities. Yet this approach misses a key element: models form a tree of inheritance, similar to that of a phylogenetic tree in biology. Models share traits with their parents. Models borrow from each other. Models developed in physical proximity interbreed - the labs and companies they are developed in. This interrelatedness is hard to characterise and control - a key challenge for reliable UQ when using ensembles of models. We propose initially to use the version control tree implied by Github, as a proxy for this, and look at model output correlation properties conditioned on graph distance in the version control tree."
  },
  {
    "objectID": "phd_projects/entries/hetherington1.html#project-description",
    "href": "phd_projects/entries/hetherington1.html#project-description",
    "title": "Model correlation and the version control tree",
    "section": "",
    "text": "When we consider the behaviour of numerical models of complex systems, it is relatively common practice to treat the predictions of different models of the same phenomena as independent and uncorrelated. This approach was, for example, taken during the Covid-19 pandemic to uncertainty quantification for the national R number, based on the outputs of an ensemble of models taken from a variety of UK universities. Yet this approach misses a key element: models form a tree of inheritance, similar to that of a phylogenetic tree in biology. Models share traits with their parents. Models borrow from each other. Models developed in physical proximity interbreed - the labs and companies they are developed in. This interrelatedness is hard to characterise and control - a key challenge for reliable UQ when using ensembles of models. We propose initially to use the version control tree implied by Github, as a proxy for this, and look at model output correlation properties conditioned on graph distance in the version control tree."
  },
  {
    "objectID": "phd_projects/entries/Bravi_antibodies.html",
    "href": "phd_projects/entries/Bravi_antibodies.html",
    "title": "Machine learning-based biophysical models of antibody properties and function",
    "section": "",
    "text": "Antibodies are proteins that play a key role in the immune response against pathogens by binding specifically to the pathogen. The body is able to modify sections of antibodies through mutations to improve the specificity and affinity of the binding to unseen pathogens. Consequently, the design of antibodies as possible therapeutic tools that can bind to specific targets (e.g., pathogens, cancerous cells) is an area of highly active research. However, which molecular and structural properties determine the specific binding of antibodies to protein targets remains unclear, thus hampering our understanding of the mutational effects in the immune response and impeding progress in the rational design of antibodies. In this project, we will develop machine learning methods to predict antibodies’ functional properties related to their binding that are informed by biophysical modelling of their sequence and structure as captured by graph-based representations of antibody-protein interactions. In particular, the aim of these models is to predict and characterize single-site mutations that can improve antibody binding to specific targets without compromising other biophysical properties, with potential applications in antibody design.\n\n\nTo achieve our main goal, we will leverage various machine learning approaches which we have developed, and we will develop new ones to exploit the increasing amount of data on antibodies and their cognate target proteins. Schematically, the objectives and tasks of the project will be:\n\nTo train a model that can capture long-range complex dependencies between amino acids at different sites along the antibody/protein amino acid chain, and which, accounting for such dependencies, can provide a single-site measure of amino acid importance to target binding. For this task we will build upon a transformer-like architecture [1].\nTo build biophysically informed models of antibody-protein interactions that can give graph representations of such interactions and antibody/protein structures, summarizing and distilling relevant biochemical and structural information. We will employ different graph construction techniques, from geometric graphs that capture packing to biophysical models of energetic interactions to higher-order models (akin to simplicial complexes) that capture many-body interactions in the structure. We will then explore strategies to machine-learn refinements of such graph representations (e.g., GCNs or GNNs) by optimizing the task of predicting antibodytarget binding.\nTo set up a multi-task learning framework whereby different prediction tasks are performed jointly (e.g., predicting structural flexibility, binding specificity, binding affinity etc.). Such a framework will rely on graph neural networks and will be designed to obtain single-site predictions of importance to target binding that account for long-range correlations between sites as well as multiple structural and biochemical constraints. These predictions will be key to estimate in silico the effect of mutations and set up a computational framework to guide mutation-based antibody design in the laboratory. Ongoing collaborations with the Imperial Department of Chemistry, as well as with the LiverpoolImperial AIChemy UKRI Hub in AI will allow us to establish links to experimental antibody design for validation and further development.\n\n\n\n\nThe field of machine learning approaches to biophysical modelling and design of immune-related proteins like antibodies has witnessed growing activity recently [2]. The supervisors’ group has recently published a machine learning method to predict antibody binding affinity to specific targets that leverages jointly a modelling framework capturing antibodies’ structural fluctuations upon binding and convolutional neural networks [3]. The ongoing research is focussing on biochemically informed graph-based representations of antibody structures [4,5] and on combining them to neural network architectures to model how structural flexibility contributes to antibodies’ functional properties related to target binding.\n\n\n\nThe coding and data developments during the project will consist of well curated computational pipelines comprising:\n\nAlgorithms to produce graph-based representations of protein data combining structural and biochemical information;\nMachine learning architectures (transformers, graph neural networks) taking protein data as input and performing different learning tasks (potentially in a multi-task learning setting).\n\nThe software deliverable will consist of the python packages made freely available e.g. via github (like we did for Ref. [3]) and usable through a webserver (like we did for Ref. [5]). Such software will allow a potential user to: pre-process custom antibody/protein data of interest to produce inputs to the machine learning methods and graph-based representations that can be used for further analysis; evaluate on them the predictions of the machine learning methods; re-train/fine-tune the machine learning architectures on the custom data; extract insights and analyze the predictions for e.g. antibody design purposes.\n\n\n\n[1] Leem, Mitchell, Farmery, Barton, Galson. Deciphering the language of antibodies using selfsupervised learning, 2022. Patterns, 3(7). [2] Bravi. Development and use of machine learning algorithms in vaccine target selection, 2024. npj Vaccines, 9(15). [3] Michalewicz, Barahona, Bravi. ANTIPASTI: interpretable prediction of antibody binding affinity exploiting Normal Modes and Deep Learning, 2024. Structure, 32: 1-13. [4] Song, Barahona, Yaliraki. Bagpype: A python package for the construction of atomistic, energy-weighted graphs from biomolecular structures, 2021. [5] Amor, B., Schaub, M., Yaliraki, S. et al. Prediction of allosteric sites and mediating interactions through bond-to-bond propensities, 2016. Nat Commun, 7:12477."
  },
  {
    "objectID": "phd_projects/entries/Bravi_antibodies.html#project-description",
    "href": "phd_projects/entries/Bravi_antibodies.html#project-description",
    "title": "Machine learning-based biophysical models of antibody properties and function",
    "section": "",
    "text": "Antibodies are proteins that play a key role in the immune response against pathogens by binding specifically to the pathogen. The body is able to modify sections of antibodies through mutations to improve the specificity and affinity of the binding to unseen pathogens. Consequently, the design of antibodies as possible therapeutic tools that can bind to specific targets (e.g., pathogens, cancerous cells) is an area of highly active research. However, which molecular and structural properties determine the specific binding of antibodies to protein targets remains unclear, thus hampering our understanding of the mutational effects in the immune response and impeding progress in the rational design of antibodies. In this project, we will develop machine learning methods to predict antibodies’ functional properties related to their binding that are informed by biophysical modelling of their sequence and structure as captured by graph-based representations of antibody-protein interactions. In particular, the aim of these models is to predict and characterize single-site mutations that can improve antibody binding to specific targets without compromising other biophysical properties, with potential applications in antibody design.\n\n\nTo achieve our main goal, we will leverage various machine learning approaches which we have developed, and we will develop new ones to exploit the increasing amount of data on antibodies and their cognate target proteins. Schematically, the objectives and tasks of the project will be:\n\nTo train a model that can capture long-range complex dependencies between amino acids at different sites along the antibody/protein amino acid chain, and which, accounting for such dependencies, can provide a single-site measure of amino acid importance to target binding. For this task we will build upon a transformer-like architecture [1].\nTo build biophysically informed models of antibody-protein interactions that can give graph representations of such interactions and antibody/protein structures, summarizing and distilling relevant biochemical and structural information. We will employ different graph construction techniques, from geometric graphs that capture packing to biophysical models of energetic interactions to higher-order models (akin to simplicial complexes) that capture many-body interactions in the structure. We will then explore strategies to machine-learn refinements of such graph representations (e.g., GCNs or GNNs) by optimizing the task of predicting antibodytarget binding.\nTo set up a multi-task learning framework whereby different prediction tasks are performed jointly (e.g., predicting structural flexibility, binding specificity, binding affinity etc.). Such a framework will rely on graph neural networks and will be designed to obtain single-site predictions of importance to target binding that account for long-range correlations between sites as well as multiple structural and biochemical constraints. These predictions will be key to estimate in silico the effect of mutations and set up a computational framework to guide mutation-based antibody design in the laboratory. Ongoing collaborations with the Imperial Department of Chemistry, as well as with the LiverpoolImperial AIChemy UKRI Hub in AI will allow us to establish links to experimental antibody design for validation and further development.\n\n\n\n\nThe field of machine learning approaches to biophysical modelling and design of immune-related proteins like antibodies has witnessed growing activity recently [2]. The supervisors’ group has recently published a machine learning method to predict antibody binding affinity to specific targets that leverages jointly a modelling framework capturing antibodies’ structural fluctuations upon binding and convolutional neural networks [3]. The ongoing research is focussing on biochemically informed graph-based representations of antibody structures [4,5] and on combining them to neural network architectures to model how structural flexibility contributes to antibodies’ functional properties related to target binding.\n\n\n\nThe coding and data developments during the project will consist of well curated computational pipelines comprising:\n\nAlgorithms to produce graph-based representations of protein data combining structural and biochemical information;\nMachine learning architectures (transformers, graph neural networks) taking protein data as input and performing different learning tasks (potentially in a multi-task learning setting).\n\nThe software deliverable will consist of the python packages made freely available e.g. via github (like we did for Ref. [3]) and usable through a webserver (like we did for Ref. [5]). Such software will allow a potential user to: pre-process custom antibody/protein data of interest to produce inputs to the machine learning methods and graph-based representations that can be used for further analysis; evaluate on them the predictions of the machine learning methods; re-train/fine-tune the machine learning architectures on the custom data; extract insights and analyze the predictions for e.g. antibody design purposes.\n\n\n\n[1] Leem, Mitchell, Farmery, Barton, Galson. Deciphering the language of antibodies using selfsupervised learning, 2022. Patterns, 3(7). [2] Bravi. Development and use of machine learning algorithms in vaccine target selection, 2024. npj Vaccines, 9(15). [3] Michalewicz, Barahona, Bravi. ANTIPASTI: interpretable prediction of antibody binding affinity exploiting Normal Modes and Deep Learning, 2024. Structure, 32: 1-13. [4] Song, Barahona, Yaliraki. Bagpype: A python package for the construction of atomistic, energy-weighted graphs from biomolecular structures, 2021. [5] Amor, B., Schaub, M., Yaliraki, S. et al. Prediction of allosteric sites and mediating interactions through bond-to-bond propensities, 2016. Nat Commun, 7:12477."
  },
  {
    "objectID": "phd_projects/entries/Hewett_fractaldomains.html",
    "href": "phd_projects/entries/Hewett_fractaldomains.html",
    "title": "Numerical methods for PDEs in fractal domains",
    "section": "",
    "text": "PDEs are a fundamental tool in the mathematical modelling of physical processes, and obtaining accurate approximate numerical solutions to PDE problems is a key area within scientific computing. Many numerical methods are available, but most are designed for the case where the PDE is to be solved in a domain with a smooth boundary (typically, Lipschitz or smoother). However, many real-world problems involve domains with non-smooth boundaries, which may possess detail on multiple length scales. Fractals provide a mathematical model for such boundaries, and in this project the student will develop numerical methods for solving PDEs in domains with fractal boundaries. This is an exciting and emerging field, with lots of challenging open mathematical and computational questions to investigate.\nThe principal supervisor’s group have already made significant progress in the study of acoustic scattering problems involving fractal scatterers, via integral equation methods. They have investigated two main approaches: (i) conventional discretizations on a smoother “pre-fractal” approximation to the domain, and (ii) novel discretizations on the true fractal domain. Computationally, for approach (i) we used the open-source Bempp software of Timo Betcke on polygonal or polyhedral prefractal approximations, and for approach (ii) we developed our own open-source integral equations package called IFSIntegrals, implementing the novel numerical quadrature rules developed recently within the group for singular integration over fractal domains. So far our investigations have been limited to acoustic simulations, and mostly to problems with Dirichlet boundary conditions, but the results are very promising both theoretically and computationally.\n\n\n\nThe objectives of the project could include: - extending the applicability of our integral equation methods to new PDEs (e.g. Maxwell equations) and/or new boundary conditions (e.g. Neumann, impedance) - the development of novel discontinuous Galerkin (DG) methods on meshes with fractal elements - the development of novel quadrature rules involving singular and/or oscillatory integrands over fractal domains - the application of the developed methods to shape optimization problems relevant to the design of fractal wave absorbers/reflectors/ transmitters.\n\n\n\nThe software deliverable would be an implementation of the numerical methods developed within the project. Depending on the nature of the methods developed, this might take the form of new modules within the Bempp or IFSIntegrals packages, which are written in Python/Rust and Julia respectively, or a standalone open-source code written in a language to be determined by the project team. The software would provide the numerical PDE community with a valuable tool with which to simulate PDEs in domains with fractal boundaries, which currently does not exist, opening up the prospect of simulating new and important problems in acoustic and electromagnetic scattering, metamaterial design and imaging technologies."
  },
  {
    "objectID": "phd_projects/entries/Hewett_fractaldomains.html#project-description",
    "href": "phd_projects/entries/Hewett_fractaldomains.html#project-description",
    "title": "Numerical methods for PDEs in fractal domains",
    "section": "",
    "text": "PDEs are a fundamental tool in the mathematical modelling of physical processes, and obtaining accurate approximate numerical solutions to PDE problems is a key area within scientific computing. Many numerical methods are available, but most are designed for the case where the PDE is to be solved in a domain with a smooth boundary (typically, Lipschitz or smoother). However, many real-world problems involve domains with non-smooth boundaries, which may possess detail on multiple length scales. Fractals provide a mathematical model for such boundaries, and in this project the student will develop numerical methods for solving PDEs in domains with fractal boundaries. This is an exciting and emerging field, with lots of challenging open mathematical and computational questions to investigate.\nThe principal supervisor’s group have already made significant progress in the study of acoustic scattering problems involving fractal scatterers, via integral equation methods. They have investigated two main approaches: (i) conventional discretizations on a smoother “pre-fractal” approximation to the domain, and (ii) novel discretizations on the true fractal domain. Computationally, for approach (i) we used the open-source Bempp software of Timo Betcke on polygonal or polyhedral prefractal approximations, and for approach (ii) we developed our own open-source integral equations package called IFSIntegrals, implementing the novel numerical quadrature rules developed recently within the group for singular integration over fractal domains. So far our investigations have been limited to acoustic simulations, and mostly to problems with Dirichlet boundary conditions, but the results are very promising both theoretically and computationally.\n\n\n\nThe objectives of the project could include: - extending the applicability of our integral equation methods to new PDEs (e.g. Maxwell equations) and/or new boundary conditions (e.g. Neumann, impedance) - the development of novel discontinuous Galerkin (DG) methods on meshes with fractal elements - the development of novel quadrature rules involving singular and/or oscillatory integrands over fractal domains - the application of the developed methods to shape optimization problems relevant to the design of fractal wave absorbers/reflectors/ transmitters.\n\n\n\nThe software deliverable would be an implementation of the numerical methods developed within the project. Depending on the nature of the methods developed, this might take the form of new modules within the Bempp or IFSIntegrals packages, which are written in Python/Rust and Julia respectively, or a standalone open-source code written in a language to be determined by the project team. The software would provide the numerical PDE community with a valuable tool with which to simulate PDEs in domains with fractal boundaries, which currently does not exist, opening up the prospect of simulating new and important problems in acoustic and electromagnetic scattering, metamaterial design and imaging technologies."
  },
  {
    "objectID": "phd_projects/entries/Berloff_Quasigeostrophic.html",
    "href": "phd_projects/entries/Berloff_Quasigeostrophic.html",
    "title": "Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model",
    "section": "",
    "text": "This Project would be ideal for a student who seeks to develop skills in software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science research. It aims for a major overhaul and upgrade of the existing Quasi-Geostrophic Coupled Model (Q-GCM) of the ocean-atmosphere system to convert this model into a versatile modular community code for extremely fast high-resolution climate modelling in arbitrary geometrical setups. The model’s ability to quickly produce global-scope multi-century climate simulations faithfully representing mesoscale ocean–atmosphere interactions would allow it to set the milestones for future research of fundamental climate processes that are currently out of reach for state-of-the-art coupled General Circulation Models (GCMs) due to prohibitive computational expenses of such simulations.\nAdvancing our understanding of multi-scale climate variability is at the heart of the Project. A particular focus here is on the internal variability of the ocean-atmosphere subsystem of the climate system, which can occur even in the absence of variations in the external forcing. Such variability can generally be classified to fall into one of the following categories:\n\ninternal variability of the oceans;\ninternal variability of the atmosphere; and\ncoupled ocean–atmosphere variability.\n\nOf course, other factors contribute to the climate variability modes on different levels (e.g., interaction with cryosphere, coupled land–atmosphere processes, and so forth). Most coupled ocean-atmosphere GCMs do not yet adequately discriminate between the scenarios (i)–(iii), because the model dynamics still lacks accurate representation of small-scale processes due to their insufficient horizontal resolutions. In particular, global coupled GCMs do not have the required capability to resolve routinely the oceanic weather represented by multi-scale ensembles of synoptic mesoscale eddies, which evolve in a complicated, spatially inhomogeneous and poorly understood way.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1: Left panel: Snapshot with results of FESOM1.4 (Wang et al. 2014) simulation on the global mesh with 4 km resolution in the full North Atlantic; white color shows upper-ocean flow speed; note the intense Gulf stream current with surrounding turbulence. Right panel: Snapshot of the upper-ocean potential vorticity anomaly as simulated by the idealized square-box quasigeostrophic ocean model coupled to the atmosphere; this model produces turbulent Gulf stream that couples to the underlying atmosphere.\n\n\n\nThe cutting edge high-resolution ocean modelling efforts, which involve enormous computational expenses https://fesom.de/models/fesom14/ typically produce a single realization worth of a few decades of simulation at most, with marginally accepted dynamical resolution of the eddies achieved at least for midlatitudes (note that at high latitudes the eddies and, hence, required resolutions become even smaller). Many such simulations are also run in the ocean-only context, despite the growing evidence that the mesoscale air-sea interaction affects, in major — and, perhaps, nonlinear ways, — the atmospheric large-scale low-frequency variability (Mathews et al. 2024). However, climate-type simulations not only require hundreds of multi-century runs for robust statistical ensemble predictions, but they also have to consider different environmental scenarios (e.g., for greenhouse gas emissions) and sensitivities to many physical factors. The ability to faithfully characterize the effects of the mesoscale ocean eddies and currents in a coupled, global setting is a major stumbling block in climate research, and is thus one of the grand research challenges of our time.\nOne way out of this deadlock is to develop accurate statistical–dynamical eddy parameterizations for the use in realistic models, which is a major task of its own. An alternative way, proposed here, is to make use of intermediate-complexity coupled ocean–atmosphere process models capable of accessing new and crucial knowledge about the processes involved, yet casting them in advanced settings that would permit direct comparisons with the real world’s climate variability.\nIndeed, driven by the surging demand of climate science, the last two decades witnessed development of idealized, intermediate-complexity, midlatitude, quasi-geostrophic (QG), ocean–atmosphere coupled models, which can routinely resolve oceanic mesoscale eddies (Hogg et al. 2003; Kravtsov et al. 2007; Berloff et al. 2007a). These models are at least 100 times more computationally efficient than the heavy-duty global coupled GCMs. The Q-GCM of Hogg et al. (2003), which is a starting point of this project, couples its oceanic and atmospheric subsystems via ageostrophic boundary layers of both fluids. Aside from the natural limitation of QG models to be formally accurate within the midlatitude belts, these models are cast in the simplest square-basin or channel geometries, which hinders their immediate application to interpreting the observed climate variability. Yet, Q-GCM is a powerful tool for simulating midlatitude coupled climate variability with fully resolved oceanic mesoscale turbulence.\nIdealized eddy-resolving ocean and coupled modelling thus far established not only existence and robustness of the intrinsic ocean-only variability dubbed as the rbulent Oscillator (e.g., Berloff et al. 2007b), but also the importance of this variability for the ocean-atmosphere coupled variability (Kravtsov et al. 2007; Kurashina and Berloff 2023a,b). Similar fundamental importance of the eddies for driving decadal variability has been established in the Southern Ocean (Hogg and Blundell 2006). Considering these coupled dynamics in progressively more realistic Q-GCM is of high priority, but this effort requires significant upgrades of the existing modelling capabilities.\n\n\nThe software development objective of the Project is very significant upgrade of the existing Q-GCM, including generation of modular geometrical setups, addition of new physics and incorporation of superior numerical algorithms, as well as the requisite updates to the post-processing tools and software library. The computational objective of the Project is to produce new multi-century simulations in the northern-hemisphere (NH) and southern-hemisphere (SH) model configurations. More specifically, we hypothesize that in the NH case the Atlantic and Pacific oceans will generate their own internal, large-scale decadal-to-interdecadal Turbulent Oscillator variability, which will be coupled through atmospheric teleconnections. In the SH case the situation is likely to be more complicated, as the midlatitude basins will be also connected via the Antarctic Circumpolar Current. The analysis objective of the Project is to gain dynamical understanding of the involved variabilities.\n\n\n\nThe starting point for the Project will be the most recent study of the Q-GCM idealized-ocean double-gyre coupling with the atmosphere, in which new, zonally asymmetric coupled variability modes have been discovered and understood (Kurashina and Berloff 2023a,b). The following Q-GCM model developments are envisioned:\n\nAdding the second (rectangular) ocean basin to represent Atlantic-Pacific teleconnections (NB: this configuration can be passed to a Master student for spin-off project);\nAdding capabilities for representing (arbitrary and) realistically shaped basins (this requires complete overhaul of the elliptic solver with the matrix capacitance method, and recoding the boundary conditions);\nUpgrading advection operators in both ocean and atmosphere components with the high-resolution, efficient CABARET advection scheme (Karabasov et al. 2009);\nAdding moist dynamics to the atmosphere (Kravtsov et al. 2022);\nDeveloping realistic NH and SH Q-GCM model configurations (i.e., with two isolated ocean basins; with three ocean basins connected by the circular Southern ocean) and obtaining the corresponding milestone solutions both with high- and low-resolution configurations (the latter will help to quantify effects of the small scales and serve as the basis for eddy parameterizations);\nDeveloping comprehensive post-processing library of numerical routines for both Eulerian and Lagrangian analyses and visualizations of the Q-GCM solutions;\nProviding initial analyses of the milestone solutions; disentangling causalities of the ocean-atmosphere and ocean-ocean couplings, as well as understanding the main mesoscale eddy effects and their mechanisms;\nDeveloping mixed-layer model for dynamics of floating tracers, such as plankton and pollutants; this will in effect prepare ground for the coupled ocean-atmosphere modelling with oceanic biochemistry and with global carbon cycle.\n\nThe student will benefit from the interdisciplinary nature of the Project that combines a great deal of original and creative research within the remit of software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science. Close interactions with external research partners will be a vital aspect of the Project, ensuring its optimal development and success: Prof. Sergey Kravtsov (University of Wisconsin, USA), Prof. William Dewar (Florida State University, USA) and Prof. Andrew Hogg (Australian National University, Australia). The Project will be a great opportunity for outreaching the climate science community and providing broad and practical impact.\n\n\n\n\nBerloff, P., A. Hogg, and W. Dewar, 2007b: The turbulent oscillator: A mechanism of low-frequency variability of the wind-driven ocean gyres. J. Phys. Oceanogr., 37, 2363–2386.\nBerloff, P., S. Kravtsov, W. Dewar, and J. McWilliams, 2007a: Ocean eddy dynamics in a coupled ocean-atmosphere model. J. Phys. Oceanogr., 37, 1103–1121.\nHogg, A., and J. Blundell, 2006: Interdecadal variability of the Southern Ocean. J. Phys. Oceanogr., 36, 1626-–1645.\nHogg, A., W. Dewar, P. Killworth et al., 2003: A quasi-geostrophic coupled model (Q-GCM). Monthly Weather Review, 131, 2261–2278.\nKarabasov, S., P. Berloff, and V. Goloviznin, 2009: CABARET in the ocean gyres. Ocean Modelling, 30, 155–168.\nKravtsov, S., I. Mastilovic, A. Hogg, W. Dewar, and J. Blundell, 2022: The Moist Quasi-Geostrophic Coupled Model: MQ-GCM 2.0. Geoscientific Model Development, 15, 7449-–7469.\nKravtsov, S., W. Dewar, P. Berloff, J. McWilliams, and M. Ghil, 2007: A highly nonlinear coupled mode of decadal variability in a midlatitude ocean-atmosphere model. Dyn. Atmos. Ocean., 43, 123–150.\nKurashina, R., and P. Berloff, 2023b: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part II: Ocean mechanisms. Climate Dynamics, doi:10.1007/s00382-023-06767-x.\nKurashina, R., and P. Berloff, 2023a: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part I: Anatomy. Climate Dynamics, doi:10.1007/s00382-023-06782-y.\nMathews, J. P., Czaja, A., Vitart, F., and Roberts, C., 2024: Gulf Stream moisture fluxes impact atmospheric blocks throughout the Northern Hemisphere. Geophysical Research Letters, 51, e2024GL108826. https://doi.org/10.1029/2024GL108826\nWang, Q., Danilov, S., Sidorenko, D., Timmermann, R., Wekerle, C., Wang, X., … and Schröter, J., 2014: The Finite Element Sea Ice-Ocean Model (FESOM) v. 1.4: formulation of an ocean general circulation model. Geoscientific Model Development, 7(2), 663–693."
  },
  {
    "objectID": "phd_projects/entries/Berloff_Quasigeostrophic.html#project-description",
    "href": "phd_projects/entries/Berloff_Quasigeostrophic.html#project-description",
    "title": "Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model",
    "section": "",
    "text": "This Project would be ideal for a student who seeks to develop skills in software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science research. It aims for a major overhaul and upgrade of the existing Quasi-Geostrophic Coupled Model (Q-GCM) of the ocean-atmosphere system to convert this model into a versatile modular community code for extremely fast high-resolution climate modelling in arbitrary geometrical setups. The model’s ability to quickly produce global-scope multi-century climate simulations faithfully representing mesoscale ocean–atmosphere interactions would allow it to set the milestones for future research of fundamental climate processes that are currently out of reach for state-of-the-art coupled General Circulation Models (GCMs) due to prohibitive computational expenses of such simulations.\nAdvancing our understanding of multi-scale climate variability is at the heart of the Project. A particular focus here is on the internal variability of the ocean-atmosphere subsystem of the climate system, which can occur even in the absence of variations in the external forcing. Such variability can generally be classified to fall into one of the following categories:\n\ninternal variability of the oceans;\ninternal variability of the atmosphere; and\ncoupled ocean–atmosphere variability.\n\nOf course, other factors contribute to the climate variability modes on different levels (e.g., interaction with cryosphere, coupled land–atmosphere processes, and so forth). Most coupled ocean-atmosphere GCMs do not yet adequately discriminate between the scenarios (i)–(iii), because the model dynamics still lacks accurate representation of small-scale processes due to their insufficient horizontal resolutions. In particular, global coupled GCMs do not have the required capability to resolve routinely the oceanic weather represented by multi-scale ensembles of synoptic mesoscale eddies, which evolve in a complicated, spatially inhomogeneous and poorly understood way.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1: Left panel: Snapshot with results of FESOM1.4 (Wang et al. 2014) simulation on the global mesh with 4 km resolution in the full North Atlantic; white color shows upper-ocean flow speed; note the intense Gulf stream current with surrounding turbulence. Right panel: Snapshot of the upper-ocean potential vorticity anomaly as simulated by the idealized square-box quasigeostrophic ocean model coupled to the atmosphere; this model produces turbulent Gulf stream that couples to the underlying atmosphere.\n\n\n\nThe cutting edge high-resolution ocean modelling efforts, which involve enormous computational expenses https://fesom.de/models/fesom14/ typically produce a single realization worth of a few decades of simulation at most, with marginally accepted dynamical resolution of the eddies achieved at least for midlatitudes (note that at high latitudes the eddies and, hence, required resolutions become even smaller). Many such simulations are also run in the ocean-only context, despite the growing evidence that the mesoscale air-sea interaction affects, in major — and, perhaps, nonlinear ways, — the atmospheric large-scale low-frequency variability (Mathews et al. 2024). However, climate-type simulations not only require hundreds of multi-century runs for robust statistical ensemble predictions, but they also have to consider different environmental scenarios (e.g., for greenhouse gas emissions) and sensitivities to many physical factors. The ability to faithfully characterize the effects of the mesoscale ocean eddies and currents in a coupled, global setting is a major stumbling block in climate research, and is thus one of the grand research challenges of our time.\nOne way out of this deadlock is to develop accurate statistical–dynamical eddy parameterizations for the use in realistic models, which is a major task of its own. An alternative way, proposed here, is to make use of intermediate-complexity coupled ocean–atmosphere process models capable of accessing new and crucial knowledge about the processes involved, yet casting them in advanced settings that would permit direct comparisons with the real world’s climate variability.\nIndeed, driven by the surging demand of climate science, the last two decades witnessed development of idealized, intermediate-complexity, midlatitude, quasi-geostrophic (QG), ocean–atmosphere coupled models, which can routinely resolve oceanic mesoscale eddies (Hogg et al. 2003; Kravtsov et al. 2007; Berloff et al. 2007a). These models are at least 100 times more computationally efficient than the heavy-duty global coupled GCMs. The Q-GCM of Hogg et al. (2003), which is a starting point of this project, couples its oceanic and atmospheric subsystems via ageostrophic boundary layers of both fluids. Aside from the natural limitation of QG models to be formally accurate within the midlatitude belts, these models are cast in the simplest square-basin or channel geometries, which hinders their immediate application to interpreting the observed climate variability. Yet, Q-GCM is a powerful tool for simulating midlatitude coupled climate variability with fully resolved oceanic mesoscale turbulence.\nIdealized eddy-resolving ocean and coupled modelling thus far established not only existence and robustness of the intrinsic ocean-only variability dubbed as the rbulent Oscillator (e.g., Berloff et al. 2007b), but also the importance of this variability for the ocean-atmosphere coupled variability (Kravtsov et al. 2007; Kurashina and Berloff 2023a,b). Similar fundamental importance of the eddies for driving decadal variability has been established in the Southern Ocean (Hogg and Blundell 2006). Considering these coupled dynamics in progressively more realistic Q-GCM is of high priority, but this effort requires significant upgrades of the existing modelling capabilities.\n\n\nThe software development objective of the Project is very significant upgrade of the existing Q-GCM, including generation of modular geometrical setups, addition of new physics and incorporation of superior numerical algorithms, as well as the requisite updates to the post-processing tools and software library. The computational objective of the Project is to produce new multi-century simulations in the northern-hemisphere (NH) and southern-hemisphere (SH) model configurations. More specifically, we hypothesize that in the NH case the Atlantic and Pacific oceans will generate their own internal, large-scale decadal-to-interdecadal Turbulent Oscillator variability, which will be coupled through atmospheric teleconnections. In the SH case the situation is likely to be more complicated, as the midlatitude basins will be also connected via the Antarctic Circumpolar Current. The analysis objective of the Project is to gain dynamical understanding of the involved variabilities.\n\n\n\nThe starting point for the Project will be the most recent study of the Q-GCM idealized-ocean double-gyre coupling with the atmosphere, in which new, zonally asymmetric coupled variability modes have been discovered and understood (Kurashina and Berloff 2023a,b). The following Q-GCM model developments are envisioned:\n\nAdding the second (rectangular) ocean basin to represent Atlantic-Pacific teleconnections (NB: this configuration can be passed to a Master student for spin-off project);\nAdding capabilities for representing (arbitrary and) realistically shaped basins (this requires complete overhaul of the elliptic solver with the matrix capacitance method, and recoding the boundary conditions);\nUpgrading advection operators in both ocean and atmosphere components with the high-resolution, efficient CABARET advection scheme (Karabasov et al. 2009);\nAdding moist dynamics to the atmosphere (Kravtsov et al. 2022);\nDeveloping realistic NH and SH Q-GCM model configurations (i.e., with two isolated ocean basins; with three ocean basins connected by the circular Southern ocean) and obtaining the corresponding milestone solutions both with high- and low-resolution configurations (the latter will help to quantify effects of the small scales and serve as the basis for eddy parameterizations);\nDeveloping comprehensive post-processing library of numerical routines for both Eulerian and Lagrangian analyses and visualizations of the Q-GCM solutions;\nProviding initial analyses of the milestone solutions; disentangling causalities of the ocean-atmosphere and ocean-ocean couplings, as well as understanding the main mesoscale eddy effects and their mechanisms;\nDeveloping mixed-layer model for dynamics of floating tracers, such as plankton and pollutants; this will in effect prepare ground for the coupled ocean-atmosphere modelling with oceanic biochemistry and with global carbon cycle.\n\nThe student will benefit from the interdisciplinary nature of the Project that combines a great deal of original and creative research within the remit of software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science. Close interactions with external research partners will be a vital aspect of the Project, ensuring its optimal development and success: Prof. Sergey Kravtsov (University of Wisconsin, USA), Prof. William Dewar (Florida State University, USA) and Prof. Andrew Hogg (Australian National University, Australia). The Project will be a great opportunity for outreaching the climate science community and providing broad and practical impact.\n\n\n\n\nBerloff, P., A. Hogg, and W. Dewar, 2007b: The turbulent oscillator: A mechanism of low-frequency variability of the wind-driven ocean gyres. J. Phys. Oceanogr., 37, 2363–2386.\nBerloff, P., S. Kravtsov, W. Dewar, and J. McWilliams, 2007a: Ocean eddy dynamics in a coupled ocean-atmosphere model. J. Phys. Oceanogr., 37, 1103–1121.\nHogg, A., and J. Blundell, 2006: Interdecadal variability of the Southern Ocean. J. Phys. Oceanogr., 36, 1626-–1645.\nHogg, A., W. Dewar, P. Killworth et al., 2003: A quasi-geostrophic coupled model (Q-GCM). Monthly Weather Review, 131, 2261–2278.\nKarabasov, S., P. Berloff, and V. Goloviznin, 2009: CABARET in the ocean gyres. Ocean Modelling, 30, 155–168.\nKravtsov, S., I. Mastilovic, A. Hogg, W. Dewar, and J. Blundell, 2022: The Moist Quasi-Geostrophic Coupled Model: MQ-GCM 2.0. Geoscientific Model Development, 15, 7449-–7469.\nKravtsov, S., W. Dewar, P. Berloff, J. McWilliams, and M. Ghil, 2007: A highly nonlinear coupled mode of decadal variability in a midlatitude ocean-atmosphere model. Dyn. Atmos. Ocean., 43, 123–150.\nKurashina, R., and P. Berloff, 2023b: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part II: Ocean mechanisms. Climate Dynamics, doi:10.1007/s00382-023-06767-x.\nKurashina, R., and P. Berloff, 2023a: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part I: Anatomy. Climate Dynamics, doi:10.1007/s00382-023-06782-y.\nMathews, J. P., Czaja, A., Vitart, F., and Roberts, C., 2024: Gulf Stream moisture fluxes impact atmospheric blocks throughout the Northern Hemisphere. Geophysical Research Letters, 51, e2024GL108826. https://doi.org/10.1029/2024GL108826\nWang, Q., Danilov, S., Sidorenko, D., Timmermann, R., Wekerle, C., Wang, X., … and Schröter, J., 2014: The Finite Element Sea Ice-Ocean Model (FESOM) v. 1.4: formulation of an ocean general circulation model. Geoscientific Model Development, 7(2), 663–693."
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "",
    "text": "Bayesian methods for calibration, that is fitting the parameters of a model to data, often require a large number of model runs to adequately explore the model parameter space. This makes them infeasible to apply to computationally intensive simulations. A common approach in this setting is to fit a statistical emulator to a limited number of model evaluations and use this as a proxy for the full model in an inference algorithm (Kennedy and O’Hagan, 2001; Rasmussen, 2003).\nA key issue in such approaches is the choice of points in the parameter space at which to evaluate the model at to form the data used to fit the emulator. Use of Bayesian experimental design approaches, where the parameter values are chosen to minimize the expected future uncertainty about the model output at each step (Kandasamy, Schneider and Póczos, 2015; Sinsbeck and Nowak, 2017; Acerbi, 2018; Oliveira, Ott, and Ramos, 2021), offer a principled approach for maximising the information we get about a model from a limited number of evaluations, and can be extended to evaluate batches of points at each iteration (Järvenpää et al., 2021) improving the opportunity for parallelisation.\nEmulator-based calibration methods often treat the model as a monolithic black box, however, in reality complex simulators typically have tuneable variables allowing a trade off between numerical accuracy and computational cost. Multi-level Monte Carlo methods (Henrich, 2001; Giles, 2015) offer one approach for exploiting this trade off, combining simulations at multiple level of fidelity to reduce computational cost while controlling error. A recent related idea in the field of probabilistic numerics, is to statistically model how numerical error varies with fidelity (Teymur et al., 2021) and use this to probabilistically extrapolate the behaviour at high fidelity.\n\n\nThe aim of this project will be to develop emulator-driven calibration approaches which employ a batched Bayesian experimental design strategy to adaptively select both the parameters to simulate the model with and variables controlling model fidelity, with an objective of maximising information gain from the simulations within a limited computational budget. This will require creating joint statistical models of the model output, its numerical error and computational cost, and devising utility functions to optimize over which balance between information gain and computational cost.\n\n\n\nThe key output of the project will be a suite of open-source software tools allowing emulator-based calibration of expensive simulator models with adaptive control of both parameters simulations are run at and variables controlling model fidelity. A central aim will be to ensure the tools developed are suitable to be used at scale on high performance computing systems, for example allowing interacting with job schedulers to asynchronously dispatch simulations, using batched sequential design strategies that allow setting off multiple runs in parallel and minimizing communication overheads by performing as much processing in-situ on nodes as possible. Where possible the aim will be to build on (and contribute back to) existing open-source tools and packages.\nThe proposed supervisory team has worked on developing and implementing related methodology in a fusion-energy modelling context in collaboration with UKAEA as part of the uncertainty quantification efforts of the ExCALIBUR fusion use-case project, NEPTUNE, and this project would build upon this existing foundation, with prototype implementations in the packages calibr and emul"
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html#project-description",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html#project-description",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "",
    "text": "Bayesian methods for calibration, that is fitting the parameters of a model to data, often require a large number of model runs to adequately explore the model parameter space. This makes them infeasible to apply to computationally intensive simulations. A common approach in this setting is to fit a statistical emulator to a limited number of model evaluations and use this as a proxy for the full model in an inference algorithm (Kennedy and O’Hagan, 2001; Rasmussen, 2003).\nA key issue in such approaches is the choice of points in the parameter space at which to evaluate the model at to form the data used to fit the emulator. Use of Bayesian experimental design approaches, where the parameter values are chosen to minimize the expected future uncertainty about the model output at each step (Kandasamy, Schneider and Póczos, 2015; Sinsbeck and Nowak, 2017; Acerbi, 2018; Oliveira, Ott, and Ramos, 2021), offer a principled approach for maximising the information we get about a model from a limited number of evaluations, and can be extended to evaluate batches of points at each iteration (Järvenpää et al., 2021) improving the opportunity for parallelisation.\nEmulator-based calibration methods often treat the model as a monolithic black box, however, in reality complex simulators typically have tuneable variables allowing a trade off between numerical accuracy and computational cost. Multi-level Monte Carlo methods (Henrich, 2001; Giles, 2015) offer one approach for exploiting this trade off, combining simulations at multiple level of fidelity to reduce computational cost while controlling error. A recent related idea in the field of probabilistic numerics, is to statistically model how numerical error varies with fidelity (Teymur et al., 2021) and use this to probabilistically extrapolate the behaviour at high fidelity.\n\n\nThe aim of this project will be to develop emulator-driven calibration approaches which employ a batched Bayesian experimental design strategy to adaptively select both the parameters to simulate the model with and variables controlling model fidelity, with an objective of maximising information gain from the simulations within a limited computational budget. This will require creating joint statistical models of the model output, its numerical error and computational cost, and devising utility functions to optimize over which balance between information gain and computational cost.\n\n\n\nThe key output of the project will be a suite of open-source software tools allowing emulator-based calibration of expensive simulator models with adaptive control of both parameters simulations are run at and variables controlling model fidelity. A central aim will be to ensure the tools developed are suitable to be used at scale on high performance computing systems, for example allowing interacting with job schedulers to asynchronously dispatch simulations, using batched sequential design strategies that allow setting off multiple runs in parallel and minimizing communication overheads by performing as much processing in-situ on nodes as possible. Where possible the aim will be to build on (and contribute back to) existing open-source tools and packages.\nThe proposed supervisory team has worked on developing and implementing related methodology in a fusion-energy modelling context in collaboration with UKAEA as part of the uncertainty quantification efforts of the ExCALIBUR fusion use-case project, NEPTUNE, and this project would build upon this existing foundation, with prototype implementations in the packages calibr and emul"
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html#references",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html#references",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "References",
    "text": "References\n\nKennedy, M. C. and O’Hagan, A. (2001). “Bayesian calibration of computer models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3): 425–464. MR1858398. doi: https://doi.org/10.1111/1467-9868.00294.\nHeinrich, S. (2001). Multilevel Monte Carlo methods. In Large-Scale Scientific Computing: Third International Conference, LSSC 2001 (pp. 58-67). Springer Berlin Heidelberg.\nRasmussen, C. E. (2003). “Gaussian Processes to Speed up Hybrid Monte Carlo for Expensive Bayesian Integrals.” Bayesian Statistics 7, 651–659. MR2003529.\nKandasamy, K., Schneider, J., and Póczos, B. (2015). “Bayesian active learning for posterior estimation.” In International Joint Conference on Artificial Intelligence, 3605–3611.\nGiles, M. B. (2015). Multilevel Monte Carlo methods. Acta numerica, 24, 259-328.\nSinsbeck, M. and Nowak, W. (2017). “Sequential Design of Computer Experiments for the Solution of Bayesian Inverse Problems.” SIAM/ASA Journal on Uncertainty Quantification, 5(1): 640–664. MR3679325. doi: https://doi.org/10.1137/15M1047659.\nAcerbi, L. (2018). “Variational Bayesian Monte Carlo.” In Advances in Neural Information Processing Systems 31 , 8223–8233.\nJärvenpää, M., Gutmann, M. U., Vehtari, A., & Marttinen, P. (2021). Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian Analysis. 16(1), 147-178.\nOliveira, R., Ott, L., & Ramos, F. (2021). “No-regret approximate inference via Bayesian optimisation”. In Uncertainty in Artificial Intelligence (pp. 2082-2092). PMLR.\nTeymur, O., Foley, C., Breen, P., Karvonen, T., & Oates, C. J. (2021). Black box probabilistic numerics. Advances in Neural Information Processing Systems, 34, 23452-23464."
  },
  {
    "objectID": "phd_projects/entries/zhang_reinforcement_learning.html",
    "href": "phd_projects/entries/zhang_reinforcement_learning.html",
    "title": "Optimization methods for high-dimensional reinforcement learning",
    "section": "",
    "text": "This PhD project aims to develop efficient zero-order optimization methods for high-dimensional reinforcement learning problems. Reinforcement learning, which is crucial for decision-making tasks in fields such as robotics, finance, and artificial intelligence, often requires solving high-dimensional optimization problems where gradients are noisy or infeasible to compute. Zero-order methods, which rely solely on function evaluations rather than gradient information, are well-suited for these black-box optimization scenarios. However, existing zero-order optimization methods suffer from the curse of dimensionality. This project seeks to leverage recent advancements in high-dimensional statistics, probability, and optimization theory to develop scalable reinforcement learning methods for high-dimensional problems.\nAlthough various reinforcement learning and zero-order optimization algorithms have been proposed in the literature, they do not explicitly exploit the intrinsic low-dimensional structures that naturally arise in many large-scale decision-making problems. As a result, existing reinforcement learning algorithms struggle to scale effectively in high-dimensional settings, leading to inefficiencies. The development of efficient RL algorithms that leverage low-dimensional structures for high-dimensional problems, along with a rigorous performance analysis, remains largely unexplored in the current literature.\nThe principal supervisor’s group has focused on developing provably convergent reinforcement learning algorithms, with an emphasis on rigorous theoretical foundations to ensure reliable performance. These works have primarily been applied to low-dimensional settings. For high-dimensional problems, the wider research community has explored complex function approximation techniques, such as neural networks, to address the curse of dimensionality. Despite their promise, these approaches often lack strong theoretical guarantees, particularly in terms of convergence and robustness, which limits their applicability in high-stakes large-scale decision-making problems.\n\n\nThe main objective of this project is to leverage recent advances in high-dimensional statistics and probability theory to learn the intricate low-dimensional structures and develop more efficient, theoretically grounded algorithms for high-dimensional reinforcement learning problems. Specifically, the project aims to:\n• Develop novel zero-order optimization methods that exploit the intrinsic low-dimensional structure of high-dimensional decision-making problems. • Design reinforcement learning algorithms that can scale efficiently in high-dimensional environments while providing provable theoretical guarantees for convergence and performance. • Investigate the use of high-dimensional statistical techniques and mean-field approximation to improve the scalability and robustness of these algorithms. • Conduct a comprehensive performance analysis of the developed algorithms, comparing them with existing approaches in the literature, particularly in terms of scalability, convergence, and computational efficiency.\n\n\n\n\nZero-Order Optimization Library: A software package implementing novel zero-order optimization methods, designed to efficiently scale in high-dimensional decision-making problems.\nReinforcement Learning Algorithms: A suite of scalable RL algorithms that exploit the intrinsic low-dimensional structure of high-dimensional problems and leverage advanced statistical techniques, with provable convergence guarantees.\nAlgorithm Implementation: Development of zero-order optimization and RL algorithms in Python, focusing on scalability for high-dimensional problems.\nOptimization Library: Creation of a flexible, scalable zero-order optimization library that integrates easily into existing machine learning frameworks.\nData Simulations: Generation of synthetic and real-world data (e.g., finance, robotics) for testing and validating the algorithms.\nPerformance Benchmarking: Development of tools to benchmark algorithm performance, focusing on scalability, efficiency, and convergence rates.\nOpen-Source Release: All code will be open-sourced with comprehensive documentation for ease of use and reproducibility by the research community."
  },
  {
    "objectID": "phd_projects/entries/zhang_reinforcement_learning.html#project-description",
    "href": "phd_projects/entries/zhang_reinforcement_learning.html#project-description",
    "title": "Optimization methods for high-dimensional reinforcement learning",
    "section": "",
    "text": "This PhD project aims to develop efficient zero-order optimization methods for high-dimensional reinforcement learning problems. Reinforcement learning, which is crucial for decision-making tasks in fields such as robotics, finance, and artificial intelligence, often requires solving high-dimensional optimization problems where gradients are noisy or infeasible to compute. Zero-order methods, which rely solely on function evaluations rather than gradient information, are well-suited for these black-box optimization scenarios. However, existing zero-order optimization methods suffer from the curse of dimensionality. This project seeks to leverage recent advancements in high-dimensional statistics, probability, and optimization theory to develop scalable reinforcement learning methods for high-dimensional problems.\nAlthough various reinforcement learning and zero-order optimization algorithms have been proposed in the literature, they do not explicitly exploit the intrinsic low-dimensional structures that naturally arise in many large-scale decision-making problems. As a result, existing reinforcement learning algorithms struggle to scale effectively in high-dimensional settings, leading to inefficiencies. The development of efficient RL algorithms that leverage low-dimensional structures for high-dimensional problems, along with a rigorous performance analysis, remains largely unexplored in the current literature.\nThe principal supervisor’s group has focused on developing provably convergent reinforcement learning algorithms, with an emphasis on rigorous theoretical foundations to ensure reliable performance. These works have primarily been applied to low-dimensional settings. For high-dimensional problems, the wider research community has explored complex function approximation techniques, such as neural networks, to address the curse of dimensionality. Despite their promise, these approaches often lack strong theoretical guarantees, particularly in terms of convergence and robustness, which limits their applicability in high-stakes large-scale decision-making problems.\n\n\nThe main objective of this project is to leverage recent advances in high-dimensional statistics and probability theory to learn the intricate low-dimensional structures and develop more efficient, theoretically grounded algorithms for high-dimensional reinforcement learning problems. Specifically, the project aims to:\n• Develop novel zero-order optimization methods that exploit the intrinsic low-dimensional structure of high-dimensional decision-making problems. • Design reinforcement learning algorithms that can scale efficiently in high-dimensional environments while providing provable theoretical guarantees for convergence and performance. • Investigate the use of high-dimensional statistical techniques and mean-field approximation to improve the scalability and robustness of these algorithms. • Conduct a comprehensive performance analysis of the developed algorithms, comparing them with existing approaches in the literature, particularly in terms of scalability, convergence, and computational efficiency.\n\n\n\n\nZero-Order Optimization Library: A software package implementing novel zero-order optimization methods, designed to efficiently scale in high-dimensional decision-making problems.\nReinforcement Learning Algorithms: A suite of scalable RL algorithms that exploit the intrinsic low-dimensional structure of high-dimensional problems and leverage advanced statistical techniques, with provable convergence guarantees.\nAlgorithm Implementation: Development of zero-order optimization and RL algorithms in Python, focusing on scalability for high-dimensional problems.\nOptimization Library: Creation of a flexible, scalable zero-order optimization library that integrates easily into existing machine learning frameworks.\nData Simulations: Generation of synthetic and real-world data (e.g., finance, robotics) for testing and validating the algorithms.\nPerformance Benchmarking: Development of tools to benchmark algorithm performance, focusing on scalability, efficiency, and convergence rates.\nOpen-Source Release: All code will be open-sourced with comprehensive documentation for ease of use and reproducibility by the research community."
  },
  {
    "objectID": "phd_projects/entries/Karin_tissue.html",
    "href": "phd_projects/entries/Karin_tissue.html",
    "title": "Agent-based simulation of tissue self-organization",
    "section": "",
    "text": "The project aims to develop a computational framework for agent-based simulations of tissue dynamics, encompassing self-organization and disease dysregulation. The primary goal is to create a software tool that facilitates the development of interpretable and predictive mechanistic simulations of the dynamic interaction network of cells within tissues. This tool will focus on how these interactions balance growth, removal, and differentiation. Understanding these processes is essential for studying healthy tissue function and the development of diseases such as cancer.\n\n\nThe framework will be based on the group’s work on effective mathematical models for cell fate decisions.\n[1] Simons BD, Karin O. Tuning of plasma cell lifespan by competition explains the longevity and heterogeneity of antibody persistence. Immunity. 2024 Mar 12;57(3):600-11.\n[2] Karin O. EnhancerNet: A predictive model of cell identity dynamics through enhancer selection. Development. 2024 Sep 17:dev-202997.\n\n\n\nSpecific applications may involve modelling stem cell dynamics in adult tissues or during embryonic development and could include collaboration with experimental groups. The project will be organized gradually, with early deliverables focusing on more limited modeling contexts. Throughout the project, the modelling framework will be tested against specific biological applications. The outcome will be an open-source software package that supports the modelling of complex self-organization in biological tissues.\n\n\n\nFrom a software development perspective, the project will utilize Python along with numerical computation libraries such as NumPy and SciPy and support the highly nonlinear nature of the underlying dynamics and possibly a large number of agents (cells). We aim to make the code as general as possible to facilitate the investigation of complex self-organizing behaviours, such as spatio-temporal patterning or mutant invasion."
  },
  {
    "objectID": "phd_projects/entries/Karin_tissue.html#project-description",
    "href": "phd_projects/entries/Karin_tissue.html#project-description",
    "title": "Agent-based simulation of tissue self-organization",
    "section": "",
    "text": "The project aims to develop a computational framework for agent-based simulations of tissue dynamics, encompassing self-organization and disease dysregulation. The primary goal is to create a software tool that facilitates the development of interpretable and predictive mechanistic simulations of the dynamic interaction network of cells within tissues. This tool will focus on how these interactions balance growth, removal, and differentiation. Understanding these processes is essential for studying healthy tissue function and the development of diseases such as cancer.\n\n\nThe framework will be based on the group’s work on effective mathematical models for cell fate decisions.\n[1] Simons BD, Karin O. Tuning of plasma cell lifespan by competition explains the longevity and heterogeneity of antibody persistence. Immunity. 2024 Mar 12;57(3):600-11.\n[2] Karin O. EnhancerNet: A predictive model of cell identity dynamics through enhancer selection. Development. 2024 Sep 17:dev-202997.\n\n\n\nSpecific applications may involve modelling stem cell dynamics in adult tissues or during embryonic development and could include collaboration with experimental groups. The project will be organized gradually, with early deliverables focusing on more limited modeling contexts. Throughout the project, the modelling framework will be tested against specific biological applications. The outcome will be an open-source software package that supports the modelling of complex self-organization in biological tissues.\n\n\n\nFrom a software development perspective, the project will utilize Python along with numerical computation libraries such as NumPy and SciPy and support the highly nonlinear nature of the underlying dynamics and possibly a large number of agents (cells). We aim to make the code as general as possible to facilitate the investigation of complex self-organizing behaviours, such as spatio-temporal patterning or mutant invasion."
  },
  {
    "objectID": "phd_projects/entries/duncan_kalise_UKAEA.html",
    "href": "phd_projects/entries/duncan_kalise_UKAEA.html",
    "title": "Robust Multi-objective and Mutli-fidelity Bayesian Optimisation of Fusion Breeder Blankets",
    "section": "",
    "text": "Within many fusion power plant concepts, breeder blankets fulfil a multi-functional role. Neutrons emitted in deuterium-tritium fusion reactions must be absorbed, in part to reduce damaging irradiation to other essential components (such as superconducting magnets) and outer structural material. The neutrons’ energy must be converted to heat; the heat must be transported by a coolant where it will be ultimately employed to drive conventional steam-based turbines for generation of electricity. Some neutrons must also participate in reactions to breed tritium to sustain fusion reactions. The tritium must be extracted at a rate that supports plant availability. Structural material must remain within safe operational conditions, while significant pressure drops in the coolant must be minimised to optimise power plant efficiency. Materials suitable for each of these functions are usually not interchangeable and thus there are necessarily trade-offs in the spatial allocation for the sub-components responsible for each function. This domain is therefore highly suitable for a multi-objective optimisation over geometric parameters.\nSimulation of the breeder blanket must necessarily span multiple physics domains, minimally including neutronics, heat transfer and computational fluid dynamics (CFD). Depending on the level of fidelity selected for CFD - which may range from 1D correlations, reduced order turbulence models, through to direct numerical simulations - the computational cost for evaluating a single design point can vary enormously. A lack of fidelity may conceal emergent phenomenon; such effects have already been noted in reference [1] where local hotspots were observed only with higher fidelity models. Meanwhile too much fidelity may render unviable any optimisation algorithm. We propose a multi-fidelity approach, where inexpensive low-fidelity simulations inform the query to expensive high-fidelity simulations towards the global optimum.\nBesides setting the appropriate level of fidelity, there are other considerations in ensuring robustness to the optimisation procedure. The limited tolerances in the manufacturing process imply an uncertainty in the material composition and inevitable small differences in the component shape compared to the simulated one. Furthermore, the empirical nuclide cross-section data for those materials (which are inputs to the neutron transport calculations) are often associated with significant uncertainties. As such, the optimisation procedure should select configurations that are stable under small perturbations in both parameter space and in the input nuclear data.\nBayesian Optimisation provides a theoretically solid foundation to perform sample-efficient maximisation of a noisy black-box function, with guaranteed convergence rates. In this project, the efficacy of Bayesian optimisation methods applied to breeder blankets will be explored.\n\n\nRecent years have seen the development of scalable and open-source multi-physics software that is suitable for the modelling of fusion components. In particular, the MOOSE framework [2] offers a flexible platform for the solution of arbitrary partial differential equations using the finite element method, with many built-in physics modules as well as couplings to specialist tools for Monte Carlo neutron transport (OpenMC) [3,4] and spectral element computational fluid dynamics (NekRS) [5]. MOOSE applications have been used to simulate a variety of breeder blanket concept designs, including both solid ceramic and liquid lead-lithium breeders [6,7,8].\nParametric optimisation studies involving MOOSE for analysis have already been performed for the simple case of a divertor monoblock component [9]. While such studies are indicative of methodology, a limiting factor in pursuing a similar approach to breeder blankets until recently was the existence of a suitable tool to generate geometry. However, a blanket geometry engine, Hypnos, has recently been developed, with an initial demonstration of the software indicating that geometric optimisation is now possible [10]. With a proven tool-chain already in place for analysis, the primary area for innovation would pertain to the development and deployment of the optimisation algorithms themselves, with a particular focus on the robustness of the outcomes.\nIn decision problems arising from industrial processes, the design parameters are often subject to uncertainty. This uncertainty can stem from limited data observability, noisy measurements, implementation challenges, or prediction errors. In the context of manufacturing, this uncertainty may arise from manufacturing tolerances, and material imperfections. Stochastic Optimsation (SO) methods have classically allowed to model this uncertainty within a decision-making framework, assuming that the decision maker has complete knowledge about the underlying uncertainty through a known probability distribution. On the other hand, in robust optimisation it is assumed that the decision maker has only minimal distributional knowledge about the underlying uncertainty, and the optimiser seeks to minimise the worst-case outcome over an uncertainty set. This has been extended to the multi-objective optimisation case in several works, where one identifies a pareto front of candidate solutions which are favourable against a set of distinct objectives. Motivated by practical limitations due to manufacturing tolerances, in [11] a robust multi-objective optimisation approach known as constraint active search (CAS) is proposed, which aims to identify diverse solutions in the region of the search space that exceeds a minimum threshold on the objectives, leveraging a post-hoc sensitivity analysis process to assess the robustness of candidate points under input noise [12].\nApproaching robust design by decoupling data collection and sensitivity analysis is central to the Taguchi method [13]. Data acquisition often revolves around finding designs that balance the mean and variance of the sensitive objective under input noise. Daulton et al [14] extended the Bayesian optimisation framework to black-box multi-objective optimisation problems, identifying a pareto front of candidate solutions with associated robustness guarantees. Integration of these approaches with large-scale simulation frameworks remains a challenge. In [14] the authors derive a new framework for batch-based black-box optimisation which enables the effective use of parallel simulations to obtain high-quality optimisation candidates. This was subsequently applied in the context of calibrating JOREK, and MHD simulator for fusion in [15]. In [16] the authors explore another optimisation-centric decision problem, namely design of experiments for optimal sensor placement, which is able to exploit multi-resolution simulation data through resolution-invariant learning methods.\n[1] F. A. Hernández et al. (2019) ‘Advancements in the Helium-Cooled Pebble Bed Breeding Blanket for the EU DEMO: Holistic Design Approach and Lessons Learned’, Fusion Science and Technology, 75(5), pp. 352–364. doi: 10.1080/15361055.2019.1607695\n[2] C. J. Permann et al. (2020) ‘MOOSE: Enabling massively parallel multiphysics simulation’, SoftwareX, 11 (100430), doi: 10.1016/j.softx.2020.100430\n[3] H. Brooks et al (2022), ‘Scalable multi-physics for fusion reactors with AURORA’, Plasma Physics and Controlled Fusion, 65 (2), 024002, doi: 10.1088/1361-6587/aca998\n[4] A.J. Novak et al (2024), ‘Monte Carlo multiphysics simulation on adaptive unstructured mesh geometry’, Nuclear Engineering and Design, 429 (113589), doi: 10.1016/j.nucengdes.2024.113589.\n[5] A.J. Novak et al (2022), ‘Coupled Monte Carlo and thermal-fluid modeling of high temperature gas reactors using Cardinal’, Annals of Nuclear Energy, 177 (109310), doi: 10.1016/j.anucene.2022.109310.\n[6] H. Brooks et al (2022), ‘Towards multiphysics simulations of fusion breeder blankets’, Int. Conf. on Physics of Reactors 2022 (American Nuclear Society) pp 2480–9\n[7] A. Novak et al (2023) ‘Multiphysics Coupling of OpenMC CAD-Based Transport to MOOSE using Cardinal and Aurora’, The International Conference on Mathematics and Computational Methods Applied to Nuclear Science and Engineering (M&C 2023)\n[8] F. Kong et al (2022), “Toward a Fully Integrated Multiphysics Simulation Framework for Fusion Blanket Design,” IEEE Transactions on Plasma Science, 50 (11) pp. 4446-4452, Nov. 2022, doi: 10.1109/TPS.2022.3173158\n[9] L. R. Humphrey et al (2024), ‘Machine learning techniques for sequential learning engineering design optimisation’, Plasma Physics and Controlled Fusion, 66 (025002), DOI 10.1088/1361-6587/ad11fb\n[10] H. Brooks et al (2024), ‘An Open-Source Digital Engineering Pipeline for Enabling In-silico Design and Qualification of Tritium Breeding Devices’, 26th Technology of Fusion Energy Meeting (TOFE 2024)\n[11] Malkomes, Gustavo, et al. “Beyond the pareto efficient frontier: Constraint active search for multiobjective experimental design.” International Conference on Machine Learning. PMLR, 2021.\n[12] Calandra, Roberto, Jan Peters, and M. P. Deisenrothy. “Pareto front modeling for sensitivity analysis in multi-objective bayesian optimization.” NIPS Workshop on Bayesian Optimization. Vol. 5. 2014.\n[13] Taguchi, Genichi. Introduction to quality engineering: designing quality into products and processes. 1986.\n[14] Daulton, Samuel, et al. “Robust multi-objective bayesian optimization under input noise.” International Conference on Machine Learning. PMLR, 2022.\n[14] Crovini, Enrico, et al. “Batch Bayesian optimization via particle gradient Flows.” arXiv preprint arXiv:2209.04722 (2022).\n[15] Crovini, E., et al. “Automatic JOREK calibration via batch Bayesian optimization.” Physics of Plasmas 31.6 (2024).\n[16] Cordero-Encinar, Paula, et al. “Deep Optimal Sensor Placement for Black Box Stochastic Simulations.” arXiv preprint arXiv:2410.12036 (2024).\n[17] K. Kandasamy et al. 2016, “Multi-fidelity bayesian optimization with continuous approximations”, arXiv:1703.06240\n[18] M. A. Gelbart et al. 2014, “Bayesian Optimisation with unknown constraints”, arXiv:1403.5607\n[19] R. Oliveira et al. 2019, “Bayesian optimisation under uncertain inputs”, arXiv:1902.07908\n\n\n\nThe project will initially consider the helium-cooled pebble bed (HCPB) breeder blanket. This concept is already well-established; having been considered by EU-DEMO during the preconceptual phase (2014-2020) it is being actively pursued in the conceptual design phase (2021-2027). Therefore, the design is at a suitable level of readiness for further optimisation, with high potential for impact should the methodologies prove successful. The optimisation will be performed in a multi-fidelity fashion (e.g. [17]), with the competing objectives of minimising pressure drop, maximising heat transfer and maximising tritium breeding ratio (TBR) subject to operational constraints (e.g. [18]). The result of the procedure should be robust to uncertainties arising from limitations in manufacturing as well as from nuclear data libraries, (e.g. [19]).\n\n\n\nThe student will leverage the multi-physics simulation engine for Bayesian Optimisation studies. A separate GitHub repository will be created, aimed at providing a general purpose framework for relevant black-box optimisation tasks. This will include baselines based on existing packages, for example the widely popular BoTorch package, as well as novel algorithms developed within the duration of the studies. Due to the ubiquity of this problem, we envisage that this framework will be valuable to decision makers across a wide range of sectors."
  },
  {
    "objectID": "phd_projects/entries/duncan_kalise_UKAEA.html#project-description",
    "href": "phd_projects/entries/duncan_kalise_UKAEA.html#project-description",
    "title": "Robust Multi-objective and Mutli-fidelity Bayesian Optimisation of Fusion Breeder Blankets",
    "section": "",
    "text": "Within many fusion power plant concepts, breeder blankets fulfil a multi-functional role. Neutrons emitted in deuterium-tritium fusion reactions must be absorbed, in part to reduce damaging irradiation to other essential components (such as superconducting magnets) and outer structural material. The neutrons’ energy must be converted to heat; the heat must be transported by a coolant where it will be ultimately employed to drive conventional steam-based turbines for generation of electricity. Some neutrons must also participate in reactions to breed tritium to sustain fusion reactions. The tritium must be extracted at a rate that supports plant availability. Structural material must remain within safe operational conditions, while significant pressure drops in the coolant must be minimised to optimise power plant efficiency. Materials suitable for each of these functions are usually not interchangeable and thus there are necessarily trade-offs in the spatial allocation for the sub-components responsible for each function. This domain is therefore highly suitable for a multi-objective optimisation over geometric parameters.\nSimulation of the breeder blanket must necessarily span multiple physics domains, minimally including neutronics, heat transfer and computational fluid dynamics (CFD). Depending on the level of fidelity selected for CFD - which may range from 1D correlations, reduced order turbulence models, through to direct numerical simulations - the computational cost for evaluating a single design point can vary enormously. A lack of fidelity may conceal emergent phenomenon; such effects have already been noted in reference [1] where local hotspots were observed only with higher fidelity models. Meanwhile too much fidelity may render unviable any optimisation algorithm. We propose a multi-fidelity approach, where inexpensive low-fidelity simulations inform the query to expensive high-fidelity simulations towards the global optimum.\nBesides setting the appropriate level of fidelity, there are other considerations in ensuring robustness to the optimisation procedure. The limited tolerances in the manufacturing process imply an uncertainty in the material composition and inevitable small differences in the component shape compared to the simulated one. Furthermore, the empirical nuclide cross-section data for those materials (which are inputs to the neutron transport calculations) are often associated with significant uncertainties. As such, the optimisation procedure should select configurations that are stable under small perturbations in both parameter space and in the input nuclear data.\nBayesian Optimisation provides a theoretically solid foundation to perform sample-efficient maximisation of a noisy black-box function, with guaranteed convergence rates. In this project, the efficacy of Bayesian optimisation methods applied to breeder blankets will be explored.\n\n\nRecent years have seen the development of scalable and open-source multi-physics software that is suitable for the modelling of fusion components. In particular, the MOOSE framework [2] offers a flexible platform for the solution of arbitrary partial differential equations using the finite element method, with many built-in physics modules as well as couplings to specialist tools for Monte Carlo neutron transport (OpenMC) [3,4] and spectral element computational fluid dynamics (NekRS) [5]. MOOSE applications have been used to simulate a variety of breeder blanket concept designs, including both solid ceramic and liquid lead-lithium breeders [6,7,8].\nParametric optimisation studies involving MOOSE for analysis have already been performed for the simple case of a divertor monoblock component [9]. While such studies are indicative of methodology, a limiting factor in pursuing a similar approach to breeder blankets until recently was the existence of a suitable tool to generate geometry. However, a blanket geometry engine, Hypnos, has recently been developed, with an initial demonstration of the software indicating that geometric optimisation is now possible [10]. With a proven tool-chain already in place for analysis, the primary area for innovation would pertain to the development and deployment of the optimisation algorithms themselves, with a particular focus on the robustness of the outcomes.\nIn decision problems arising from industrial processes, the design parameters are often subject to uncertainty. This uncertainty can stem from limited data observability, noisy measurements, implementation challenges, or prediction errors. In the context of manufacturing, this uncertainty may arise from manufacturing tolerances, and material imperfections. Stochastic Optimsation (SO) methods have classically allowed to model this uncertainty within a decision-making framework, assuming that the decision maker has complete knowledge about the underlying uncertainty through a known probability distribution. On the other hand, in robust optimisation it is assumed that the decision maker has only minimal distributional knowledge about the underlying uncertainty, and the optimiser seeks to minimise the worst-case outcome over an uncertainty set. This has been extended to the multi-objective optimisation case in several works, where one identifies a pareto front of candidate solutions which are favourable against a set of distinct objectives. Motivated by practical limitations due to manufacturing tolerances, in [11] a robust multi-objective optimisation approach known as constraint active search (CAS) is proposed, which aims to identify diverse solutions in the region of the search space that exceeds a minimum threshold on the objectives, leveraging a post-hoc sensitivity analysis process to assess the robustness of candidate points under input noise [12].\nApproaching robust design by decoupling data collection and sensitivity analysis is central to the Taguchi method [13]. Data acquisition often revolves around finding designs that balance the mean and variance of the sensitive objective under input noise. Daulton et al [14] extended the Bayesian optimisation framework to black-box multi-objective optimisation problems, identifying a pareto front of candidate solutions with associated robustness guarantees. Integration of these approaches with large-scale simulation frameworks remains a challenge. In [14] the authors derive a new framework for batch-based black-box optimisation which enables the effective use of parallel simulations to obtain high-quality optimisation candidates. This was subsequently applied in the context of calibrating JOREK, and MHD simulator for fusion in [15]. In [16] the authors explore another optimisation-centric decision problem, namely design of experiments for optimal sensor placement, which is able to exploit multi-resolution simulation data through resolution-invariant learning methods.\n[1] F. A. Hernández et al. (2019) ‘Advancements in the Helium-Cooled Pebble Bed Breeding Blanket for the EU DEMO: Holistic Design Approach and Lessons Learned’, Fusion Science and Technology, 75(5), pp. 352–364. doi: 10.1080/15361055.2019.1607695\n[2] C. J. Permann et al. (2020) ‘MOOSE: Enabling massively parallel multiphysics simulation’, SoftwareX, 11 (100430), doi: 10.1016/j.softx.2020.100430\n[3] H. Brooks et al (2022), ‘Scalable multi-physics for fusion reactors with AURORA’, Plasma Physics and Controlled Fusion, 65 (2), 024002, doi: 10.1088/1361-6587/aca998\n[4] A.J. Novak et al (2024), ‘Monte Carlo multiphysics simulation on adaptive unstructured mesh geometry’, Nuclear Engineering and Design, 429 (113589), doi: 10.1016/j.nucengdes.2024.113589.\n[5] A.J. Novak et al (2022), ‘Coupled Monte Carlo and thermal-fluid modeling of high temperature gas reactors using Cardinal’, Annals of Nuclear Energy, 177 (109310), doi: 10.1016/j.anucene.2022.109310.\n[6] H. Brooks et al (2022), ‘Towards multiphysics simulations of fusion breeder blankets’, Int. Conf. on Physics of Reactors 2022 (American Nuclear Society) pp 2480–9\n[7] A. Novak et al (2023) ‘Multiphysics Coupling of OpenMC CAD-Based Transport to MOOSE using Cardinal and Aurora’, The International Conference on Mathematics and Computational Methods Applied to Nuclear Science and Engineering (M&C 2023)\n[8] F. Kong et al (2022), “Toward a Fully Integrated Multiphysics Simulation Framework for Fusion Blanket Design,” IEEE Transactions on Plasma Science, 50 (11) pp. 4446-4452, Nov. 2022, doi: 10.1109/TPS.2022.3173158\n[9] L. R. Humphrey et al (2024), ‘Machine learning techniques for sequential learning engineering design optimisation’, Plasma Physics and Controlled Fusion, 66 (025002), DOI 10.1088/1361-6587/ad11fb\n[10] H. Brooks et al (2024), ‘An Open-Source Digital Engineering Pipeline for Enabling In-silico Design and Qualification of Tritium Breeding Devices’, 26th Technology of Fusion Energy Meeting (TOFE 2024)\n[11] Malkomes, Gustavo, et al. “Beyond the pareto efficient frontier: Constraint active search for multiobjective experimental design.” International Conference on Machine Learning. PMLR, 2021.\n[12] Calandra, Roberto, Jan Peters, and M. P. Deisenrothy. “Pareto front modeling for sensitivity analysis in multi-objective bayesian optimization.” NIPS Workshop on Bayesian Optimization. Vol. 5. 2014.\n[13] Taguchi, Genichi. Introduction to quality engineering: designing quality into products and processes. 1986.\n[14] Daulton, Samuel, et al. “Robust multi-objective bayesian optimization under input noise.” International Conference on Machine Learning. PMLR, 2022.\n[14] Crovini, Enrico, et al. “Batch Bayesian optimization via particle gradient Flows.” arXiv preprint arXiv:2209.04722 (2022).\n[15] Crovini, E., et al. “Automatic JOREK calibration via batch Bayesian optimization.” Physics of Plasmas 31.6 (2024).\n[16] Cordero-Encinar, Paula, et al. “Deep Optimal Sensor Placement for Black Box Stochastic Simulations.” arXiv preprint arXiv:2410.12036 (2024).\n[17] K. Kandasamy et al. 2016, “Multi-fidelity bayesian optimization with continuous approximations”, arXiv:1703.06240\n[18] M. A. Gelbart et al. 2014, “Bayesian Optimisation with unknown constraints”, arXiv:1403.5607\n[19] R. Oliveira et al. 2019, “Bayesian optimisation under uncertain inputs”, arXiv:1902.07908\n\n\n\nThe project will initially consider the helium-cooled pebble bed (HCPB) breeder blanket. This concept is already well-established; having been considered by EU-DEMO during the preconceptual phase (2014-2020) it is being actively pursued in the conceptual design phase (2021-2027). Therefore, the design is at a suitable level of readiness for further optimisation, with high potential for impact should the methodologies prove successful. The optimisation will be performed in a multi-fidelity fashion (e.g. [17]), with the competing objectives of minimising pressure drop, maximising heat transfer and maximising tritium breeding ratio (TBR) subject to operational constraints (e.g. [18]). The result of the procedure should be robust to uncertainties arising from limitations in manufacturing as well as from nuclear data libraries, (e.g. [19]).\n\n\n\nThe student will leverage the multi-physics simulation engine for Bayesian Optimisation studies. A separate GitHub repository will be created, aimed at providing a general purpose framework for relevant black-box optimisation tasks. This will include baselines based on existing packages, for example the widely popular BoTorch package, as well as novel algorithms developed within the duration of the studies. Due to the ubiquity of this problem, we envisage that this framework will be valuable to decision makers across a wide range of sectors."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html",
    "href": "phd_projects/entries/gutschow.html",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "",
    "text": "The Large Hadron Collider (LHC) has generated an immense amount of data, which, when combined with advanced Standard Model (SM) predictions, presents new opportunities for discovering physics beyond the SM. Current analysis frameworks, such as Rivet, allow for data-theory comparisons, but incorporating higher-order calculations (NLO, NNLO) into these comparisons remains challenging. Additionally, there is potential to enhance traditional statistical analyses with machine learning-based anomaly detection to identify mismodelling or uncover hints of new physics. This project aims to improve both the affordability and accuracy of SM calculations and apply them to large-scale metadata analysis of collider data."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#existing-background-work",
    "href": "phd_projects/entries/gutschow.html#existing-background-work",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "",
    "text": "The Large Hadron Collider (LHC) has generated an immense amount of data, which, when combined with advanced Standard Model (SM) predictions, presents new opportunities for discovering physics beyond the SM. Current analysis frameworks, such as Rivet, allow for data-theory comparisons, but incorporating higher-order calculations (NLO, NNLO) into these comparisons remains challenging. Additionally, there is potential to enhance traditional statistical analyses with machine learning-based anomaly detection to identify mismodelling or uncover hints of new physics. This project aims to improve both the affordability and accuracy of SM calculations and apply them to large-scale metadata analysis of collider data."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/gutschow.html#main-objectives-of-the-project",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\n\nOptimising SM Calculations: The project seeks to make state-of-the-art SM predictions computationally affordable and accessible for large-scale collider data analysis. This will be achieved through performance profiling of existing tools and integrating higher-order calculations into Monte Carlo (MC) particle-level predictions using reweighting techniques.\nComprehensive Metadata Analysis: The student will perform large-scale metadata analysis using Rivet, comparing experimental data from LHC experiments like ATLAS and CMS with optimised SM predictions. P-value distributions will be generated to quantify the level of agreement between theory and data across multiple observables.\nAnomaly Detection Framework: Machine learning algorithms will be applied to p-value distributions to detect anomalies in data-theory comparisons. This will enhance sensitivity in identifying potential Monte Carlo mismodelling or uncovering signals of new physics."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/gutschow.html#details-of-softwaredata-deliverables",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\n\nPerformance-Optimised Prediction Tools: The student will profile existing SM tools (e.g. Sherpa, Herwig) to identify and address computational bottlenecks, optimising them for large-scale collider data comparisons.\nIntegration of Higher-Order Calculations: Incorporation of NLO and NNLO corrections into MC particle-level predictions through reweighting techniques, making these sophisticated calculations accessible for widespread use.\nData-Theory Comparison Framework: A Rivet-based framework for generating detailed p-value distributions, which will provide a robust statistical foundation for comparing experimental data with SM predictions.\nMachine Learning-Based Anomaly Detection: Development of a machine learning-enhanced system to detect inconsistencies in data-theory comparisons, using unsupervised learning techniques to identify potential new physics signals or MC mismodelling."
  },
  {
    "objectID": "phd_projects/phd_project_list.html",
    "href": "phd_projects/phd_project_list.html",
    "title": "PhD Projects",
    "section": "",
    "text": "The following projects are advertised for 2025 entry into the CDT.\n\n\n\n        \n            \n            \n                Domain-specific Compiler Technology for Finite Element Simulation on Tensor Hardware\n                Imperial: Mathematics or Computing\n                \n                Principal advisor: Prof Paul Kelly and Prof David Ham\n            \n                \n        \n\n        \n            \n            \n                Stochastic resetting in many-body interacting particle systems\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Thibault Bertrand and Prof. Paul Bressloff\n            \n                \n        \n\n        \n            \n            \n                Accumulation and absorption of active particles at surfaces\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Thibault Bertrand and Prof. Paul Bressloff\n            \n                \n        \n\n        \n            \n            \n                Out-of-distribution detection with long-tailed learning and vision-language models\n                UCL: Statistics\n                \n                Principal advisor: Prof Jinghao Xue\n            \n                \n        \n\n        \n            \n            \n                Robust Multi-objective and Mutli-fidelity Bayesian Optimisation of Fusion Breeder Blankets\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Andrew Duncan (with cosupervision from Dr Lorenzo Zanisi and Dr Helen Brooks at UKAEA)\n            \n                \n        \n\n        \n            \n            \n                Particles for Fusion\n                Imperial: Mathematics\n                \n                Principal advisor: Prof David Ham (with UKAEA cosupervision)\n            \n                \n        \n\n        \n            \n            \n                Computational modelling for educational research\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Andreas Joergensen (with Prof Camille Kandiko Howson (CHERS))\n            \n                \n        \n\n        \n            \n            \n                Numerical methods for Mean Field Games\n                UCL: Mathematics\n                \n                Principal advisor: Dr Iain Smears\n            \n                \n        \n\n        \n            \n            \n                Constrained Generative Models for Optimisation and Scientific Modelling\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Deniz Akyildiz\n            \n                \n        \n\n        \n            \n            \n                Taming Time and Dimension: Advanced Scientific Computing for Next-Generation Diffusion Models\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Dante Kalise Prof Greg Pavliotis \n            \n                \n        \n\n        \n            \n            \n                The propagation of uncertainties across coupled climate models\n                UCL: Statistics\n                \n                Principal advisor: Prof Serge Guillas\n            \n                \n        \n\n        \n            \n            \n                Smart Image-Based Sensor Optimisation for Fusion Simulation Validation\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Andrew Duncan, Dr Dante Kalise and Lloyd Fletcher (UKAEA)\n            \n                \n        \n\n        \n            \n            \n                Single Cell Ageing and mtDNA: learning and simulation\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Nick Jones\n            \n                \n        \n\n        \n            \n            \n                Mathematical and Computational Modeling of Resilience in Multilayer Networks\n                Imperial: Computing\n                \n                Principal advisor: Dr. Giuliano Casale and Prof. Emil Lupu\n            \n                \n        \n\n        \n            \n            \n                UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection\n                : Advanced Research Computing\n                \n                Principal advisor: Christian Gutschow\n            \n                \n        \n\n        \n            \n            \n                Optimal control methods for agent-based models\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Greg Pavliotis and Dr Dante Kalise\n            \n                \n        \n\n        \n            \n            \n                Aligned diffusion models for machine learning\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Felipe Tobar\n            \n                \n        \n\n        \n            \n            \n                Deep learning with symmetry\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Jeroen Lamb\n            \n                \n        \n\n        \n            \n            \n                Differentiable probabilistic deep learning with generative denoising diffusion models\n                UCL: MSSL\n                \n                Principal advisor: Prof Jason McEwen\n            \n                \n        \n\n        \n            \n            \n                Next generation implicit numerics for atmosphere models\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Colin Cotter\n            \n                \n        \n\n        \n            \n            \n                Optimization methods for high-dimensional reinforcement learning\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Yufei Zhang\n            \n                \n        \n\n        \n            \n            \n                Large-scale high-performance solver for therapeutic ultrasound applications\n                UCL: Division of Surgery and Interventional Science\n                \n                Principal advisor: Dr Pierre Gélat\n            \n                \n        \n\n        \n            \n            \n                Developing multi-physics, multi-scale wave modelling on graphics cards\n                Imperial: Mechanical Engineering\n                \n                Principal advisor: Dr Peter Huthwaite\n            \n                \n        \n\n        \n            \n            \n                Backpropagation through rough differential equations\n                Imperial: Mathematics & Imperial-X\n                \n                Principal advisor: Dr Christopher Salvi\n            \n                \n        \n\n        \n            \n            \n                High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials\n                UCL: Mechanical Engineering\n                \n                Principal advisor: Dr Reza Haqshenas\n            \n                \n        \n\n        \n            \n            \n                Renormalisation Group Perspective on Neural Networks\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Thibaut Bertrand and Dr Gunnar Pruessner\n            \n                \n        \n\n        \n            \n            \n                Optimising Experimental Design for Nuclear Materials Research Using Foundation Models\n                Imperial: Mathematics and Materials\n                \n                Principal advisor: Dr Sam Cooper and Dr Felipe Tobar\n            \n                \n        \n\n        \n            \n            \n                Model correlation and the version control tree\n                UCL: ARC\n                \n                Principal advisor: Prof James Hetherington\n            \n                \n        \n\n        \n            \n            \n                AI code generation and numerical codes\n                UCL: ARC\n                \n                Principal advisor: Prof James Hetherington\n            \n                \n        \n\n        \n            \n            \n                Inference and inverse problems for stochastic interacting particle systems\n                Imperial: Mathemetics\n                \n                Principal advisor: Prof Greg A. Pavliotis\n            \n                \n        \n\n        \n            \n            \n                All-at-once deep learning methods for nonlinear PDE based inverse problems\n                UCL: Computer Science\n                \n                Principal advisor: Prof Marta Betcke\n            \n                \n        \n\n        \n            \n            \n                Agent-based simulation of tissue self-organization\n                Imperial: Mathematics\n                \n                Principal advisor: Dr.Omer Karin\n            \n                \n        \n\n        \n            \n            \n                Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach\n                UCL: Mathematics\n                \n                Principal advisor: Prof Hao Ni\n            \n                \n        \n\n        \n            \n            \n                Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems\n                UCL: Advanced Research Computing Centre\n                \n                Principal advisor: Dr Matt Graham\n            \n                \n        \n\n        \n            \n            \n                Optimal transport for probabilistic machine learning\n                Imperial: Mathematics\n                \n                Principal advisor: Dr Felipe Tobar\n            \n                \n        \n\n        \n            \n            \n                Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Pavel Berloff\n            \n                \n        \n\n        \n            \n            \n                Numerical methods for PDEs in fractal domains\n                UCL: Mathematics\n                \n                Principal advisor: Prof David Hewett\n            \n                \n        \n\n        \n            \n            \n                Re-griding the TROVE nuclear motion program\n                UCL: Physics and Astronomy\n                \n                Principal advisor: Prof Jonathan Tennyson\n            \n                \n        \n\n        \n            \n            \n                Reinforcement Learning for Full-Hexahedral Mesh Generation\n                Imperial: Aeronautics\n                \n                Principal advisor: Prof Joaquim Peiro and Dr Mashy Green (UCL ARC)\n            \n                \n        \n\n        \n            \n            \n                Drift detection in graph streams and its applications in healthcare\n                UCL: Computer Science and Advanced Research Computing Centre\n                \n                Principal advisor: Dr. Yevgeniya Kovalchuk\n            \n                \n        \n\n        \n            \n            \n                HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers\n                UCL: Chemistry and Advanced Research Computing\n                \n                Principal advisor: Prof Peter V. Coveney\n            \n                \n        \n\n        \n            \n            \n                Time Reversal Imaging and Learned Physics with Neural ODEs\n                UCL: Computer Science\n                \n                Principal advisor: Prof Simon Arridge\n            \n                \n        \n\n        \n            \n            \n                Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)\n                UCL: Medical Physics and Biomedical Engineering\n                \n                Principal advisor: Prof Ben Cox\n            \n                \n        \n\n        \n            \n            \n                Transfer Learning for Monte Carlo\n                UCL: Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                Improving impaired hearing through sound reconstruction from neural activity patterns\n                UCL: Computer Science\n                \n                Principal advisor: Prof Martin Benning\n            \n                \n        \n\n        \n            \n            \n                Machine Learning for Low-Cost Data Assimilation\n                UCL: Computer Science\n                \n                Principal advisor: Prof Marc Deisenroth\n            \n                \n        \n\n        \n            \n            \n                Fast Multipole Methods on modern architectures\n                UCL: Maths and Advanced Research Computing Centre\n                \n                Principal advisor: Prof Timo Betcke\n            \n                \n        \n\n        \n            \n            \n                A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems\n                Imperial: Mathematics\n                \n                Principal advisor: Dr.Cristopher Salvi\n            \n                \n        \n\n        \n            \n            \n                Transfer Learning and Few-Shot Learning for Science: Learning with Limited Data and Compute\n                UCL: Computer Science\n                \n                Principal advisor: Dr Benjamin Guedj\n            \n                \n        \n\n        \n            \n            \n                A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Colin Cotter and Dr. Dante Kalise\n            \n                \n        \n\n        \n            \n            \n                Algorithms and software for differentiable programming at the ML/PDE/DAE interface\n                Imperial: Computing and Mathematics\n                \n                Principal advisor: Prof Ruth Misener and Prof David Ham\n            \n                \n        \n\n        \n            \n            \n                Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems\n                UCL: Chemistry and Advanced Research Computing\n                \n                Principal advisor: Prof Peter V. Coveney\n            \n                \n        \n\n        \n            \n            \n                Inference of gene regulatory networks from single cell data\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Vahid Shahrezaei and Dr. Philipp Thomas \n            \n                \n        \n\n        \n            \n            \n                Automated Bayesian inference in stochastic differential equation models\n                UCL: Advanced Research Computing Centre\n                \n                Principal advisor: Dr Matt Graham\n            \n                \n        \n\n        \n            \n            \n                PDE-driven Digital Twins\n                Imperial: Mathematics\n                \n                Principal advisor: Prof Colin Cotter\n            \n                \n        \n\n        \n            \n            \n                A Unified Form Language for PDEs in non-divergence form\n                UCL: Mathematics\n                \n                Principal advisor: Dr Max Jensen\n            \n                \n        \n\n        \n            \n            \n                PARALIFT – A PARAllel framework for LIFTed training of neural networks\n                UCL: Computer Science\n                \n                Principal advisor: Prof Martin Benning\n            \n                \n        \n\n        \n            \n            \n                Simulation-based Inference for Expensive Scientific Simulators\n                UCL: Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                A computational framework for heterogeneous coupling in large scale computations\n                UCL: Mathematics\n                \n                Principal advisor: Prof Erik Burman\n            \n                \n        \n\n        \n            \n            \n                Data-driven multi-scale modelling of bacterial biofilms\n                UCL: Mathematics\n                \n                Principal advisor: Dr. Philip Pearce\n            \n                \n        \n\n        \n            \n            \n                Koopman operator learning of nonlinear PDEs\n                Imperial: Mathematics\n                \n                Principal advisor: Dr.Nicholas Boulle\n            \n                \n        \n\n        \n            \n            \n                Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models\n                UCL: Statistics\n                \n                Principal advisor: Prof Alexandros Beskos\n            \n                \n        \n\n        \n            \n            \n                Robust Bayesian Inference at Scale\n                UCL: Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                Efficient Computation of Transition States in Molecular Dynamics\n                Imperial: Computing\n                \n                Principal advisor: Dr. Panos Parpas\n            \n                \n        \n\n        \n            \n            \n                Computational solution of inverse problems using large datasets of low rank\n                UCL: Mathematics\n                \n                Principal advisor: Prof Erik Burman\n            \n                \n        \n\n        \n            \n            \n                Machine learning-based biophysical models of antibody properties and function\n                Imperial: Mathematics\n                \n                Principal advisor: Dr. Barbara Bravi and Prof Mauricio Barahona\n            \n                \n        \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n\t\tEPSRC Centre for Doctoral Training in\n\t\t\n\t\tCollaborative Computational Modelling at the Interface \n\t\t\n\t",
    "section": "",
    "text": "CCMI is a collaboration between University College London and Imperial College London to offer a novel and innovative PhD training programme at the interface of computational modelling, data sciences, and research software engineering.\n\n\n\n\n\n\n\nStudents will join world-class research groups across UCL and Imperial, and will benefit from a bespoke training programme around research software engineering and its applications in the computational and data sciences.\n\n\n\n\n\n\n\nBlog Posts\n\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                    \n                        \n                            \n                                CCMI - How we got here\n                            \n                            How CCMI got from a vague idea to an existing centre.\n                        \n                    \n                \n                    \n                \n            \n                \n                    \n                    \n                        \n                            \n                                CCMI is about to start\n                            \n                            The first CCMI cohort is about to arrive.\n                        \n                    \n                \n                    \n                \n            \n                \n                    \n                    \n                        \n                            \n                                Nobel Prizes in Chemistry and Physics for AI research\n                            \n                            The 2024 nobel prizes in Physics and Chemistry were given out to breakthrough research in AI from members of the UCL community.\n                        \n                    \n                \n                    \n                \n            \n                \n                    \n                    \n                        \n                            \n                                To be or not to be in the office\n                            \n                            A few thoughts on remote vs in-office working in a CDT.\n                        \n                    \n                \n                    \n                \n            \n\n            \n                \n                Previous\n            \n            \n                \n                Next\n            \n        \n\n    \nNo matching items\n\n\n\n    \n        \n        \n            CCMI offers a fully funded 4 year PhD programme at either UCL or Imperial College\n        \n    \n    \n        \n        \n            We offer a bespoke cohort training programme consisting of dedicated block workshops, software weeks, summer schools, and many other activities\n        \n    \n    \n        \n        \n            The first intake starts in Sept. 2025 (call for applications to start in autumn 2024)\n        \n    \n    \n        \n        \n            We will provide research opportunities across computational mathematics, scientific algorithms, high-performance computing, research software engineering, data sciences, and applications\n        \n    \n    \n        \n        \n            We have external partners ranging from innovative SMEs to world-leading HPC labs\n        \n    \n    \n\nNo matching items\n\n\nEvents\n\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Grand Opening  \n                                Oct 1, 2025\n                                    \n                                We are celebraing the opening of the CCMI CDT.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Online Open Day  \n                                Nov 10, 2025\n                                    \n                                We are organising an online open day to get to know the CDT, our research and how the application process works.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Open Day  \n                                Nov 24, 2025\n                                    \n                                We are organising an open day to get to know the CDT, our research and how the application process works.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Seminar on AI and Scientific Computing  \n                                Oct 28, 2025\n                                    \n                                Prof Petros Koumoutsakos from Harvard University is giving a seminar on AI and Scientific Computing.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Seminar on Low-rank approximation by column subset selection  \n                                Nov 11, 2025\n                                    \n                                Dr Alice Cortinovis from the University of Pisa is presenting new results on low-rank approximations of matrices.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Training Week on Programming Models for Scientific Computing  \n                                Oct 6, 2025\n                                    \n                                Our first CDT training week is starting.\n                            \n                        \n                    \n                    \n                \n            \n\n        \n\n        \n            \n            Previous\n        \n        \n            \n            Next\n        \n    \n\n\nNo matching items\n\n\n\n\nEvents\n\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Grand Opening  \n                                Oct 1, 2025\n                                    \n                                We are celebraing the opening of the CCMI CDT.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Online Open Day  \n                                Nov 10, 2025\n                                    \n                                We are organising an online open day to get to know the CDT, our research and how the application process works.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    CCMI Open Day  \n                                Nov 24, 2025\n                                    \n                                We are organising an open day to get to know the CDT, our research and how the application process works.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Seminar on AI and Scientific Computing  \n                                Oct 28, 2025\n                                    \n                                Prof Petros Koumoutsakos from Harvard University is giving a seminar on AI and Scientific Computing.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Seminar on Low-rank approximation by column subset selection  \n                                Nov 11, 2025\n                                    \n                                Dr Alice Cortinovis from the University of Pisa is presenting new results on low-rank approximations of matrices.\n                            \n                        \n                    \n                    \n                \n            \n                \n                    \n                        \n                            \n                                \n                                    Training Week on Programming Models for Scientific Computing  \n                                Oct 6, 2025\n                                    \n                                Our first CDT training week is starting.\n                            \n                        \n                    \n                    \n                \n            \n\n        \n\n        \n            \n            Previous\n        \n        \n            \n            Next\n        \n    \n\n\nNo matching items\n\n\n\n\n\nKEY FACTS\n\n\n\n    \n        \n        \n            CCMI offers a fully funded 4 year PhD programme at either UCL or Imperial College\n        \n    \n    \n        \n        \n            We offer a bespoke cohort training programme consisting of dedicated block workshops, software weeks, summer schools, and many other activities\n        \n    \n    \n        \n        \n            The first intake starts in Sept. 2025 (call for applications to start in autumn 2024)\n        \n    \n    \n        \n        \n            We will provide research opportunities across computational mathematics, scientific algorithms, high-performance computing, research software engineering, data sciences, and applications\n        \n    \n    \n        \n        \n            We have external partners ranging from innovative SMEs to world-leading HPC labs\n        \n    \n    \n\nNo matching items\n\n\n\n\n\n\n\nWould you like to join?\n\n\n Register to receive updates for 2026 entry"
  },
  {
    "objectID": "training/training.html",
    "href": "training/training.html",
    "title": "The CCMI Training Programme",
    "section": "",
    "text": "The CCMI graduate training programme rests on three foundations.\n\nA four year research thesis on the interface of computational sciences, research software engineering, and data sciences.\nCollaborative interface working groups, which are week long block events that combine topical overviews, research discussions, software exploration, and discovery of new ideas.\nA software journey that involves active collaboration in open-source projects, close collaboration with external partners, regular software weeks, sprint sessions, industry talks on software engineering, and many other activities."
  },
  {
    "objectID": "training/thesis.html",
    "href": "training/thesis.html",
    "title": "Research thesis",
    "section": "",
    "text": "The four year research thesis will lead to a PhD degree from either UCL or Imperial College.\nAs part of the interview process students choose an available research project either at UCL or at Imperial College.\nThesis topics include the UCL Departments of Mathematics, Statistics, and Computer Science, and the Imperial Departments of Mathematics and Computing.\nFor a continuously updated selection of potential thesis topics please browse our collection of PhD projects."
  },
  {
    "objectID": "events/posts/2025_virtual_open_day.html",
    "href": "events/posts/2025_virtual_open_day.html",
    "title": "CCMI Online Open Day",
    "section": "",
    "text": "On 10 November, there will be a virtual open day from 2pm to 4pm GMT. At this event, you will have an opportunity to hear from and meet the CDT team, researchers who will be supervising some students, and the current CDT students.\nIf you want to take part please fill out our CDT registration form. No need to fill it out again if you have already registered your interest in the CDT.\nWe will send out online joining links to registered participants before the event.\nWe are looking forward to meeting you!"
  },
  {
    "objectID": "events/posts/2025_ccmi_opening.html",
    "href": "events/posts/2025_ccmi_opening.html",
    "title": "CCMI Grand Opening",
    "section": "",
    "text": "On Wednesday 1 October we will celebrate the Grand Opening of the CCMI CDT from 2pm onwards. We will start with a special invited colloquium by Prof Tobias Weinzierl from the University of Durham, followed by an open reception.\nIf you would like to participate please register at Eventbrite."
  },
  {
    "objectID": "events/posts/2025_seminar_10_28.html",
    "href": "events/posts/2025_seminar_10_28.html",
    "title": "Seminar on AI and Scientific Computing",
    "section": "",
    "text": "LOCATION: First Floor, Function Room, 90 High Holborn WC1V 6BH\nTIME: 14:00-15:00, 28/10/2025\nSPEAKER: Petros Koumoutsakos, Herbert S. Winokur, Jr. Professor of Computing in Science and Engineering, Harvard University\nTITLE: AI and Scientific Computing: Algorithmic Alloys for Forecasting and Optimization of Complex Systems\nABSTRACT: Computational science and Artificial Intelligence have been drivers and benefactors of advances in algorithms and hardware, each in different ways, and originally with different targets. The intellectual space between these two fields is home to exciting opportunities for scientific discovery and engineering innovation. I will discuss algorithmic alloys based on the fusion of data driven and equation driven methodologies for the prediction and control of complex flows. I will also present ideas of developing algorithmic alloys for fusing experiments and simulations for understanding and controlling complex systems."
  },
  {
    "objectID": "events/posts/2025_first_training_week.html",
    "href": "events/posts/2025_first_training_week.html",
    "title": "Training Week on Programming Models for Scientific Computing",
    "section": "",
    "text": "On Monday 1 October our first training week is starting. Students will get to know programming models for Scientific Computing through a mixture of lectures, student led seminars and practical programming sessions. Topics parallel programming models, GPU development and software libraries for mathematical tasks. On the final day we look forward to a guest lecture by Chris Richardson from Cambridge on FEniCS."
  },
  {
    "objectID": "events/posts/2025_seminar_11_11.html",
    "href": "events/posts/2025_seminar_11_11.html",
    "title": "Seminar on Low-rank approximation by column subset selection",
    "section": "",
    "text": "LOCATION: ROOM 660, 6th Floor, Chemistry, Imperial College South Kensington Campus\nTIME: 14:00-15:00, 11/11/2025\nSPEAKER: Dr Alice Cortinovis, Department of Computer Science, University of Pisa\nTITLE: Low-rank approximation by column subset selection\nABSTRACT: The problem of approximating a large matrix B with a matrix of lower rank is important for speeding up computations and compressing data. The best rank-k approximation of B can be obtained from a singular value decomposition of B, but this process is too costly for large-scale matrices. A practical alternative is to approximate B using only a small subset of its actual columns. This approach preserves the original meaning of the data stored in the columns, making the result both interpretable and computationally efficient. How to choose these columns to get a good low-rank approximation? In this talk, I will pick them at random! More precisely, I will choose columns by sampling from a particular probability distribution that ensures that the resulting low-rank approximation is, in expectation, almost as good as the optimal rank-k approximation of B. I will also highlight how this strategy can be adapted to a range of other low-rank approximations that are built using columns and/or rows of the matrix B."
  },
  {
    "objectID": "events/posts/2025_in_person_open_day.html",
    "href": "events/posts/2025_in_person_open_day.html",
    "title": "CCMI Open Day",
    "section": "",
    "text": "On 24 November, there will be a CCMI CDT open day, taking place from 3:30pm to 5:30pm GMT in the Ground Floor Collaboration Space at UCL Bidborough House, 38-50 Bidborough St, London, WC1H 9BT. At this event, you will have an opportunity to hear from and meet the CDT team, researchers who will be supervising some students, and the current CDT students.\nTo attend please sign up at:\nhttps://www.eventbrite.com/e/collaborative-computational-modelling-at-the-interface-ccmi-cdt-open-day-tickets-1968015024917?aff=oddtdtcreator\nWe are looking forward to meeting you!"
  },
  {
    "objectID": "events/event_list.html",
    "href": "events/event_list.html",
    "title": "CCMI Events",
    "section": "",
    "text": "The CCMI runs a fortnightly seminar that alternates between being held at UCL and Imperial. It covers topics relevant to the CDT, including scientific computing, numerical analysis, machine learning, and application areas.\nIf you wish to receive email notifications of upcoming seminar talks, you can join our mailing list by sending an email containing “subscribe” to ccmi-cdt-seminar-request@ucl.ac.uk. You will then recieve an email telling you how to confirm your subscription."
  },
  {
    "objectID": "events/event_list.html#ccmi-seminar",
    "href": "events/event_list.html#ccmi-seminar",
    "title": "CCMI Events",
    "section": "",
    "text": "The CCMI runs a fortnightly seminar that alternates between being held at UCL and Imperial. It covers topics relevant to the CDT, including scientific computing, numerical analysis, machine learning, and application areas.\nIf you wish to receive email notifications of upcoming seminar talks, you can join our mailing list by sending an email containing “subscribe” to ccmi-cdt-seminar-request@ucl.ac.uk. You will then recieve an email telling you how to confirm your subscription."
  },
  {
    "objectID": "training/working_groups.html",
    "href": "training/working_groups.html",
    "title": "Collaborative interface working groups (CIWGs)",
    "section": "",
    "text": "Collaborative interface working groups (CIWGs) deliver the core topical training programme that every CCCMI student goes through.\nThey are organised in the form of week long cohort workships that cover an introduction to research topics, research and software landscape overviews, exploration of current research challenges, and a discovery phase for future trends and new directions.\n\n\n\nStructure of collaborative interface working groups\n\n\nIn contrast to traditional lectures these working groups are centered around learning how to be an effective researcher and will contain significant student led components.\nCIWGs are delivered across the first two years in the following topic areas:\n\nModern programming models for scientific computing\nSoftware engineering fundamentals\nData intensive computations\nPDE Discretisations\nDeep learning in theory and practice\nResponsible computational modelling\nHigh dimensional problems and optimisation"
  },
  {
    "objectID": "training/software_journey.html",
    "href": "training/software_journey.html",
    "title": "The Software Journey",
    "section": "",
    "text": "The software journey has been designed to give every CCMI student training to become a world class research software engineer. Students will collaborate on a diverse range of open-source software projects, some suggested by students themselves, others by academics or industrial partners. Students will learn how to focus ideas, develop minimum viable products, pitch ideas to stakeholders, and collaborate as teams. This will be supported by a dedicated termly software week with software sprints, progress talks, external seminars delivered by partners, and other activities.\nThe software journey is an ongoing activity throughout the whole four year training programme. Every CDT student is expected to actively contribute to the software journey and develop their skills as professional research software engineers."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequenty Asked Questions",
    "section": "",
    "text": "NoteWhat is CCMI?\n\n\n\n\n\nCCMI stands for Collaborative Computational Modelling at the Interface. Our vision is to train a future generation of scientists working at the interface of physics driven modelling, data driven modelling and research software engineering. Our trainees will move fluently across these interfaces, understanding their interdependencies to build the next generation of simulations and translate them to impacts in industry and society.\n\n\n\n\n\n\n\n\n\nNoteWhat is a CDT?\n\n\n\n\n\nCDT stands for ‘Centre for Doctoral Training’, where Universities, and their partners, come together to offer combined research and expertise to support and train PhD students. By undertaking your PhD in a CDT, you’ll be part of a prestigious cohort that will study together within a structured programme, providing a supportive and collaborative environment. You’ll join a thriving group and carry out your research alongside people from different backgrounds, with different perspectives.\n\n\n\n\n\n\n\n\n\nNoteHow will a PhD from the CCMI CDT benefit my career?\n\n\n\n\n\nYou will receive cutting edge training on the interface of data sciences, simulation, and research software engineering, opening up careers across research and engineering, the financial sector, tech companies, and anywhere that data and simulations are relevant. Your software skills will make you a highly sought after candidate for any development role and provide an advantage over many other data or simulation oriented PhD level training programmes.\n\n\n\n\n\n\n\n\n\nNoteHow long is the programme?\n\n\n\n\n\nThe programme is available as a 4 year, full time programme.\nThere is an option to study part time. This is available to postgraduate researchers that do not need a student visa to study in the UK. On the application form, there is a question to note whether you are applying for full time or part time study. Part time schedules will be built on a case-by-case basis and would be discussed at the interview stage, if you are shortlisted at the initial application stage\n\n\n\n\n\n\n\n\n\nNoteHow many students will there be in each cohort?\n\n\n\n\n\nThere will be approximately 14 students per cohort, starting in September each year of admission. These studentships will be evenly distributed across UCL and Imperial.\nThere will be a total of approximately 70-80 students, over a 5 year period.\n\n\n\n\n\n\n\n\n\nNoteWhen will the PhD Programme start?\n\n\n\n\n\nThe first cohort in the CCMI CDT will start in September 2025.\n\n\n\n\n\n\n\n\n\nNoteAre these PhD opportunities funded? What is the stipend payment?\n\n\n\n\n\nYes, postgraduate researchers at the CCMI CDT will receive a full 4 year studentship (pro-rata for part-time students)\nTo cover living expenses, the UKRI rate (London-weighted) doctoral stipend, plus an additional £2,000 top-up, will be paid to postgraduate researchers each year, in regular instalments. The UKRI stipends tend to increase each year with inflation. The stipend for 2025/2026 will be confirmed in Spring 2025, but is estimated to be £21,870, giving a total estimated stipend of £23,870 including top-up. For more information on UKRI stipends, please visit their website.\nAnnual tuition will be paid and there is funding available to support research training costs such as conference attendance.\nFor additional information on the cost of studying in London, please visit the UCL website.\n\n\n\n\n\n\n\n\n\nNoteDo I need to choose a topic for my PhD research?\n\n\n\n\n\nNo, you don’t need to contact a supervisor before applying. Unlike more traditional approaches to applying for a PhD, if you are interested in applying for the CCMI CDT, you are not required or expected to contact anyone about project supervision.\nPlease see the list of projects that are offered. If you pass the interview stage, we will hold an open day for you to learn more about the project areas on offer and we will ask you to submit which projects/research areas you are interested in. You will then have a chance to meet with supervisors to discuss projects.\n\n\n\n\n\n\n\n\n\nNoteHow is the programme structured?\n\n\n\n\n\nThe CCMI CDT programme rests on 3 foundations - a 4 year research thesis, collaborative interface working groups and a software journey. For more information about the structure of the programme, please visit our Training Programme pages\n\n\n\n\n\n\n\n\n\nNoteWill I get any industrial experience?\n\n\n\n\n\nNot all projects will automatically include industry experience. Some projects, co-sponsored by external industry partners, will have an expected placement at the partner institution as an integral part of the research. Some projects may also require students to work elsewhere as part of their research. This will be determined individually, as per the corresponding research projects and will form a component of the thesis.\nWhilst internships are not a mandatory component of the CDT, we do encourage and support students who are interested in industry experience. We welcome this discussion as part of your project matching, should you pass the interview stage of the application.\n\n\n\n\n\n\n\n\n\nNoteWhat are the entry requirements to apply?\n\n\n\n\n\nA UK Master’s degree in a relevant discipline with Merit, or a minimum of an upper second-class UK Bachelor’s degree in a relevant discipline, or an overseas qualification of an equivalent standard. Work experience may also be taken into account.\nRelevant disciplines are broadly STEM subjects (Science, Technology, Engineering, Mathematics). Related disciplines at the interface of STEM may be acceptable. During the application stage, we will assess the suitability of your technical background.\nThe English language level for this programme is: Level 1.\n\n\n\n\n\n\n\n\n\nNoteI already have a PhD qualification, can I still apply?\n\n\n\n\n\nYes, you can apply for the CCMI CDT if you already have obtained a PhD.\n\n\n\n\n\n\n\n\n\nNoteWhat funding rules do I need to be aware of?\n\n\n\n\n\nThere are residence requirements for postgraduate research funding from Engineering and Physical Sciences Research Council (EPSRC). UKRI-funded studentships are open to UK (home) and international students. To be considered a ‘home’ student, you must meet one of the following criteria set out by UKRI:\n\nbe a UK national\nhave settled status\nhave pre-settled status\nhave indefinite leave to remain or enter.\n\nAdditionally, you will need to meet specified residency requirements. The decision about fee status will be taken by the host university before a formal offer is made. If a candidate does not meet any of the above criteria, they will be classified as an international student. International applicants are eligible for tuition fees at the UK rate and a stipend at the UKRI rate. Some scholarships to cover the difference to full overseas tuition rates may be available and are given out on a competitive basis. However, there are restrictions on the number of overseas applicants funded through the CDT.\nWe encourage applications from international candidates, but please be aware that there is typically strong competition within universities for this limited funding.\nNormally, to be eligible for a full award from the EPSRC, a student must a) have no restrictions on how long they can stay in the UK and b) have been ordinarily resident in the UK for at least 3 years prior to the start of the studentship. For more details visit the UKRI website.\n\n\n\n\n\n\n\n\n\nNoteHow do I apply?\n\n\n\n\n\nThe application procedure consists of 4 stages: The Online Application, Interview, Project Matching, and pro-forma application for document and fee status check. Please see full details of our application process at our Application Procedure page. If applications are not yet open, but you would like to be notified when they are, please do register your interest and we will notify you when applications open.\n\n\n\n\n\n\n\n\n\nNoteDo I need to submit a research proposal or choose supervisor when I apply?\n\n\n\n\n\nNo. Before you apply, we would ask you to review the projects on offer at the ‘Projects’ page on our website.\nIf you pass the interview stage of the application process, you will have the opportunity to learn more about our projects and supervisors. You will be asked submit your top three preferred projects so we can understand your research interests. You will then meet potential supervisors one on one during our supervisor matching process.\n\n\n\n\n\n\n\n\n\nNoteI haven’t yet graduated from my current university course, can I still apply?\n\n\n\n\n\nYes. At the initial application stage we will ask you about qualifications you have already achieved, your current studies and predicted grades. If successful at interview and project matching, you would be made an offer which is conditional upon the outcome of your current studies.\n\n\n\n\n\n\n\n\n\nNoteDo I need to upload references when I apply?\n\n\n\n\n\nWe do not ask for references at the initial application stage. If you reach the offer stage, you will then be asked to supply references. References will be submitted after project matching, directly to UCL or Imperial, with your pro-forma application.\n\n\n\n\n\n\n\n\n\nNoteWhen will the interviews take place?\n\n\n\n\n\nFor all relevant application deadlines please see the information under How to apply.\n\n\n\n\n\n\n\n\n\nNoteWhen will I know if my application has been successful?\n\n\n\n\n\nShould you be invited to interview, we would aim to inform you of the outcome of the interview within a week."
  },
  {
    "objectID": "phd_projects/entries/Pavliotis_inference.html",
    "href": "phd_projects/entries/Pavliotis_inference.html",
    "title": "Inference and inverse problems for stochastic interacting particle systems",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\n\n\n\nStochastic interacting particle systems (SIPS) arise in many applications, including mathematical biology, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. In addition, many PDE models, such as the Keller-Segel model for bacterial chemotaxis and the Onsager model for liquid crystals, can be interpreted as the mean field limit of a system of interacting diffusions. Quite often, the interaction law of the SIPS is not known and has to be inferred from data. The objective of this project is to develop efficient and accurate inference methodologies for learning the interaction law of SIPS and of their mean field limit from data. We will consider both the parametric and the nonparametric inference problem, and we will develop and implement a variety of methodologies, including maximum likelihood-based techniques, stochastic gradient descent in continuous time, kernel methods and spectral theoretic methodologies. We will also use recently developed neural calibration techniques. In addition, we will also interpret the inference problem for SIPS as an inverse problem for the mean field PDE and then apply Bayesian methods for inverse problems. For this, the measurement model will be taken to be noisy measurements of the solution to the PDE, e.g. of the Keller-Segel or of the Onager model, and more generally of the nonlinear and nonlocal McKean-Vlasov mean field PDE.\n\n\n\n\nAn inference toolbox for SIPS and their mean field limit, including efficient SDE solvers for the SIPS. All of the methods that will be studied and developed (MLE, SGDCT, neural calibration) will be included in the toolbox that we will develop.\nSoftware for implementing Bayesian inverse problem methodologies to the mean field nonlinear, nonlocal PDEs."
  },
  {
    "objectID": "phd_projects/entries/Pavliotis_inference.html#project-description",
    "href": "phd_projects/entries/Pavliotis_inference.html#project-description",
    "title": "Inference and inverse problems for stochastic interacting particle systems",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\n\n\n\nStochastic interacting particle systems (SIPS) arise in many applications, including mathematical biology, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. In addition, many PDE models, such as the Keller-Segel model for bacterial chemotaxis and the Onsager model for liquid crystals, can be interpreted as the mean field limit of a system of interacting diffusions. Quite often, the interaction law of the SIPS is not known and has to be inferred from data. The objective of this project is to develop efficient and accurate inference methodologies for learning the interaction law of SIPS and of their mean field limit from data. We will consider both the parametric and the nonparametric inference problem, and we will develop and implement a variety of methodologies, including maximum likelihood-based techniques, stochastic gradient descent in continuous time, kernel methods and spectral theoretic methodologies. We will also use recently developed neural calibration techniques. In addition, we will also interpret the inference problem for SIPS as an inverse problem for the mean field PDE and then apply Bayesian methods for inverse problems. For this, the measurement model will be taken to be noisy measurements of the solution to the PDE, e.g. of the Keller-Segel or of the Onager model, and more generally of the nonlinear and nonlocal McKean-Vlasov mean field PDE.\n\n\n\n\nAn inference toolbox for SIPS and their mean field limit, including efficient SDE solvers for the SIPS. All of the methods that will be studied and developed (MLE, SGDCT, neural calibration) will be included in the toolbox that we will develop.\nSoftware for implementing Bayesian inverse problem methodologies to the mean field nonlinear, nonlocal PDEs."
  },
  {
    "objectID": "phd_projects/entries/cotter_nextgen.html",
    "href": "phd_projects/entries/cotter_nextgen.html",
    "title": "Next generation implicit numerics for atmosphere models",
    "section": "",
    "text": "The classical numerical approaches to building atmosphere models rely on complicated splitting methods that deal with different parts of the model: waves, transport, moisture processes (clouds, evaporation, rain, ice etc), radiation, boundary layers, convection, etc. These splitting methods lead to highly complicated codes, time schemes that are difficult to analyse for stability/accuracy, and occasionally numerical artifacts the coupling of fluid dynamics and other physics. In this project we are pursuing an alternative goal: to translate as much of the system as possible into a single monolithic PDE coupling all the variables, and solve it with an implicit Runge-Kutta method. This is made possible by recent advances in massively parallel iterative methods for solving the implicit systems that come from this equation: we shift the complications from the timestepping scheme into the iterative solver.\nAs a first step, we will build an atmosphere model consisting of the fluid dynamics component plus moisture processes, in this framework. Moisture processes involve switches (e.g., when maximum humidity is reached, any surplus water vapour is converted into cloud); we will deal with this using advanced “Variational Inequality” Newton solvers facilitated using PETSc [1]. The spatial discretisation will be build from compatible finite element methods closely related to those being implemented in the next generation LFRic modelling system at the Met Office. The software will be developed using Firedrake [2], which is a system for solving complicated PDEs using advanced finite element methods based on domain specific languages and code generation.\nThe resulting modelling system will be automatically differentiable using the py-adjoint system (https://github.com/dolfin-adjoint/pyadjoint), making it suitable for blending with machine learning tools, towards our goal of hybrid physics-based/data-driven modelling approaches.\n[1] S. Balay, S. Abhyankar, M. Adams, S. Benson, J. Brown, P. Brune, K. Buschelman, E. Constantinescu, L. Dalcin, A. Dener, V. Eijkhout, J. Faibussowitsch, W. Gropp, V. Hapla, T. Isaac, P. Jolivet, D. Karpeyev, D. Kaushik, M. Knepley, F. Kong, S. Kruger, D. May, L. Curfman McInnes, R. Mills, L. Mitchell, T. Munson, J. Roman, K. Rupp, P. Sanan, J Sarich, B. Smith, H. Suh, S. Zampini, H. Zhang, and H. Zhang, J. Zhang, PETSc/TAO Users Manual, ANL-21/39 - Revision 3.22, 2024. https://doi.org/10.2172/2205494, https://petsc.org/release/docs/manual/manual.pdf\n[2] David A. Ham, Paul H. J. Kelly, Lawrence Mitchell, Colin J. Cotter, Robert C. Kirby, Koki Sagiyama, Nacime Bouziani, Sophia Vorderwuelbecke, Thomas J. Gregory, Jack Betteridge, Daniel R. Shapero, Reuben W. Nixon-Hill, Connor J. Ward, Patrick E. Farrell, Pablo D. Brubeck, India Marsden, Thomas H. Gibson, Miklós Homolya, Tianjiao Sun, Andrew T. T. McRae, Fabio Luporini, Alastair Gregory, Michael Lange, Simon W. Funke, Florian Rathgeber, Gheorghe-Teodor Bercea, and Graham R. Markall. Firedrake User Manual. Imperial College London and University of Oxford and Baylor University and University of Washington, first edition edition, 5 2023. doi:10.25561/104839.\n\n\nWe have a body of ten years of research in methods and software for atmosphere models, which is summarised in [3] and [4].\n[3] Cotter, Colin J. “Compatible finite element methods for geophysical fluid dynamics.” Acta Numerica 32 (2023): 291-393.\n[4] Gibson, Thomas H., Andrew TT McRae, Colin J. Cotter, Lawrence Mitchell, and David A. Ham. Compatible Finite Element Methods for Geophysical Flows: Automation and Implementation Using Firedrake. Springer Nature, 2019.\n\n\n\nThis project is available to researchers with a wide variety of interests, who might focus on one or more of: * designing scalable iterative methods allowing the use of highly parallel supercomputers, * developing interative solvers that seamlessly incorporate moisture processes, * developing stabilisation schemes that allow the model to incorporate the effects of unresolved turbulent scales, * time-parallel algorithms using ParaDiag methods [5], * benchmarking the quality of the simulation in challenging testcases such as fronts and storms, * exploration of computationally optimal configurations using e.g. high order discretisations and emergent Firedrake capability on GPUs.\n[5] Hope-Collins, J., Hamdan, A., Bauer, W., Mitchell, L. and Cotter, C., 2024. asQ: parallel-in-time finite element simulations using ParaDiag for geoscientific models and beyond. arXiv preprint arXiv:2409.18792.\n\n\n\n\nThe research will contribute to open source software developed in Python (with automatically generated high performance C code)"
  },
  {
    "objectID": "phd_projects/entries/cotter_nextgen.html#project-description",
    "href": "phd_projects/entries/cotter_nextgen.html#project-description",
    "title": "Next generation implicit numerics for atmosphere models",
    "section": "",
    "text": "The classical numerical approaches to building atmosphere models rely on complicated splitting methods that deal with different parts of the model: waves, transport, moisture processes (clouds, evaporation, rain, ice etc), radiation, boundary layers, convection, etc. These splitting methods lead to highly complicated codes, time schemes that are difficult to analyse for stability/accuracy, and occasionally numerical artifacts the coupling of fluid dynamics and other physics. In this project we are pursuing an alternative goal: to translate as much of the system as possible into a single monolithic PDE coupling all the variables, and solve it with an implicit Runge-Kutta method. This is made possible by recent advances in massively parallel iterative methods for solving the implicit systems that come from this equation: we shift the complications from the timestepping scheme into the iterative solver.\nAs a first step, we will build an atmosphere model consisting of the fluid dynamics component plus moisture processes, in this framework. Moisture processes involve switches (e.g., when maximum humidity is reached, any surplus water vapour is converted into cloud); we will deal with this using advanced “Variational Inequality” Newton solvers facilitated using PETSc [1]. The spatial discretisation will be build from compatible finite element methods closely related to those being implemented in the next generation LFRic modelling system at the Met Office. The software will be developed using Firedrake [2], which is a system for solving complicated PDEs using advanced finite element methods based on domain specific languages and code generation.\nThe resulting modelling system will be automatically differentiable using the py-adjoint system (https://github.com/dolfin-adjoint/pyadjoint), making it suitable for blending with machine learning tools, towards our goal of hybrid physics-based/data-driven modelling approaches.\n[1] S. Balay, S. Abhyankar, M. Adams, S. Benson, J. Brown, P. Brune, K. Buschelman, E. Constantinescu, L. Dalcin, A. Dener, V. Eijkhout, J. Faibussowitsch, W. Gropp, V. Hapla, T. Isaac, P. Jolivet, D. Karpeyev, D. Kaushik, M. Knepley, F. Kong, S. Kruger, D. May, L. Curfman McInnes, R. Mills, L. Mitchell, T. Munson, J. Roman, K. Rupp, P. Sanan, J Sarich, B. Smith, H. Suh, S. Zampini, H. Zhang, and H. Zhang, J. Zhang, PETSc/TAO Users Manual, ANL-21/39 - Revision 3.22, 2024. https://doi.org/10.2172/2205494, https://petsc.org/release/docs/manual/manual.pdf\n[2] David A. Ham, Paul H. J. Kelly, Lawrence Mitchell, Colin J. Cotter, Robert C. Kirby, Koki Sagiyama, Nacime Bouziani, Sophia Vorderwuelbecke, Thomas J. Gregory, Jack Betteridge, Daniel R. Shapero, Reuben W. Nixon-Hill, Connor J. Ward, Patrick E. Farrell, Pablo D. Brubeck, India Marsden, Thomas H. Gibson, Miklós Homolya, Tianjiao Sun, Andrew T. T. McRae, Fabio Luporini, Alastair Gregory, Michael Lange, Simon W. Funke, Florian Rathgeber, Gheorghe-Teodor Bercea, and Graham R. Markall. Firedrake User Manual. Imperial College London and University of Oxford and Baylor University and University of Washington, first edition edition, 5 2023. doi:10.25561/104839.\n\n\nWe have a body of ten years of research in methods and software for atmosphere models, which is summarised in [3] and [4].\n[3] Cotter, Colin J. “Compatible finite element methods for geophysical fluid dynamics.” Acta Numerica 32 (2023): 291-393.\n[4] Gibson, Thomas H., Andrew TT McRae, Colin J. Cotter, Lawrence Mitchell, and David A. Ham. Compatible Finite Element Methods for Geophysical Flows: Automation and Implementation Using Firedrake. Springer Nature, 2019.\n\n\n\nThis project is available to researchers with a wide variety of interests, who might focus on one or more of: * designing scalable iterative methods allowing the use of highly parallel supercomputers, * developing interative solvers that seamlessly incorporate moisture processes, * developing stabilisation schemes that allow the model to incorporate the effects of unresolved turbulent scales, * time-parallel algorithms using ParaDiag methods [5], * benchmarking the quality of the simulation in challenging testcases such as fronts and storms, * exploration of computationally optimal configurations using e.g. high order discretisations and emergent Firedrake capability on GPUs.\n[5] Hope-Collins, J., Hamdan, A., Bauer, W., Mitchell, L. and Cotter, C., 2024. asQ: parallel-in-time finite element simulations using ParaDiag for geoscientific models and beyond. arXiv preprint arXiv:2409.18792.\n\n\n\n\nThe research will contribute to open source software developed in Python (with automatically generated high performance C code)"
  },
  {
    "objectID": "phd_projects/entries/BetckeM_inverseproblems.html",
    "href": "phd_projects/entries/BetckeM_inverseproblems.html",
    "title": "All-at-once deep learning methods for nonlinear PDE based inverse problems",
    "section": "",
    "text": "In inverse problems (IPs) a natural role for deep learning (DL) is to encode prior information contained in the training set e.g. anatomical and pathological similarities between patients. Learned reconstruction methods realise this in different ways learning a prior/proximal operator for use within an optimisation scheme or a post-processing correction or a pseudo-inverse e.g. via an unrolled scheme (see [1] for an overview of such methods, [2] for methods with convergence guarantees, [3, 4] for examples our group’s work in the context of Photoacoustic tomography). While general PDE solvers are designed to work for all coefficients (in an appropriate function space), for the PDEs modelling the forward operator of a nonlinear IP we may have additional information about the distribution of the coefficients which can be used to improve performance. This idea underpins approaches based on unrolling of an iterative forward solver e.g., specifically in the context of solution of high frequency Helmholtz equation, GMRES with the net acting as a (flexible) parametric preconditioner [5], or iterates of a learned Born series [6], and more generic neural operators which directly construct a neural network approximation to the Green’s function of a linear PDE as a function of its coefficients (see e.g. [7,8,9]).\n\n\n\nThis project will combine both these strands of research. We will seek to optimise the forward solver’s performance for the coefficients on the manifold of the prior which is akin to adaptivity in classical PDE solvers and combine it optimally with the leaned prior into an all-at-once inversion procedure. Possible approaches to construction of such an all-at-once framework include: Unrolling and parametrisation of a nonlinear solver, e.g. Gauss-Newton, for minimising a regularised nonlinear data fitting functional. The Fréchet derivative of the nonlinear data term involves solution of the forward PDE and its adjoint which can be replaced by neural operators (or forward unrolling). The regularisation functional can be replaced with an input convex neural network, or for non-smooth priors, the prox of the regularisation functional can be replaced with a neural network. The whole assembly can then be jointly trained on the same training set following one of the training paradigms e.g. variational, fix point or adversarial. Another formulation can be proposed based on a PDE constraint optimisation framework which opts to minimise the regularised linear data consistency term subject to PDE constraints. The first order optimality conditions contain the Fréchet derivative, the forward and adjoint PDE solves, and the derivative of the regularisation functional, which again can be parametrised similarly as described above and used e.g. as a direction within first order optimisation method. As a part of the project we will investigate and develop suitable neural network architectures e.g. for the forward/adjoint neural operator pairs, Fréchet derivatives, regularisation functionals or their proxes, and efficient training strategies.\n\n\n\nThe objective of the project is to spearhead the paradigm of hybrid learned and model based all-at-once methods paying equal attention to utilising available data sets to aid solution of both the forward and the inverse problem involved. To push the paradigm into the wider inverse problems community of researchers and practitioners a high-quality accompanying software package is necessary. Along the core functionality a.k.a. the all-at-once frameworks, the package will include implementations of state-of-the-art hybrid model and data based forward solvers interfacing to popular PDE software e.g., https://ngsolve.org/, https://fenicsproject.org/, http://www.k-wave.org/ to allow flexibility of the underlying PDEs and the method of their solution. We will include implementations of the state-of-the-art neural operators, learned and analytical proximal/regularisation operators and optimisation methods, and will investigate feasibility of interfacing to popular inverse problems packages such as ODL https://odlgroup.github.io/odl/.\n[1] Arridge, Simon, Maass, Peter, Ozan, Öktem, Schönlieb, Carola-Bibiane. (2019). Solving inverse problems using data-driven models. Acta Numerica. 28. 1-174.\n[2] S. Mukherjee, A. Hauptmann, O. Öktem, M. Pereyra and C. -B. Schönlieb, Learned Reconstruction Methods With Convergence Guarantees: A survey of concepts and applications. IEEE Signal Processing Magazine, vol. 40, no. 1, pp. 164-182, Jan. 2023.\n[3] Hauptmann A, Lucka F, Betcke M, Huynh N, Adler J, Cox B, Beard P, Ourselin S, Arridge S. Model-based learning for accelerated, limited-view 3-D photoacoustic tomography. IEEE Transactions on Medical Imaging. 2018 Mar 29;37(6):1382-93.\n[4] Bolin Pan and Marta M. Betcke, On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector, SIAM Journal on Imaging Sciences 2023 16:2, 770-801\n[5] Stanziola, A., Arridge, S. R., Cox, B. T., & Treeby, B. E. (2021). A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. Journal of Computational Physics, 441, 110430.\n[6] Antonio Stanziola, Simon Arridge, Ben T. Cox, Bradley E. Treeby; A learned Born series for highly-scattering media. JASA Express Lett. 1 May 2023; 3 (5): 052401.\n[7] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. ICLR International Conference on Learning Representations (2021). https://openreview.net/forum?id=c8P9NQVtmnO.\n[8] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, Andrew Stuart. Neural Operator: Graph Kernel Network for Partial Differential Equations. ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (2019), https://openreview.net/forum?id=fg2ZFmXFO3.\n[9] Boullé, N., Townsend, A. Learning Elliptic Partial Differential Equations with Randomized Linear Algebra. Found Comput Math 23, 709–739 (2023)."
  },
  {
    "objectID": "phd_projects/entries/BetckeM_inverseproblems.html#project-description",
    "href": "phd_projects/entries/BetckeM_inverseproblems.html#project-description",
    "title": "All-at-once deep learning methods for nonlinear PDE based inverse problems",
    "section": "",
    "text": "In inverse problems (IPs) a natural role for deep learning (DL) is to encode prior information contained in the training set e.g. anatomical and pathological similarities between patients. Learned reconstruction methods realise this in different ways learning a prior/proximal operator for use within an optimisation scheme or a post-processing correction or a pseudo-inverse e.g. via an unrolled scheme (see [1] for an overview of such methods, [2] for methods with convergence guarantees, [3, 4] for examples our group’s work in the context of Photoacoustic tomography). While general PDE solvers are designed to work for all coefficients (in an appropriate function space), for the PDEs modelling the forward operator of a nonlinear IP we may have additional information about the distribution of the coefficients which can be used to improve performance. This idea underpins approaches based on unrolling of an iterative forward solver e.g., specifically in the context of solution of high frequency Helmholtz equation, GMRES with the net acting as a (flexible) parametric preconditioner [5], or iterates of a learned Born series [6], and more generic neural operators which directly construct a neural network approximation to the Green’s function of a linear PDE as a function of its coefficients (see e.g. [7,8,9]).\n\n\n\nThis project will combine both these strands of research. We will seek to optimise the forward solver’s performance for the coefficients on the manifold of the prior which is akin to adaptivity in classical PDE solvers and combine it optimally with the leaned prior into an all-at-once inversion procedure. Possible approaches to construction of such an all-at-once framework include: Unrolling and parametrisation of a nonlinear solver, e.g. Gauss-Newton, for minimising a regularised nonlinear data fitting functional. The Fréchet derivative of the nonlinear data term involves solution of the forward PDE and its adjoint which can be replaced by neural operators (or forward unrolling). The regularisation functional can be replaced with an input convex neural network, or for non-smooth priors, the prox of the regularisation functional can be replaced with a neural network. The whole assembly can then be jointly trained on the same training set following one of the training paradigms e.g. variational, fix point or adversarial. Another formulation can be proposed based on a PDE constraint optimisation framework which opts to minimise the regularised linear data consistency term subject to PDE constraints. The first order optimality conditions contain the Fréchet derivative, the forward and adjoint PDE solves, and the derivative of the regularisation functional, which again can be parametrised similarly as described above and used e.g. as a direction within first order optimisation method. As a part of the project we will investigate and develop suitable neural network architectures e.g. for the forward/adjoint neural operator pairs, Fréchet derivatives, regularisation functionals or their proxes, and efficient training strategies.\n\n\n\nThe objective of the project is to spearhead the paradigm of hybrid learned and model based all-at-once methods paying equal attention to utilising available data sets to aid solution of both the forward and the inverse problem involved. To push the paradigm into the wider inverse problems community of researchers and practitioners a high-quality accompanying software package is necessary. Along the core functionality a.k.a. the all-at-once frameworks, the package will include implementations of state-of-the-art hybrid model and data based forward solvers interfacing to popular PDE software e.g., https://ngsolve.org/, https://fenicsproject.org/, http://www.k-wave.org/ to allow flexibility of the underlying PDEs and the method of their solution. We will include implementations of the state-of-the-art neural operators, learned and analytical proximal/regularisation operators and optimisation methods, and will investigate feasibility of interfacing to popular inverse problems packages such as ODL https://odlgroup.github.io/odl/.\n[1] Arridge, Simon, Maass, Peter, Ozan, Öktem, Schönlieb, Carola-Bibiane. (2019). Solving inverse problems using data-driven models. Acta Numerica. 28. 1-174.\n[2] S. Mukherjee, A. Hauptmann, O. Öktem, M. Pereyra and C. -B. Schönlieb, Learned Reconstruction Methods With Convergence Guarantees: A survey of concepts and applications. IEEE Signal Processing Magazine, vol. 40, no. 1, pp. 164-182, Jan. 2023.\n[3] Hauptmann A, Lucka F, Betcke M, Huynh N, Adler J, Cox B, Beard P, Ourselin S, Arridge S. Model-based learning for accelerated, limited-view 3-D photoacoustic tomography. IEEE Transactions on Medical Imaging. 2018 Mar 29;37(6):1382-93.\n[4] Bolin Pan and Marta M. Betcke, On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector, SIAM Journal on Imaging Sciences 2023 16:2, 770-801\n[5] Stanziola, A., Arridge, S. R., Cox, B. T., & Treeby, B. E. (2021). A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. Journal of Computational Physics, 441, 110430.\n[6] Antonio Stanziola, Simon Arridge, Ben T. Cox, Bradley E. Treeby; A learned Born series for highly-scattering media. JASA Express Lett. 1 May 2023; 3 (5): 052401.\n[7] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. ICLR International Conference on Learning Representations (2021). https://openreview.net/forum?id=c8P9NQVtmnO.\n[8] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, Andrew Stuart. Neural Operator: Graph Kernel Network for Partial Differential Equations. ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (2019), https://openreview.net/forum?id=fg2ZFmXFO3.\n[9] Boullé, N., Townsend, A. Learning Elliptic Partial Differential Equations with Randomized Linear Algebra. Found Comput Math 23, 709–739 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Ni_StochasticPDE.html",
    "href": "phd_projects/entries/Ni_StochasticPDE.html",
    "title": "Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach",
    "section": "",
    "text": "Stochastic partial differential equations (SPDEs) are power mathematical tools to model random spatiotemporal dynamics, with broader applications ranging from weather forecasting to fluid dynamics. Machine learning (ML)-based approaches are emerging as it can be used to effectively handle the curse of dimensionality of traditional numerical methods. It can lead to the high-performing SPDE solver with significant computational acceleration. Moreover, the generalization of physics- informed neural networks from PDEs to SPDEs may provide a novel family of neural networks for analysing noisy spatiotemporal data, which can be used to discover the hidden physics law and predict the future data evolution. Most of ML algorithms for learning SPDEs fall into the supervised learning category, where the input and output pairs are “observable data” consisted of driving noise and the corresponding solution trajectories. These data are simulated by high quality numerical solver. One important aspect of improving ML algorithms is to design neural networks to better approximate the SPDE solution map [1 – 3]. Among them, our proposed Deep latent regularity network incorporates Regularity structure, a groundbreaking work on SPDEs by the Fields medallist Martin Hairer, to design neural networks. This model demonstrates superior performance in terms of accuracy and inferences time on various SPDEs such as the stochastic 2D Navier-Stokes equation.\n\n\n\nIn this project, we address the challenge of learning solutions to SPDEs using only observable solution trajectory samples. This approach is crucial for realistically modeling spatio-temporal dynamics in real-world applications, like fluid dynamics, where the driving noise is often unobservable. Our primary goal is to develop an unsupervised learning method for solving parabolic stochastic PDEs. To this end, we explore the generalization of Weak Adversarial Networks (WANs) [4, 5] —originally an unsupervised method for learning PDE solutions inspired by their weak solution—to the case of SPDEs. To achieve the superior accuracy and efficiency, we will design the suitable physics-informed neural networks (such as DLR net [3] or neural SPDEs [2]), which are capable of effectively approximate the primal network for SPDE solution and the dual network for the weak solution.\nThe main objectives of the projects are two folds:\n\nto design the unsupervised learning methodology for solving parabolic SPDEs based on WANs and establish the theoretical foundations for the proposed networks;\no validate the effectiveness of the proposed model on a number of SPDE examples by benchmarking with the conventional SPDE solvers and state-of-the-art ML models.\n\n\n\n\nThe expected deliverables are outlined as follows:\n\nData: Simulate the numerical solutions of the SPDE examples, such as the dynamic model ϕ_1^4 in [1] and the stochastic 2D Navier-Stokes equation [2].\nCodes: Implement the python toolbox using PyTorch for the proposed ML methods to solve parabolic SPDEs and conduct numerical experiments for model comparison.\n\nReference\n[1]. Zhang, D., Guo, L. and Karniadakis, G.E., 2020. Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. SIAM Journal on Scientific Computing, 42(2), pp.A639-A665.\n[2]. Salvi, C., Lemercier, M. and Gerasimovics, A., 2022. Neural stochastic PDEs: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35, pp.1333-1344.\n[3]. Gong, S., Hu, P., Meng, Q., Wang, Y., Zhu, R., Chen, B., Ma, Z., Ni, H. and Liu, T.Y., 2023, June. Deep latent regularity network for modeling stochastic partial differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7740-7747).\n[4]. Zang, Y., Bao, G., Ye, X. and Zhou, H., 2020. Weak adversarial networks for high-dimensional partial differential equations. Journal of Computational Physics, 411, p.109409.\n[5]. Oliva, P.V., Wu, Y., He, C. and Ni, H., 2022. Towards fast weak adversarial training to solve high dimensional parabolic partial differential equations using XNODE-WAN. Journal of Computational Physics, 463, p.111233.\n[6]. Chevyrev, Ilya, Andris Gerasimovics, and Hendrik Weber. “Feature engineering with regularity structures.” arXiv preprint arXiv:2108.05879 (2021)."
  },
  {
    "objectID": "phd_projects/entries/Ni_StochasticPDE.html#project-description",
    "href": "phd_projects/entries/Ni_StochasticPDE.html#project-description",
    "title": "Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach",
    "section": "",
    "text": "Stochastic partial differential equations (SPDEs) are power mathematical tools to model random spatiotemporal dynamics, with broader applications ranging from weather forecasting to fluid dynamics. Machine learning (ML)-based approaches are emerging as it can be used to effectively handle the curse of dimensionality of traditional numerical methods. It can lead to the high-performing SPDE solver with significant computational acceleration. Moreover, the generalization of physics- informed neural networks from PDEs to SPDEs may provide a novel family of neural networks for analysing noisy spatiotemporal data, which can be used to discover the hidden physics law and predict the future data evolution. Most of ML algorithms for learning SPDEs fall into the supervised learning category, where the input and output pairs are “observable data” consisted of driving noise and the corresponding solution trajectories. These data are simulated by high quality numerical solver. One important aspect of improving ML algorithms is to design neural networks to better approximate the SPDE solution map [1 – 3]. Among them, our proposed Deep latent regularity network incorporates Regularity structure, a groundbreaking work on SPDEs by the Fields medallist Martin Hairer, to design neural networks. This model demonstrates superior performance in terms of accuracy and inferences time on various SPDEs such as the stochastic 2D Navier-Stokes equation.\n\n\n\nIn this project, we address the challenge of learning solutions to SPDEs using only observable solution trajectory samples. This approach is crucial for realistically modeling spatio-temporal dynamics in real-world applications, like fluid dynamics, where the driving noise is often unobservable. Our primary goal is to develop an unsupervised learning method for solving parabolic stochastic PDEs. To this end, we explore the generalization of Weak Adversarial Networks (WANs) [4, 5] —originally an unsupervised method for learning PDE solutions inspired by their weak solution—to the case of SPDEs. To achieve the superior accuracy and efficiency, we will design the suitable physics-informed neural networks (such as DLR net [3] or neural SPDEs [2]), which are capable of effectively approximate the primal network for SPDE solution and the dual network for the weak solution.\nThe main objectives of the projects are two folds:\n\nto design the unsupervised learning methodology for solving parabolic SPDEs based on WANs and establish the theoretical foundations for the proposed networks;\no validate the effectiveness of the proposed model on a number of SPDE examples by benchmarking with the conventional SPDE solvers and state-of-the-art ML models.\n\n\n\n\nThe expected deliverables are outlined as follows:\n\nData: Simulate the numerical solutions of the SPDE examples, such as the dynamic model ϕ_1^4 in [1] and the stochastic 2D Navier-Stokes equation [2].\nCodes: Implement the python toolbox using PyTorch for the proposed ML methods to solve parabolic SPDEs and conduct numerical experiments for model comparison.\n\nReference\n[1]. Zhang, D., Guo, L. and Karniadakis, G.E., 2020. Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. SIAM Journal on Scientific Computing, 42(2), pp.A639-A665.\n[2]. Salvi, C., Lemercier, M. and Gerasimovics, A., 2022. Neural stochastic PDEs: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35, pp.1333-1344.\n[3]. Gong, S., Hu, P., Meng, Q., Wang, Y., Zhu, R., Chen, B., Ma, Z., Ni, H. and Liu, T.Y., 2023, June. Deep latent regularity network for modeling stochastic partial differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7740-7747).\n[4]. Zang, Y., Bao, G., Ye, X. and Zhou, H., 2020. Weak adversarial networks for high-dimensional partial differential equations. Journal of Computational Physics, 411, p.109409.\n[5]. Oliva, P.V., Wu, Y., He, C. and Ni, H., 2022. Towards fast weak adversarial training to solve high dimensional parabolic partial differential equations using XNODE-WAN. Journal of Computational Physics, 463, p.111233.\n[6]. Chevyrev, Ilya, Andris Gerasimovics, and Hendrik Weber. “Feature engineering with regularity structures.” arXiv preprint arXiv:2108.05879 (2021)."
  },
  {
    "objectID": "phd_projects/entries/bertrand1.html",
    "href": "phd_projects/entries/bertrand1.html",
    "title": "Stochastic resetting in many-body interacting particle systems",
    "section": "",
    "text": "Large systems of interacting particles are central to many applications across natural and social sciences. In physics, particles may represent ions in a plasma, molecules in a passive or active fluids, or galaxies in a cosmological model, while in biology, they often represent microorganisms like eukaryotic cells or bacteria that can exhibit complex behaviours. In economics and social sciences, particles typically represent individual agents like investors or institutions in a model of financial markets or individuals and communities in models of opinion formation. In these systems, robust emergent behaviour often arises even from very simple rules of interaction. Paradigmatic examples in systems of interacting active particles include motility induced phase separation and non-trivial swarming behaviour. A major challenge is to reduce the mathematical complexity of such systems by studying them at a coarse-grained level rather than at the level of single agents.\nA classical approach is to derive a macroscopic ic model that provides a continuous description of the dynamics in terms of global densities evolving according to non-linear partial differential equations. Such kinetic formulations date back to the foundations of statistical mechanics and the Boltzmann equation of dilute gases interacting via direct collisions. This is in general a complicated task and important (often uncontrolled) approximations need to be made. In recent years, however, much of the focus has been on the mean-field limit of particles with long range or collisionless interactions. Two paradigmatic examples are interacting Brownian particles in the overdamped regime and the Kuramoto model of coupled phase oscillators.\nFinally, the concept of stochastic resetting has recently emerged. Stochastic resetting is the process in which a system, such as a diffusive particle, is intermittently “reset” to an initial state, thereby restarting its evolution at stochastic times. Stochastic resetting has recently been under intense scrutiny because it has been shown to enhance search efficiency, create non-equilibrium steady states (NESS), and offer insights into a wide range of processes, from chemical reactions to biological foraging behaviours in a mathematically tractable framework. However, almost all previous studies of stochastic resetting have focused on single-particle systems.\n\n\nThe main goal of this project is to use a combination of mean-field theory, coarse-graining techniques, dimensional reduction, and agent-based numerical simulations to explore the effects of stochastic resetting on large-scale interacting particle systems, including both Kuramoto-based oscillator networks and systems of passive/active particles. Topics of interest include the following:\n• Existence of NESS in systems of interacting particles under stochastic resetting – First, we will investigate the existence of a NESS for the population density PDE of an interacting particle system with local resetting and pairwise interactions. We will ask whether the NESS exhibits phase transitions along analogous lines to previous studies of Brownian gases without resetting.\n• Exploring differences between local and global resetting – under local resetting each particle is independently reset following its own sequence of times, while in global resetting all particles are simultaneously reset. In the latter case, the resulting PDE for the population density is itself subject to resetting. That is, mean field theory breaks down and statistical correlations between the particles arise even in the absence of interactions. We aim to develop new analytical strategies to derive PDE descriptions of these systems, strategies which will be informed by our large-scale simulations.\n• Bridging local and global resetting – in a variety of models, particles can be organized in subsystems (i.e. communities on network-based Kuramoto systems or clusters arising in systems of interacting active Brownian particles). We will introduce the concept of subsystem resetting, in which subsystems can be reset simultaneously leaving the rest of the system unchanged. We will explore the conditions under which subsystem resetting can induce global resetting. Focusing on the Kuramoto model, we will ask whether subsystem resetting can induce system spanning correlation and global synchronization. Using both analytical and numerical methods (like genetic algorithms), we devise strategies to design network topologies which optimize the emergence of synchronization from subsystem resetting.\n• Extrinsic vs intrinsic coupling – In large interacting particle systems, the coupling between individual particles can either be “intrinsic” (i.e. direct pairwise interactions) or “extrinsic” (i.e. mediated by a common external medium). An example of extrinsic particle-particle interactions would be the quorum sensing observed in bacterial colonies. We are interested in comparing the emergent collective dynamics observed in the case of systems with intrinsic and extrinsic interactions.\n• Passive vs active particles resetting – For passive Brownian particles, the state of each particle is simply defined to be its position. On the other hand, for an active particle it is necessary to specify both its position and velocity state (or at least its orientation). We will explore how the choice of resetting protocol affects the collective behaviour exhibited by these systems.\n• Finite-size effects – In all the studies, we will investigate numerically the breakdown of mean field theory as the number N of interacting particles decreases. To do so, we will focus on understanding how macroscopic observables scale with system size.\n\n\n\nThe success of this project will rely on the development of: 1. numerical algorithms for a large-scale computational exploration of a variety of minimal systems in statistical mechanics; 2. development of efficient numerical algorithms for agent-based modelling both on networks (in the context of the Kuramoto model) and off-lattice (for simulations of passive and active particles systems); 3. purpose-built, scalable and adaptable software implementing advanced numerical solutions to highly nonlinear systems of PDEs and SPDEs; 4. development of genetic algorithms to solve the inverse problem of finding the network structure of our Kuramoto model which optimizes global synchronization from the smallest subsystem resetting."
  },
  {
    "objectID": "phd_projects/entries/bertrand1.html#project-description",
    "href": "phd_projects/entries/bertrand1.html#project-description",
    "title": "Stochastic resetting in many-body interacting particle systems",
    "section": "",
    "text": "Large systems of interacting particles are central to many applications across natural and social sciences. In physics, particles may represent ions in a plasma, molecules in a passive or active fluids, or galaxies in a cosmological model, while in biology, they often represent microorganisms like eukaryotic cells or bacteria that can exhibit complex behaviours. In economics and social sciences, particles typically represent individual agents like investors or institutions in a model of financial markets or individuals and communities in models of opinion formation. In these systems, robust emergent behaviour often arises even from very simple rules of interaction. Paradigmatic examples in systems of interacting active particles include motility induced phase separation and non-trivial swarming behaviour. A major challenge is to reduce the mathematical complexity of such systems by studying them at a coarse-grained level rather than at the level of single agents.\nA classical approach is to derive a macroscopic ic model that provides a continuous description of the dynamics in terms of global densities evolving according to non-linear partial differential equations. Such kinetic formulations date back to the foundations of statistical mechanics and the Boltzmann equation of dilute gases interacting via direct collisions. This is in general a complicated task and important (often uncontrolled) approximations need to be made. In recent years, however, much of the focus has been on the mean-field limit of particles with long range or collisionless interactions. Two paradigmatic examples are interacting Brownian particles in the overdamped regime and the Kuramoto model of coupled phase oscillators.\nFinally, the concept of stochastic resetting has recently emerged. Stochastic resetting is the process in which a system, such as a diffusive particle, is intermittently “reset” to an initial state, thereby restarting its evolution at stochastic times. Stochastic resetting has recently been under intense scrutiny because it has been shown to enhance search efficiency, create non-equilibrium steady states (NESS), and offer insights into a wide range of processes, from chemical reactions to biological foraging behaviours in a mathematically tractable framework. However, almost all previous studies of stochastic resetting have focused on single-particle systems.\n\n\nThe main goal of this project is to use a combination of mean-field theory, coarse-graining techniques, dimensional reduction, and agent-based numerical simulations to explore the effects of stochastic resetting on large-scale interacting particle systems, including both Kuramoto-based oscillator networks and systems of passive/active particles. Topics of interest include the following:\n• Existence of NESS in systems of interacting particles under stochastic resetting – First, we will investigate the existence of a NESS for the population density PDE of an interacting particle system with local resetting and pairwise interactions. We will ask whether the NESS exhibits phase transitions along analogous lines to previous studies of Brownian gases without resetting.\n• Exploring differences between local and global resetting – under local resetting each particle is independently reset following its own sequence of times, while in global resetting all particles are simultaneously reset. In the latter case, the resulting PDE for the population density is itself subject to resetting. That is, mean field theory breaks down and statistical correlations between the particles arise even in the absence of interactions. We aim to develop new analytical strategies to derive PDE descriptions of these systems, strategies which will be informed by our large-scale simulations.\n• Bridging local and global resetting – in a variety of models, particles can be organized in subsystems (i.e. communities on network-based Kuramoto systems or clusters arising in systems of interacting active Brownian particles). We will introduce the concept of subsystem resetting, in which subsystems can be reset simultaneously leaving the rest of the system unchanged. We will explore the conditions under which subsystem resetting can induce global resetting. Focusing on the Kuramoto model, we will ask whether subsystem resetting can induce system spanning correlation and global synchronization. Using both analytical and numerical methods (like genetic algorithms), we devise strategies to design network topologies which optimize the emergence of synchronization from subsystem resetting.\n• Extrinsic vs intrinsic coupling – In large interacting particle systems, the coupling between individual particles can either be “intrinsic” (i.e. direct pairwise interactions) or “extrinsic” (i.e. mediated by a common external medium). An example of extrinsic particle-particle interactions would be the quorum sensing observed in bacterial colonies. We are interested in comparing the emergent collective dynamics observed in the case of systems with intrinsic and extrinsic interactions.\n• Passive vs active particles resetting – For passive Brownian particles, the state of each particle is simply defined to be its position. On the other hand, for an active particle it is necessary to specify both its position and velocity state (or at least its orientation). We will explore how the choice of resetting protocol affects the collective behaviour exhibited by these systems.\n• Finite-size effects – In all the studies, we will investigate numerically the breakdown of mean field theory as the number N of interacting particles decreases. To do so, we will focus on understanding how macroscopic observables scale with system size.\n\n\n\nThe success of this project will rely on the development of: 1. numerical algorithms for a large-scale computational exploration of a variety of minimal systems in statistical mechanics; 2. development of efficient numerical algorithms for agent-based modelling both on networks (in the context of the Kuramoto model) and off-lattice (for simulations of passive and active particles systems); 3. purpose-built, scalable and adaptable software implementing advanced numerical solutions to highly nonlinear systems of PDEs and SPDEs; 4. development of genetic algorithms to solve the inverse problem of finding the network structure of our Kuramoto model which optimizes global synchronization from the smallest subsystem resetting."
  },
  {
    "objectID": "phd_projects/entries/Tobar_machinelearning.html",
    "href": "phd_projects/entries/Tobar_machinelearning.html",
    "title": "Optimal transport for probabilistic machine learning",
    "section": "",
    "text": "During the last decade, optimal transport (OT) has penetrated the core technical aspects of machine learning (ML). In particular, OT’s ability to define meaningful distances among generative models has allowed to design better, ad hoc, learning strategies for models defined over complex data structures.\nWe have developed OT methods, including Wasserstein-inspired distances and novel types of barycentres, for Gaussian processes, time series analysis, Bayesian model selection, outlier detection, trajectory tracking, natural language processing, and clustering of distributions.\n\n\n\nTo design and validate learning strategies and architectures for probabilistic generative models using concepts and resources from OT. Likewise, to explore the use and benefits that ML methods bring to the computation of OT. The project includes both theoretical and applied (computational) aspects.\n\nTo explore the state of the art in the interface between computational OT and probabilistic machine learning\nTo identify which aspects of learning strategies, or model architectures, can be enhanced via OT\nTo devise directions in which ML can improve OT computation (e.g., speed and robustness)\nTo provide theoretical guarantees for the developed solutions\nTo produce experimental validation of the proposed methodologies for general applied subjects (e.g., climate, astronomy, audio, social sciences, health)\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML communities\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing particular applications to scientific or social challenges."
  },
  {
    "objectID": "phd_projects/entries/Tobar_machinelearning.html#project-description",
    "href": "phd_projects/entries/Tobar_machinelearning.html#project-description",
    "title": "Optimal transport for probabilistic machine learning",
    "section": "",
    "text": "During the last decade, optimal transport (OT) has penetrated the core technical aspects of machine learning (ML). In particular, OT’s ability to define meaningful distances among generative models has allowed to design better, ad hoc, learning strategies for models defined over complex data structures.\nWe have developed OT methods, including Wasserstein-inspired distances and novel types of barycentres, for Gaussian processes, time series analysis, Bayesian model selection, outlier detection, trajectory tracking, natural language processing, and clustering of distributions.\n\n\n\nTo design and validate learning strategies and architectures for probabilistic generative models using concepts and resources from OT. Likewise, to explore the use and benefits that ML methods bring to the computation of OT. The project includes both theoretical and applied (computational) aspects.\n\nTo explore the state of the art in the interface between computational OT and probabilistic machine learning\nTo identify which aspects of learning strategies, or model architectures, can be enhanced via OT\nTo devise directions in which ML can improve OT computation (e.g., speed and robustness)\nTo provide theoretical guarantees for the developed solutions\nTo produce experimental validation of the proposed methodologies for general applied subjects (e.g., climate, astronomy, audio, social sciences, health)\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML communities\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing particular applications to scientific or social challenges."
  },
  {
    "objectID": "phd_projects/entries/gelat.html",
    "href": "phd_projects/entries/gelat.html",
    "title": "Large-scale high-performance solver for therapeutic ultrasound applications",
    "section": "",
    "text": "The supervisory team, together with Dr Elwin van ’t Wout (Pontificia Universidad Católica de Chile), has been developing the open-source Python library OptimUS [1] for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney [2] as well as osteoid osteoma [3]. OptimUS featured as part of an international software benchmarking exercise [4] for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic affiliations worldwide as well as a pedagogical tool for undergraduate and postgraduate students. OptimUS leverages the Bempp kernel developed by Prof Timo Betcke (UCL Department of Mathematics).\nUsing OptimUS to solve large biomedical problems at high frequencies is currently challenging due to the large RAM consumption required, which is of the order of GigaByte to TeraByte. A dedicated high-frequency solver is therefore required to achieve realistic simulations at the operational MHz frequencies relevant to laboratory and clinical biomedical ultrasound applications. A promising kernel-independent Fast Multipole Method (FMM) BEM has recently been developed for the next release of the Bempp library and has been used to solve the Laplace equation. Research into dedicated high-frequency FMM implementation is needed to achieve BEM simulations on a larger scale than other numerical approaches. BEM has a distinct advantage over finite element and finite-difference time domain schemes as it suffers from only minimal numerical dispersion and pollution effects. It is therefore anticipated that the development of a high-performance FMM BEM formulation for Helmholtz kernels will be transformative in the field of biomedical ultrasound.\n• Main objectives of the project This project aims to develop an efficient FMM BEM formulation for high-frequency Helmholtz kernels. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from MRI/CT images. This formulation will also be used as a basis for treating the physics of weakly nonlinear wave propagation in tissue, which requires solving inhomogeneous Helmholtz equations for higher order harmonics. The performance of this new solver will be benchmarked against existing CPU implementations and will also be tested alongside the spectral element method solver recently developed by external partner Prof Garth Wells’ team at University of Cambridge. It will also be benchmarked against other toolboxes used by the biomedical ultrasound community (e.g. k-Wave)\nThe successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as acoustic tomography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its wider adoption in clinical settings, where it could be used for personalised treatment plans based on anatomical models derived from MRI/CT scans, ultimately improving patient outcomes.\nThe supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art facilities required for the successful delivery of this project. These facilities include high-performance computing workstations as well as the UCL Research Computing Platforms Service.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of new BEM formulations for solving ultrasound waves in piecewise homogeneous and heterogeneous domains. The student will be actively involved in software development using Rust and Python, employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in focused ultrasound treatment platforms used in the clinic. References\n[1] Gélat, P., Haqshenas, S. R., and van ′t Wout, E., “OptimUS: A Python library for solving 3D acoustic wave propagation,” https://github.com/optimuslib/optimus.\n[2] Haqshenas, S.R., Gélat, P., van’t Wout, E., Betcke, T. and Saffari, N., 2021. A fast full-wave solver for calculating ultrasound propagation in the body. Ultrasonics, 110, p.106240.\n[3] van’t Wout, E., Haqshenas, S.R., Gélat, P., Betcke, T. and Saffari, N., 2022. Frequency-robust preconditioning of boundary integral equations for acoustic transmission. Journal of Computational Physics, 462, p.111229.\n[4] Aubry, J.F., Bates, O., Boehm, C., Butts Pauly, K., Christensen, D., Cueto, C., Gélat, P., Guasch, L., Jaros, J., Jing, Y. and Jones, R., 2022. Benchmark problems for transcranial ultrasound simulation: Intercomparison of compressional wave models. The Journal of the Acoustical Society of America, 152(2), pp.1003-1019."
  },
  {
    "objectID": "phd_projects/entries/gelat.html#project-description",
    "href": "phd_projects/entries/gelat.html#project-description",
    "title": "Large-scale high-performance solver for therapeutic ultrasound applications",
    "section": "",
    "text": "The supervisory team, together with Dr Elwin van ’t Wout (Pontificia Universidad Católica de Chile), has been developing the open-source Python library OptimUS [1] for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney [2] as well as osteoid osteoma [3]. OptimUS featured as part of an international software benchmarking exercise [4] for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic affiliations worldwide as well as a pedagogical tool for undergraduate and postgraduate students. OptimUS leverages the Bempp kernel developed by Prof Timo Betcke (UCL Department of Mathematics).\nUsing OptimUS to solve large biomedical problems at high frequencies is currently challenging due to the large RAM consumption required, which is of the order of GigaByte to TeraByte. A dedicated high-frequency solver is therefore required to achieve realistic simulations at the operational MHz frequencies relevant to laboratory and clinical biomedical ultrasound applications. A promising kernel-independent Fast Multipole Method (FMM) BEM has recently been developed for the next release of the Bempp library and has been used to solve the Laplace equation. Research into dedicated high-frequency FMM implementation is needed to achieve BEM simulations on a larger scale than other numerical approaches. BEM has a distinct advantage over finite element and finite-difference time domain schemes as it suffers from only minimal numerical dispersion and pollution effects. It is therefore anticipated that the development of a high-performance FMM BEM formulation for Helmholtz kernels will be transformative in the field of biomedical ultrasound.\n• Main objectives of the project This project aims to develop an efficient FMM BEM formulation for high-frequency Helmholtz kernels. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from MRI/CT images. This formulation will also be used as a basis for treating the physics of weakly nonlinear wave propagation in tissue, which requires solving inhomogeneous Helmholtz equations for higher order harmonics. The performance of this new solver will be benchmarked against existing CPU implementations and will also be tested alongside the spectral element method solver recently developed by external partner Prof Garth Wells’ team at University of Cambridge. It will also be benchmarked against other toolboxes used by the biomedical ultrasound community (e.g. k-Wave)\nThe successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as acoustic tomography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its wider adoption in clinical settings, where it could be used for personalised treatment plans based on anatomical models derived from MRI/CT scans, ultimately improving patient outcomes.\nThe supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art facilities required for the successful delivery of this project. These facilities include high-performance computing workstations as well as the UCL Research Computing Platforms Service.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of new BEM formulations for solving ultrasound waves in piecewise homogeneous and heterogeneous domains. The student will be actively involved in software development using Rust and Python, employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in focused ultrasound treatment platforms used in the clinic. References\n[1] Gélat, P., Haqshenas, S. R., and van ′t Wout, E., “OptimUS: A Python library for solving 3D acoustic wave propagation,” https://github.com/optimuslib/optimus.\n[2] Haqshenas, S.R., Gélat, P., van’t Wout, E., Betcke, T. and Saffari, N., 2021. A fast full-wave solver for calculating ultrasound propagation in the body. Ultrasonics, 110, p.106240.\n[3] van’t Wout, E., Haqshenas, S.R., Gélat, P., Betcke, T. and Saffari, N., 2022. Frequency-robust preconditioning of boundary integral equations for acoustic transmission. Journal of Computational Physics, 462, p.111229.\n[4] Aubry, J.F., Bates, O., Boehm, C., Butts Pauly, K., Christensen, D., Cueto, C., Gélat, P., Guasch, L., Jaros, J., Jing, Y. and Jones, R., 2022. Benchmark problems for transcranial ultrasound simulation: Intercomparison of compressional wave models. The Journal of the Acoustical Society of America, 152(2), pp.1003-1019."
  },
  {
    "objectID": "phd_projects/entries/huthwaite.html",
    "href": "phd_projects/entries/huthwaite.html",
    "title": "Developing multi-physics, multi-scale wave modelling on graphics cards",
    "section": "",
    "text": "The use of the time-explicit FE method for simulating ultrasound in many engineering components has been demonstrated to be accurate for propagation within metals, and the use graphics cards such as through software packages like Pogo (developed in the Imperial NDE group) enables the algorithm to be solved efficiently. The goal of this work is to improve the accuracy in more complex scenarios, where both multi-scale and multi-physics behaviour become important. An example of this is ultrasound within human tissue, where there are a wide range of scales from the body itself down to sub-cell structures, and with properties vary from liquid to solid (supporting shear and longitudinal waves) with variable complex anisotropy and periodic structures. The intention here is to develop an efficient integrated solution for simulating wave interaction with such structures. Development of multi-physics will enable more complex physical behaviour to be captured enabling greater understanding of various phenomena, such as the low speed shear wave (1-3m/s) seen in tissue and used for elastography (compared to the 1500m/s for the longitudinal wave of general interest). This also has applications in the emulsions and structures used in batteries, an important new application of ultrasound, to test battery integrity.\n\n\nThis will build upon the Pogo software package, which has been used for simulating ultrasound within NDE (Non-Destructive Evaluation) for over 10 years and is now used throughout the world in industry and academia.\nDr Huthwaite has extensive experience with a variety of approximations for simulating waves in different materials, including ray theory (eikonal equation) and Born approximation-type approaches, as well as developing the FE method on graphics cards. He has been involved in techniques to model complex solid media such as large-grained material, developing models of over 1 billion degrees of freedom. Some multi-scale work has been done in the broader research community, typically with small-scale evaluations to estimate homogenised properties, which are then used for larger simulations. Multi-physics approaches have also been utilised and are common in areas such as transduction where different phenomena need to be coupled, but integrating these with multi-scale techniques presents a new challenge.\n\n\n\nDeliver an efficient approach for accurate modelling of wave propagation through multiscale, multi-physics problems. Apply the method to propagation through complex structures, such as tissue, and ideally compare to experimental measurements.\n\n\n\nThe outcomes will be incorporated into the Pogo software package for application across a variety of areas including NDE, medicine and underwater ultrasound. Describe coding and data developments during the project. Specific techniques will need to be developed to code the method to achieve results without compromising GPU performance."
  },
  {
    "objectID": "phd_projects/entries/huthwaite.html#project-description",
    "href": "phd_projects/entries/huthwaite.html#project-description",
    "title": "Developing multi-physics, multi-scale wave modelling on graphics cards",
    "section": "",
    "text": "The use of the time-explicit FE method for simulating ultrasound in many engineering components has been demonstrated to be accurate for propagation within metals, and the use graphics cards such as through software packages like Pogo (developed in the Imperial NDE group) enables the algorithm to be solved efficiently. The goal of this work is to improve the accuracy in more complex scenarios, where both multi-scale and multi-physics behaviour become important. An example of this is ultrasound within human tissue, where there are a wide range of scales from the body itself down to sub-cell structures, and with properties vary from liquid to solid (supporting shear and longitudinal waves) with variable complex anisotropy and periodic structures. The intention here is to develop an efficient integrated solution for simulating wave interaction with such structures. Development of multi-physics will enable more complex physical behaviour to be captured enabling greater understanding of various phenomena, such as the low speed shear wave (1-3m/s) seen in tissue and used for elastography (compared to the 1500m/s for the longitudinal wave of general interest). This also has applications in the emulsions and structures used in batteries, an important new application of ultrasound, to test battery integrity.\n\n\nThis will build upon the Pogo software package, which has been used for simulating ultrasound within NDE (Non-Destructive Evaluation) for over 10 years and is now used throughout the world in industry and academia.\nDr Huthwaite has extensive experience with a variety of approximations for simulating waves in different materials, including ray theory (eikonal equation) and Born approximation-type approaches, as well as developing the FE method on graphics cards. He has been involved in techniques to model complex solid media such as large-grained material, developing models of over 1 billion degrees of freedom. Some multi-scale work has been done in the broader research community, typically with small-scale evaluations to estimate homogenised properties, which are then used for larger simulations. Multi-physics approaches have also been utilised and are common in areas such as transduction where different phenomena need to be coupled, but integrating these with multi-scale techniques presents a new challenge.\n\n\n\nDeliver an efficient approach for accurate modelling of wave propagation through multiscale, multi-physics problems. Apply the method to propagation through complex structures, such as tissue, and ideally compare to experimental measurements.\n\n\n\nThe outcomes will be incorporated into the Pogo software package for application across a variety of areas including NDE, medicine and underwater ultrasound. Describe coding and data developments during the project. Specific techniques will need to be developed to code the method to achieve results without compromising GPU performance."
  },
  {
    "objectID": "phd_projects/entries/Tennyson_TROVE.html",
    "href": "phd_projects/entries/Tennyson_TROVE.html",
    "title": "Re-griding the TROVE nuclear motion program",
    "section": "",
    "text": "Tennyson and Yurchenko have developed a series of programs for solving the quantum mechanical Schrodinger equation for the motion of nuclei for small molecules. These programs are highly efficient and widely used (eg they underpin the ERC funded ExoMol project). TROVE (Theoretical ROVibrational Energies, see https://spectrove.readthedocs.io) is the most flexible of these program as it can be extended to molecules of arbitrary size and complexity. However the number of degrees of freedom that need to be considered grows as 3N-6 where N is the number of atoms. At present TROVE us products of one-dimensional grids which becomes increasingly inefficient a N increases. In addition we have been systematically updating our methodology to treat processes which lead to continuum states of the molecule being occupied. Experience with small (N=2 or N=3) molecules shows that specialized grids, currently not implemented in TROVE. To tackled cases with N &gt; 3 will require regridding of TROVE.\n\n\n\nThe main objective will be to develop methods of using multidimensional grids in place of products on 1 dimension grids. Two versions of this will be developed: for bound state problems largely aimed at systems with N &gt; 6 and for problems which need to consider continuum states where at present we cannot address problems with N=4 despite many requests to work on such problems. Initial work focus on the possibility of using Smolyak grids but probably the project will explore other possibilities The main output will be a version of TROVE with greatly enhanced functionality. The student will have the opportunity to run calculations for key problems if they wish.\n\n\n\nTROVE has a github repository: https://github.com/Trovemaster/TROVE Original coding can be done independent of the main software as suitable grid algorithms are developed. The final version will then integrated in the main TROVE software package."
  },
  {
    "objectID": "phd_projects/entries/Tennyson_TROVE.html#project-description",
    "href": "phd_projects/entries/Tennyson_TROVE.html#project-description",
    "title": "Re-griding the TROVE nuclear motion program",
    "section": "",
    "text": "Tennyson and Yurchenko have developed a series of programs for solving the quantum mechanical Schrodinger equation for the motion of nuclei for small molecules. These programs are highly efficient and widely used (eg they underpin the ERC funded ExoMol project). TROVE (Theoretical ROVibrational Energies, see https://spectrove.readthedocs.io) is the most flexible of these program as it can be extended to molecules of arbitrary size and complexity. However the number of degrees of freedom that need to be considered grows as 3N-6 where N is the number of atoms. At present TROVE us products of one-dimensional grids which becomes increasingly inefficient a N increases. In addition we have been systematically updating our methodology to treat processes which lead to continuum states of the molecule being occupied. Experience with small (N=2 or N=3) molecules shows that specialized grids, currently not implemented in TROVE. To tackled cases with N &gt; 3 will require regridding of TROVE.\n\n\n\nThe main objective will be to develop methods of using multidimensional grids in place of products on 1 dimension grids. Two versions of this will be developed: for bound state problems largely aimed at systems with N &gt; 6 and for problems which need to consider continuum states where at present we cannot address problems with N=4 despite many requests to work on such problems. Initial work focus on the possibility of using Smolyak grids but probably the project will explore other possibilities The main output will be a version of TROVE with greatly enhanced functionality. The student will have the opportunity to run calculations for key problems if they wish.\n\n\n\nTROVE has a github repository: https://github.com/Trovemaster/TROVE Original coding can be done independent of the main software as suitable grid algorithms are developed. The final version will then integrated in the main TROVE software package."
  },
  {
    "objectID": "phd_projects/entries/peiro.html",
    "href": "phd_projects/entries/peiro.html",
    "title": "Reinforcement Learning for Full-Hexahedral Mesh Generation",
    "section": "",
    "text": "Machine learning and its inherent ability for pattern matching is proposed as an alternative to current state-of-the-art trial-and-error methods of hexahedral mesh generation that could potentially overcome their limitations and lead to its ultimate, yet unfulfilled, goal: a fully automatic full-hexahedral meshing tool.\n\n\nMesh generation is the scaffolding that supports modelling and simulation: an accurate and efficient simulation requires a high-quality mesh that appropriately captures the complex geometrical and physical features of the problem, whilst ensuring the stability of the numerical method employed for such simulation. Hexahedral elements are the preferred choice for the majority of applications in finite element analysis because their better approximation and stability prop- erties when compared with their tetrahedral counterparts. However, the lack of automatic, robust and reliable mesh generators of full hexahedral meshes means that mixed or full tetrahedral meshes must be used for discretizing complex ge- ometries. Current methods for generating complex, unstructured all-hexahedral meshes are heuristic and often require extensive user input form accumulated experience and time-consuming trial-and-error procedures. We will adopt state-of-the-art machine learning methods for deep reinforce- ment learning, such as Monte Carlo tree searching and its derivatives, that have demonstrated their ability to cope with the demands of ‘learning’ what it takes to win complex games such as chess or go, to identify winning strategies for the full-hexahedral mesh generation game. Despite recent interest in using ma- chine learning techniques for mesh generation, novelty here lies on viewing mesh generation as a ‘game’. These techniques will be used to perform and assess mesh topological mesh operations or ‘moves’. We will consider two main types of such operations: hexahedral-to-hexahedral operations aiming at improving overall mesh quality, and polyhedral-to-hexahedral operations to increase the percentage of converted hexahedra and their mesh quality. The idea of assimilating mesh generation to a game is new.\n\n\n\nIn the absence of a theoretically based holistic approach to full-hexahedral mesh generation, we seek to investigate machine learning techniques for improving the performance of state-of-the-art procedures for the topological modification of full-hexahedral or hex-dominant meshes with a view to achieve high-quality full-hexahedral meshes. The idea behind the proposed methodology is to view topological mesh modi- fication operations as ‘moves’ of a game with the aim of achieving full-hexahedral meshes of optimal a priori mesh quality. The main objectives of the work are:\n\nTo implement and train state-of-the-art machine learning methods based on deep reinforcement learning to ‘play’ the mesh generation ‘game’.\nTo identify the optimal ‘rules of the game’, or suitable criteria of a priori mesh quality.\nTo select a suitable set of ‘game moves’, or topological mesh modifications, and assess their performance according to the rules of the game.\nTo investigate the possibility of incorporating ‘sacrificial moves’ for im- proved performance, wherein a ‘move’ that gives initially lower quality in the short term ultimately results in a much higher-quality mesh after many ‘moves’.\n\n\n\n\nThis work will lead to the development of a robust full-hexahedral meshing ca- pability of interest to both academia and industry. The capability will integrate:\n\nA software implementation of a library for mesh modification operations: hexahedra-to-hexahedra and polyhedra-to-hexahedra. The library will be stand-alone and callable by existing mesh generators such as NekMesh, a general open-source high-order mesh generator under the Nektar++ spectral/hp element framework.\nUse of state-of-the-art software for machine learning, such as Tensorflow or pyTorch, for the development of the reinforcement learning of the ’hex- ahedral meshing game."
  },
  {
    "objectID": "phd_projects/entries/peiro.html#project-description",
    "href": "phd_projects/entries/peiro.html#project-description",
    "title": "Reinforcement Learning for Full-Hexahedral Mesh Generation",
    "section": "",
    "text": "Machine learning and its inherent ability for pattern matching is proposed as an alternative to current state-of-the-art trial-and-error methods of hexahedral mesh generation that could potentially overcome their limitations and lead to its ultimate, yet unfulfilled, goal: a fully automatic full-hexahedral meshing tool.\n\n\nMesh generation is the scaffolding that supports modelling and simulation: an accurate and efficient simulation requires a high-quality mesh that appropriately captures the complex geometrical and physical features of the problem, whilst ensuring the stability of the numerical method employed for such simulation. Hexahedral elements are the preferred choice for the majority of applications in finite element analysis because their better approximation and stability prop- erties when compared with their tetrahedral counterparts. However, the lack of automatic, robust and reliable mesh generators of full hexahedral meshes means that mixed or full tetrahedral meshes must be used for discretizing complex ge- ometries. Current methods for generating complex, unstructured all-hexahedral meshes are heuristic and often require extensive user input form accumulated experience and time-consuming trial-and-error procedures. We will adopt state-of-the-art machine learning methods for deep reinforce- ment learning, such as Monte Carlo tree searching and its derivatives, that have demonstrated their ability to cope with the demands of ‘learning’ what it takes to win complex games such as chess or go, to identify winning strategies for the full-hexahedral mesh generation game. Despite recent interest in using ma- chine learning techniques for mesh generation, novelty here lies on viewing mesh generation as a ‘game’. These techniques will be used to perform and assess mesh topological mesh operations or ‘moves’. We will consider two main types of such operations: hexahedral-to-hexahedral operations aiming at improving overall mesh quality, and polyhedral-to-hexahedral operations to increase the percentage of converted hexahedra and their mesh quality. The idea of assimilating mesh generation to a game is new.\n\n\n\nIn the absence of a theoretically based holistic approach to full-hexahedral mesh generation, we seek to investigate machine learning techniques for improving the performance of state-of-the-art procedures for the topological modification of full-hexahedral or hex-dominant meshes with a view to achieve high-quality full-hexahedral meshes. The idea behind the proposed methodology is to view topological mesh modi- fication operations as ‘moves’ of a game with the aim of achieving full-hexahedral meshes of optimal a priori mesh quality. The main objectives of the work are:\n\nTo implement and train state-of-the-art machine learning methods based on deep reinforcement learning to ‘play’ the mesh generation ‘game’.\nTo identify the optimal ‘rules of the game’, or suitable criteria of a priori mesh quality.\nTo select a suitable set of ‘game moves’, or topological mesh modifications, and assess their performance according to the rules of the game.\nTo investigate the possibility of incorporating ‘sacrificial moves’ for im- proved performance, wherein a ‘move’ that gives initially lower quality in the short term ultimately results in a much higher-quality mesh after many ‘moves’.\n\n\n\n\nThis work will lead to the development of a robust full-hexahedral meshing ca- pability of interest to both academia and industry. The capability will integrate:\n\nA software implementation of a library for mesh modification operations: hexahedra-to-hexahedra and polyhedra-to-hexahedra. The library will be stand-alone and callable by existing mesh generators such as NekMesh, a general open-source high-order mesh generator under the Nektar++ spectral/hp element framework.\nUse of state-of-the-art software for machine learning, such as Tensorflow or pyTorch, for the development of the reinforcement learning of the ’hex- ahedral meshing game."
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#existing-background-work",
    "href": "phd_projects/entries/Coveney_HemeLB.html#existing-background-work",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Existing background work",
    "text": "Existing background work\nHemeLB is a highly scalable lattice-Boltzmann code which manifests strong scaling on all CPU and GPU platforms where it is currently deployed. These include, inter alia, Frontier (currently the world’s only exascale computer), Summit, Aurora, LUMI, and Archer2. It is designed to model and simulate personalised blood flow throughout the entire human vasculature, from head to toe. A great deal of software engineering has been invested in getting the code to this performance level, which is almost unrivalled on a global basis. It may well feature as a science driver for the post-exascale world and to ensure that is the case we must continue to support and develop the code base. HemeLB has been developed over more than 15 years within Prof Peter Coveney’s group at the Centre for Computational Science. Its applications are wide-ranging, from smaller scale investigations of blood flow in aneurysms and arteriovenous malformations through to the latest simulations in which HemeLB is coupled to a state of the art model of the human heart (Alya from Barcelona Supercomputing Center). [1,2,3]"
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/Coveney_HemeLB.html#main-objectives-of-the-project",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\nIn this research, we will be involved in globally leading edge developments of the HemeLB code which will be influenced by discussions about the way supercomputer are going to be designed for the post exascale era – that is, a form of genuinely interactive co-design with our collaborators in the DoE Leadership Computing Computing Facilities at Oak Ridge and Argonne National Laboraties and associated computing companies.\nThe applications will continue to grow around modelling and simulating the virtual human. These applications will evolve in terms of optimising and enhancing the resolution of the simulations, rendering and visualising the output in situ on GPUs and providing a computational steering facility which can be used by both scientists and clinicians when running the code, so as to most effectively assess numerous “what if?” scenarios. The overall field of computational biomedicine is currently evolving fast in the direction of human digital twins (HDTs) and we expect the HemeLB software to be integral to these endeavours within research and clinical applications in the near future too. [4, 5]."
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Coveney_HemeLB.html#details-of-softwaredata-deliverables",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe student will need to become familiar with the substantial existing code base of HemeLB and its various development branches. Through this route, we would like to make available versions of the code that run effectively on Isambard-AI, Dawn and other UK based platforms, as well as on Frontier and Aurora and a number of European machines including LUMI and with our collaborators at Leibniz Rechenzentrum in Munich. Versions of the code which are suitable for inclusion in development of and support for computational steering and visualisation will be developed in collaboration with NVIDIA, particularly in the context of Isambard-AI, which will enter its production phase in the second half of 2024. HDT applications which engage with end-users, particularly clinicians and physiologists, will put a premium on the usability and ease of access of the code from remote machines which may well include clouds.\n\n[1] I. Zacharoudiou, J. W. S. McCullough, P. V. Coveney, “Development and performance of a HemeLB GPU code for human-scale blood flow simulation”, Computer Physics Communications, 282, 108548 (2023) DOI:10.1016/j.cpc.2022.108548\n[2] J. W. S. McCullough, R. A. Richardson, A. Patronis, R. Halver, R. Marshall, M. Ruefenacht, B. J. N. Wylie, T. Odaker, M. Wiedemann, B. Lloyd, E. Neufeld, G. Sutmann, A. Skjellum, D. Kranzlmüller and P. V. Coveney, “Towards blood flow in the virtual human: efficient self-coupling of HemeLB”, J R Soc Interface Focus 11, 20190119 (2020), DOI:10.1098/rsfs.2019.0119\n[3] A. Patronis, R. A. Richardson, S. Schmieschek, B. J. Wylie, R. W. Nash and P. V. Coveney, “Modelling Patient-Specific Magnetic Drug Targeting within the Intracranial Vasculature”, Frontiers of Physiology, 9:331 (2018), DOI: 10.3389/fphys.2018.00331\n[4] C. A. Franco, M. Jones, I. Geudens, M. O. Bernabeu, A. Ragab, A. Lima, R. T. Collins, L. K. Phng, P. V. Coveney, H. Gerhardt, “Dynamic endothelial cell rearrangements drive developmental vessel regression”, PLoS Biology, 13(4), e1002125 (2015), DOI: 10.1371/journal.pbio.1002125\n[5] P. V. Coveney and R. R. Highfield, Virtual You: How Building Your Digital Twin Will Revolutionize Medicine and Change Your Life, Princeton University Press (2023) DOI:10.1515/9780691223407"
  },
  {
    "objectID": "phd_projects/entries/akyildiz.html",
    "href": "phd_projects/entries/akyildiz.html",
    "title": "Constrained Generative Models for Optimisation and Scientific Modelling",
    "section": "",
    "text": "Generative models display impressive abilities to simulate and generate realistic looking multimedia data, such as images and audio. However, when it comes to domains where many constraints are present, such as optimisation or physical modelling, they fall short of producing high-quality samples based on the training data alone. In these cases, samples often have unrealistic artefacts and unphysical behaviour. Therefore, it is of great interest to develop principled ways to incorporate constraints into generative models, e.g. diffusion models and flows. This project will first look at developing an optimisation-based methodology where one is interested in generating samples that minimise certain cost functions that can be described by constraints of the problem at hand. We aim to produce methodology and corresponding software to incorporate general constraints. Then the project will move ahead to incorporate constraints arising from physical modelling problems, e.g., described by a partial differential equation (PDE). All of this is envisioned to be converted to a modular software package, that can be interfaced with many popular software packages to improve its usability for researchers.\n\n\nConstraining generative models is a popular topic, since it is generally recognised that they cannot produce samples obeying, e.g., physical constraints well. Below is very recent work in this direction:\nFishman, N., Klarner, L., De Bortoli, V., Mathieu, E., & Hutchinson, M. J. Diffusion Models for Constrained Domains. Transactions on Machine Learning Research, 2023.\nKong, L., Du, Y., Mu, W., Neklyudov, K., De Bortoli, V., Wang, H., Wu, D., Ferber, A., Ma, Y.A., Gomes, C.P. and Zhang, C., 2024. Diffusion models as constrained samplers for optimization with unknown constraints. arXiv preprint arXiv:2402.18012.\nFishman, N., Klarner, L., Mathieu, E., Hutchinson, M. and De Bortoli, V., 2024. Metropolis sampling for constrained diffusion models. Advances in Neural Information Processing Systems, 36.\nI have worked on diffusion models for inverse problems [1], which is a special case of constraining the generative model (instead of a loss function, one uses a likelihood – where the log-likelihood can be interpreted as a loss function). This worked really well for inverse problems, and we are already working on extensions of this idea using sequential Monte Carlo within my group for inverse problems as well as for general losses. Other relevant works include variational deep generative modelling with physics constraints, which can be seen from [2, 3, 4].\n[1] Boys, B., Girolami, M., Pidstrigach, J., Reich, S., Mosca, A. and Akyildiz, O.D., 2023. Tweedie moment projected diffusions for inverse problems. Transactions of Machine Learning Research, 2024.\n[2] Vadeboncoeur, A., Akyildiz, Ö.D., Kazlauskaite, I., Girolami, M. and Cirak, F., 2023. Fully probabilistic deep models for forward and inverse problems in parametric PDEs. Journal of Computational Physics, 491, p.112369.\n[3] Vadeboncoeur, A., Kazlauskaite, I., Papandreou, Y., Cirak, F., Girolami, M. and Akyildiz, O.D., 2023, July. Random grid neural processes for parametric partial differential equations. In International Conference on Machine Learning (ICML) (pp. 34759-34778).\n[4] Akyildiz, O.D., Girolami, M., Stuart, A.M. and Vadeboncoeur, A., 2024. Efficient Prior Calibration From Indirect Data. arXiv preprint arXiv:2405.17955.\n\n\n\nThis project aims at publishing two or three research articles with clear methodological improvements compared to the state-of-the-art. This will include first working on general optimisation problems and developing a methodology to solve optimisation problems using generative models, most notably, diffusion and flow models (but also potentially others). Once the methodology is developed, the methodology will be converted to a software package (written in JAX, as envisioned), properly documented, and aimed at general use. To improve then the usability of the software, we will incorporate physical constraints as ready-to-go functions which can be made to guide generative models.\n\n\n\nWe aim at interfacing popular diffusion and flow model code (or incorporating into ours) with general purpose optimizers (again, can be modified) and PDE solvers. My earlier project as cited above [1] produced a mini software package, called DiffusionJAX: https://github.com/bb515/diffusionjax Further developments of this will be implementing general purpose optimisation methods as well as PDE solvers, such as the ones based on finite elements (see the relevant projects I worked in [5], [6] and software output: https://github.com/connor-duffin/ula-statfem). The final aim is to combine the strengths of the JAX and available generative modelling and scientific modelling code, combined into a coherent framework to work with the problem of generating data with complicated constraints that arise in real-world modelling.\n[5] Akyildiz, Ö.D., Duffin, C., Sabanis, S. and Girolami, M., 2022. Statistical finite elements via Langevin dynamics. SIAM/ASA Journal on Uncertainty Quantification, 10(4), pp.1560-1585.\n[6] Glyn-Davies, A., Duffin, C., Kazlauskaite, I., Girolami, M. and Akyildiz, Ö.D., 2024. Statistical Finite Elements via Interacting Particle Langevin Dynamics. arXiv preprint arXiv:2409.07101."
  },
  {
    "objectID": "phd_projects/entries/akyildiz.html#project-description",
    "href": "phd_projects/entries/akyildiz.html#project-description",
    "title": "Constrained Generative Models for Optimisation and Scientific Modelling",
    "section": "",
    "text": "Generative models display impressive abilities to simulate and generate realistic looking multimedia data, such as images and audio. However, when it comes to domains where many constraints are present, such as optimisation or physical modelling, they fall short of producing high-quality samples based on the training data alone. In these cases, samples often have unrealistic artefacts and unphysical behaviour. Therefore, it is of great interest to develop principled ways to incorporate constraints into generative models, e.g. diffusion models and flows. This project will first look at developing an optimisation-based methodology where one is interested in generating samples that minimise certain cost functions that can be described by constraints of the problem at hand. We aim to produce methodology and corresponding software to incorporate general constraints. Then the project will move ahead to incorporate constraints arising from physical modelling problems, e.g., described by a partial differential equation (PDE). All of this is envisioned to be converted to a modular software package, that can be interfaced with many popular software packages to improve its usability for researchers.\n\n\nConstraining generative models is a popular topic, since it is generally recognised that they cannot produce samples obeying, e.g., physical constraints well. Below is very recent work in this direction:\nFishman, N., Klarner, L., De Bortoli, V., Mathieu, E., & Hutchinson, M. J. Diffusion Models for Constrained Domains. Transactions on Machine Learning Research, 2023.\nKong, L., Du, Y., Mu, W., Neklyudov, K., De Bortoli, V., Wang, H., Wu, D., Ferber, A., Ma, Y.A., Gomes, C.P. and Zhang, C., 2024. Diffusion models as constrained samplers for optimization with unknown constraints. arXiv preprint arXiv:2402.18012.\nFishman, N., Klarner, L., Mathieu, E., Hutchinson, M. and De Bortoli, V., 2024. Metropolis sampling for constrained diffusion models. Advances in Neural Information Processing Systems, 36.\nI have worked on diffusion models for inverse problems [1], which is a special case of constraining the generative model (instead of a loss function, one uses a likelihood – where the log-likelihood can be interpreted as a loss function). This worked really well for inverse problems, and we are already working on extensions of this idea using sequential Monte Carlo within my group for inverse problems as well as for general losses. Other relevant works include variational deep generative modelling with physics constraints, which can be seen from [2, 3, 4].\n[1] Boys, B., Girolami, M., Pidstrigach, J., Reich, S., Mosca, A. and Akyildiz, O.D., 2023. Tweedie moment projected diffusions for inverse problems. Transactions of Machine Learning Research, 2024.\n[2] Vadeboncoeur, A., Akyildiz, Ö.D., Kazlauskaite, I., Girolami, M. and Cirak, F., 2023. Fully probabilistic deep models for forward and inverse problems in parametric PDEs. Journal of Computational Physics, 491, p.112369.\n[3] Vadeboncoeur, A., Kazlauskaite, I., Papandreou, Y., Cirak, F., Girolami, M. and Akyildiz, O.D., 2023, July. Random grid neural processes for parametric partial differential equations. In International Conference on Machine Learning (ICML) (pp. 34759-34778).\n[4] Akyildiz, O.D., Girolami, M., Stuart, A.M. and Vadeboncoeur, A., 2024. Efficient Prior Calibration From Indirect Data. arXiv preprint arXiv:2405.17955.\n\n\n\nThis project aims at publishing two or three research articles with clear methodological improvements compared to the state-of-the-art. This will include first working on general optimisation problems and developing a methodology to solve optimisation problems using generative models, most notably, diffusion and flow models (but also potentially others). Once the methodology is developed, the methodology will be converted to a software package (written in JAX, as envisioned), properly documented, and aimed at general use. To improve then the usability of the software, we will incorporate physical constraints as ready-to-go functions which can be made to guide generative models.\n\n\n\nWe aim at interfacing popular diffusion and flow model code (or incorporating into ours) with general purpose optimizers (again, can be modified) and PDE solvers. My earlier project as cited above [1] produced a mini software package, called DiffusionJAX: https://github.com/bb515/diffusionjax Further developments of this will be implementing general purpose optimisation methods as well as PDE solvers, such as the ones based on finite elements (see the relevant projects I worked in [5], [6] and software output: https://github.com/connor-duffin/ula-statfem). The final aim is to combine the strengths of the JAX and available generative modelling and scientific modelling code, combined into a coherent framework to work with the problem of generating data with complicated constraints that arise in real-world modelling.\n[5] Akyildiz, Ö.D., Duffin, C., Sabanis, S. and Girolami, M., 2022. Statistical finite elements via Langevin dynamics. SIAM/ASA Journal on Uncertainty Quantification, 10(4), pp.1560-1585.\n[6] Glyn-Davies, A., Duffin, C., Kazlauskaite, I., Girolami, M. and Akyildiz, Ö.D., 2024. Statistical Finite Elements via Interacting Particle Langevin Dynamics. arXiv preprint arXiv:2409.07101."
  },
  {
    "objectID": "phd_projects/entries/salvi_backpropagation.html",
    "href": "phd_projects/entries/salvi_backpropagation.html",
    "title": "Backpropagation through rough differential equations",
    "section": "",
    "text": "In this project, we will build on the work of the two supervisors [https://arxiv.org/abs/2009.08295, https://arxiv.org/abs/2201.07566] to study backpropagation through rough differential equations (RDEs), a rough analysis generalisation of SDEs including driving noises possibly rougher than Brownian motion. We will be particularly interested in designing algebraically reversible solvers for Neural RDEs, with enjoy the advantages of both discretise-then-optimise and optimise-then-discretise methods. A key tool we will use are Butcher series expansion, allowing to express the pathwise solution of an RDE in terms of trees representing certain iterated integrals of the driving noise. This expansion allows to capture symmetries to be imposed on the resulting numerical scheme such as algebraic reversibility.\n\n\nNeural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. The reversible Heun method for SDEs https://arxiv.org/abs/2105.13493, built from the analogous scheme for ODEs https://arxiv.org/abs/2102.04668, is to the best of our knowledge, the only algebraically reversible SDE solver to have been developed.\n\n\n\nDevelop higher order algebraically reversible solvers for RDEs. Study convergence/error and stability analysis of the proposed algorithms. Calibrate the algorithm to real-world data, showcasing its application in financial market simulations.\n\n\n\nThe integration of symbolic computations needed for the Butcher series expansion in Diffrax. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/salvi_backpropagation.html#project-description",
    "href": "phd_projects/entries/salvi_backpropagation.html#project-description",
    "title": "Backpropagation through rough differential equations",
    "section": "",
    "text": "In this project, we will build on the work of the two supervisors [https://arxiv.org/abs/2009.08295, https://arxiv.org/abs/2201.07566] to study backpropagation through rough differential equations (RDEs), a rough analysis generalisation of SDEs including driving noises possibly rougher than Brownian motion. We will be particularly interested in designing algebraically reversible solvers for Neural RDEs, with enjoy the advantages of both discretise-then-optimise and optimise-then-discretise methods. A key tool we will use are Butcher series expansion, allowing to express the pathwise solution of an RDE in terms of trees representing certain iterated integrals of the driving noise. This expansion allows to capture symmetries to be imposed on the resulting numerical scheme such as algebraic reversibility.\n\n\nNeural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. The reversible Heun method for SDEs https://arxiv.org/abs/2105.13493, built from the analogous scheme for ODEs https://arxiv.org/abs/2102.04668, is to the best of our knowledge, the only algebraically reversible SDE solver to have been developed.\n\n\n\nDevelop higher order algebraically reversible solvers for RDEs. Study convergence/error and stability analysis of the proposed algorithms. Calibrate the algorithm to real-world data, showcasing its application in financial market simulations.\n\n\n\nThe integration of symbolic computations needed for the Butcher series expansion in Diffrax. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/pavliotis_kalise.html",
    "href": "phd_projects/entries/pavliotis_kalise.html",
    "title": "Optimal control methods for agent-based models",
    "section": "",
    "text": "Agent-based models (ABM), often described in terms of systems of interacting diffusions (multidimensional stochastic differential equations) arise in many applications, including mathematical biology, e.g. models for chemotaxis, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. Such systems often exhibit interesting collective behaviour as a result of the interaction between agents. Several different macroscopic configurations of the ABM are possible, for different choices of the parameters in the system, such as the interaction strength. The transition between different macroscopic configurations can have dramatic effects on the behaviour of the ABM\nGreg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed optimal control methods for high-dimensional problems which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nThe goal of the proposed project is to develop efficient control methodologies for steering the ABM dynamics towards desired macroscopic configurations. Different approaches, such as optimal control for PDEs and data driven approaches based on model predictive control will be developed.\n\n\n\nSoftware deliverables will include:\n\nAn ABM simulator for trajectory generation. To date, there’s no universal standard/benchmarks for ABM simulation, despite a generic structure based on the interaction forces in the system. We will develop a trajectory simulator which is fundamental for the study of data-driven methods and for optimisation and control purposes. Such simulator will provide sufficient freedom for prescribing interaction forces, thus being useful for different applications including opinion dynamics, swarm robotics, and pedestrian motion, among others. The simulator will be constructed using state-of-the-art methods for accurate and fast approximation of large-scale SDEs.\nAn optimal control toolbox for ABM. There are several optimal control solvers readily available for traditional control engineering applications (robotics, power electronics), however, none of them scale to high-dimensional settings which are natural in ABMs. Moreover, they do not exploit model structure and eventually resort to treating the optimal control problem as a large-scale, model-free, nonlinear optimization problem. Instead, we will develop a toolbox for control of ABMs based on adjoint calculus, making extensive use of the particular model structure of ABMs and circumventing calls to external nonlinear optimization solvers."
  },
  {
    "objectID": "phd_projects/entries/pavliotis_kalise.html#project-description",
    "href": "phd_projects/entries/pavliotis_kalise.html#project-description",
    "title": "Optimal control methods for agent-based models",
    "section": "",
    "text": "Agent-based models (ABM), often described in terms of systems of interacting diffusions (multidimensional stochastic differential equations) arise in many applications, including mathematical biology, e.g. models for chemotaxis, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. Such systems often exhibit interesting collective behaviour as a result of the interaction between agents. Several different macroscopic configurations of the ABM are possible, for different choices of the parameters in the system, such as the interaction strength. The transition between different macroscopic configurations can have dramatic effects on the behaviour of the ABM\nGreg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed optimal control methods for high-dimensional problems which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nThe goal of the proposed project is to develop efficient control methodologies for steering the ABM dynamics towards desired macroscopic configurations. Different approaches, such as optimal control for PDEs and data driven approaches based on model predictive control will be developed.\n\n\n\nSoftware deliverables will include:\n\nAn ABM simulator for trajectory generation. To date, there’s no universal standard/benchmarks for ABM simulation, despite a generic structure based on the interaction forces in the system. We will develop a trajectory simulator which is fundamental for the study of data-driven methods and for optimisation and control purposes. Such simulator will provide sufficient freedom for prescribing interaction forces, thus being useful for different applications including opinion dynamics, swarm robotics, and pedestrian motion, among others. The simulator will be constructed using state-of-the-art methods for accurate and fast approximation of large-scale SDEs.\nAn optimal control toolbox for ABM. There are several optimal control solvers readily available for traditional control engineering applications (robotics, power electronics), however, none of them scale to high-dimensional settings which are natural in ABMs. Moreover, they do not exploit model structure and eventually resort to treating the optimal control problem as a large-scale, model-free, nonlinear optimization problem. Instead, we will develop a toolbox for control of ABMs based on adjoint calculus, making extensive use of the particular model structure of ABMs and circumventing calls to external nonlinear optimization solvers."
  },
  {
    "objectID": "phd_projects/entries/Tobar_diffusion_models.html",
    "href": "phd_projects/entries/Tobar_diffusion_models.html",
    "title": "Aligned diffusion models for machine learning",
    "section": "",
    "text": "Diffusion models (DM) are the state of the art on generative modelling, their ability to learn (implicit) probabilistic models over complex structured datasets is unparalleled and it has been validated on images and audio in a number of applications. The generality and wide applicability of DMs motivates a variety of research directions including fairness, reinforcement-learning-based enhancement, prevention of mode collapse, AI alignment, and accelerated computation. Alignment, in particular, is a much-desired feature in DMs as they are currently being deployed for use by the general public, where they might deal with sensitive data and critical decision making.\nWe have developed fine-tuning techniques for DM with the principal aim of aligning the samples generated by the DM to human criteria. Our contributions have been tested on novel DM samplers that follow objectives that are difficult to describe by a standard discriminant function. This includes producing samples that are aesthetic, incompressible, or that do not include violent content (e.g., explicit images).\n\n\n\nTo design and validate strategies to align diffusion models with human criteria. The project comprises both theoretical and computational aspects.\n\nTo explore the state of the art in DMs and the techniques currently used for guidance and finetuning.\nTo understand current alignment techniques using, e.g., guidance and reinforcement learning\nTo identify which tools in the ML literature and related resources from computational mathematics, optimisation, statistics, and probability, can be used to propose control (alignment) loops for sampling in DMs\nTo design an experiment, and an experimental setup, to validate the proposed alignment techniques in applications involving fairness, social sciences, health, or general generative modelling\nTo analyse, both from theoretical and practical perspectives, the developed alignment techniques so as to provide convincing evidence of alignment in DMs\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML community\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing applications to scientific or social challenges. Describe coding and data developments during the project. See above."
  },
  {
    "objectID": "phd_projects/entries/Tobar_diffusion_models.html#project-description",
    "href": "phd_projects/entries/Tobar_diffusion_models.html#project-description",
    "title": "Aligned diffusion models for machine learning",
    "section": "",
    "text": "Diffusion models (DM) are the state of the art on generative modelling, their ability to learn (implicit) probabilistic models over complex structured datasets is unparalleled and it has been validated on images and audio in a number of applications. The generality and wide applicability of DMs motivates a variety of research directions including fairness, reinforcement-learning-based enhancement, prevention of mode collapse, AI alignment, and accelerated computation. Alignment, in particular, is a much-desired feature in DMs as they are currently being deployed for use by the general public, where they might deal with sensitive data and critical decision making.\nWe have developed fine-tuning techniques for DM with the principal aim of aligning the samples generated by the DM to human criteria. Our contributions have been tested on novel DM samplers that follow objectives that are difficult to describe by a standard discriminant function. This includes producing samples that are aesthetic, incompressible, or that do not include violent content (e.g., explicit images).\n\n\n\nTo design and validate strategies to align diffusion models with human criteria. The project comprises both theoretical and computational aspects.\n\nTo explore the state of the art in DMs and the techniques currently used for guidance and finetuning.\nTo understand current alignment techniques using, e.g., guidance and reinforcement learning\nTo identify which tools in the ML literature and related resources from computational mathematics, optimisation, statistics, and probability, can be used to propose control (alignment) loops for sampling in DMs\nTo design an experiment, and an experimental setup, to validate the proposed alignment techniques in applications involving fairness, social sciences, health, or general generative modelling\nTo analyse, both from theoretical and practical perspectives, the developed alignment techniques so as to provide convincing evidence of alignment in DMs\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML community\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing applications to scientific or social challenges. Describe coding and data developments during the project. See above."
  },
  {
    "objectID": "phd_projects/entries/kalise_pavliotis.html",
    "href": "phd_projects/entries/kalise_pavliotis.html",
    "title": "Taming Time and Dimension: Advanced Scientific Computing for Next-Generation Diffusion Models",
    "section": "",
    "text": "Diffusion models have emerged as a powerful class of generative models, with applications in image, audio, and 3D synthesis. This PhD project aims to advance the computational efficiency of diffusion models by addressing two primary challenges: expensive time integration during sampling and the curse of dimensionality in data representation. We propose to develop novel scientific computing methods combining advanced time integration schemes with high-dimensional approximation techniques. This interdisciplinary approach will bridge state-of-the-art generative AI with classical numerical analysis and modern tensor approximation methods, potentially unlocking major efficiency gains while maintaining or improving generation quality.\n\n\n\nRecent advancements in diffusion model sampling have focused on either improving time integration or tackling high-dimensional representations. On the time integration front, methods like DPM-Solver and DEIS have leveraged exponential integrators and sophisticated ODE solvers. For high-dimensional approximation, techniques such as tensor decompositions and sparse grids have shown promise in related fields. However, there remains a significant gap in approaches that effectively combine these two aspects for diffusion models. This project builds upon these foundations while aiming to create a unified framework that addresses both challenges simultaneously.\nRelevant references: Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, Saining Xie arXiv preprint arXiv:2401.08740\nStable generative modeling using Schroedinger bridges Georg Gottwald, Fengyi Li, Youssef Marzouk, Sebastian Reich arXiv preprint arXiv:2401.04372\nGenerative Modelling with Tensor Train approximations of Hamilton–Jacobi–Bellman equations David Sommer, Robert Gruhlke, Max Kirstein, Martin Eigel, Claudia Schillings arXiv preprint arXiv:2402.15285\nImproved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling Qinsheng Zhang, Jiaming Song, Yongxin Chen arXiv preprint arXiv:2308.02157\n\n\n\nThe primary objectives of this project are to: (1) Develop novel time integration schemes tailored for diffusion model ODEs/SDEs, incorporating adaptive stepping and error control. (2) Investigate high-dimensional approximation techniques, including tensor decompositions and sparse grids, for efficient representation of diffusion model states and operators. (3) Design algorithms that combine advanced time integration with high-dimensional approximation, including multi-level and multi-fidelity approaches. (4) Conduct rigorous theoretical analysis of the proposed methods, deriving stability conditions, convergence rates, and error bounds.\n\n\n\nThe project will produce a suite of scientific computing algorithms specifically designed for diffusion models. These will include novel time integration schemes, high-dimensional approximation techniques, and hybrid methods that combine both approaches. The algorithms will be built with a focus on both scalability to large-scale diffusion models and modularity, enabling easy integration with existing diffusion model implementations."
  },
  {
    "objectID": "phd_projects/entries/kalise_pavliotis.html#project-description",
    "href": "phd_projects/entries/kalise_pavliotis.html#project-description",
    "title": "Taming Time and Dimension: Advanced Scientific Computing for Next-Generation Diffusion Models",
    "section": "",
    "text": "Diffusion models have emerged as a powerful class of generative models, with applications in image, audio, and 3D synthesis. This PhD project aims to advance the computational efficiency of diffusion models by addressing two primary challenges: expensive time integration during sampling and the curse of dimensionality in data representation. We propose to develop novel scientific computing methods combining advanced time integration schemes with high-dimensional approximation techniques. This interdisciplinary approach will bridge state-of-the-art generative AI with classical numerical analysis and modern tensor approximation methods, potentially unlocking major efficiency gains while maintaining or improving generation quality.\n\n\n\nRecent advancements in diffusion model sampling have focused on either improving time integration or tackling high-dimensional representations. On the time integration front, methods like DPM-Solver and DEIS have leveraged exponential integrators and sophisticated ODE solvers. For high-dimensional approximation, techniques such as tensor decompositions and sparse grids have shown promise in related fields. However, there remains a significant gap in approaches that effectively combine these two aspects for diffusion models. This project builds upon these foundations while aiming to create a unified framework that addresses both challenges simultaneously.\nRelevant references: Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, Saining Xie arXiv preprint arXiv:2401.08740\nStable generative modeling using Schroedinger bridges Georg Gottwald, Fengyi Li, Youssef Marzouk, Sebastian Reich arXiv preprint arXiv:2401.04372\nGenerative Modelling with Tensor Train approximations of Hamilton–Jacobi–Bellman equations David Sommer, Robert Gruhlke, Max Kirstein, Martin Eigel, Claudia Schillings arXiv preprint arXiv:2402.15285\nImproved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling Qinsheng Zhang, Jiaming Song, Yongxin Chen arXiv preprint arXiv:2308.02157\n\n\n\nThe primary objectives of this project are to: (1) Develop novel time integration schemes tailored for diffusion model ODEs/SDEs, incorporating adaptive stepping and error control. (2) Investigate high-dimensional approximation techniques, including tensor decompositions and sparse grids, for efficient representation of diffusion model states and operators. (3) Design algorithms that combine advanced time integration with high-dimensional approximation, including multi-level and multi-fidelity approaches. (4) Conduct rigorous theoretical analysis of the proposed methods, deriving stability conditions, convergence rates, and error bounds.\n\n\n\nThe project will produce a suite of scientific computing algorithms specifically designed for diffusion models. These will include novel time integration schemes, high-dimensional approximation techniques, and hybrid methods that combine both approaches. The algorithms will be built with a focus on both scalability to large-scale diffusion models and modularity, enabling easy integration with existing diffusion model implementations."
  },
  {
    "objectID": "phd_projects/entries/guillas.html",
    "href": "phd_projects/entries/guillas.html",
    "title": "The propagation of uncertainties across coupled climate models",
    "section": "",
    "text": "The propagation of uncertainties across coupled models is a grand challenge, not yet with a solution for climate (ocean, sea-ice, atmosphere, land, etc.). The resources and runtimes required by simulations at different scales differ vastly. Linking models through emulators is now ripe (Ming, D. and Guillas, S. (2021) Linked Gaussian process emulation for systems of computer models using Matérn kernels and adaptive design, SIAM/ASA Journal on Uncertainty Quantification. 9(4), 1615-1642.) and representations such as Deep Gaussian Processes (Ming, D., Williamson, D., and Guillas, S. (2023) Deep Gaussian process emulation using stochastic imputation. Technometrics. 65(2), 150-161) are available, but the software, and the practical and methodological challenges require investigations. There is great interest at the Met Office, and at UKAEA, since coupling of multiple models is a necessary step in these complex models. The community has not yet addressed this issue head-on, so this is a timely project to deliver some initial groundbreaking steps. The principal supervisor and collaborators have set up software platforms, within a prior project at the Alan Turing Institute, with the Multi-Output Gaussian Process Emulator (MOGP), and within the EPSRC project SEAVEA (2021-2025), a set-up that facilitates the design and organisation of runs. For applications to climate modelling the group of Prof Guillas has been working on the topic, but with only one submodel, with a paper now under submission: D. Giles, J. Briant, C. J. Morcrette, S. Guillas, “Sensitivity of simulated tropical precipitation in a coarse climate model to the inclusion of subgrid variability machine learnt from high resolution weather simulations”.\n\n\n\nThe objectives are to create surrogates of sub-models and link them within a network of models, merging some simulators, some emulators. The network can be viewed as a Deep Gaussian Process with partial exposure of the hidden layers (via individual runs of the simulators). Challenges include the dimensions to reduce to forward the right level of information across the network, the construction of uncertainties within such a hybrid set-up, and the design of such computer experiments. Campaigns of runs will be co-designed with the Met Office, which will provide computing access to novel facilities. ### Details of Software/Data Deliverables\n\n\n\nThe codes are now available to start with linked emulators and Deep Gaussian Processes (R and python: https://github.com/mingdeyu/DGP), and the dimension reduction and emulation of multi-outputs (MOGP in python: https://github.com/alan-turing-institute/mogp-emulator). Developments include managing and optimising the creation of the overall network of models, including uncertainties, with an emphasis on memory requirements due to large dimensions and the trade-off of computation v. accuracy, ultimately at the exascale."
  },
  {
    "objectID": "phd_projects/entries/guillas.html#project-description",
    "href": "phd_projects/entries/guillas.html#project-description",
    "title": "The propagation of uncertainties across coupled climate models",
    "section": "",
    "text": "The propagation of uncertainties across coupled models is a grand challenge, not yet with a solution for climate (ocean, sea-ice, atmosphere, land, etc.). The resources and runtimes required by simulations at different scales differ vastly. Linking models through emulators is now ripe (Ming, D. and Guillas, S. (2021) Linked Gaussian process emulation for systems of computer models using Matérn kernels and adaptive design, SIAM/ASA Journal on Uncertainty Quantification. 9(4), 1615-1642.) and representations such as Deep Gaussian Processes (Ming, D., Williamson, D., and Guillas, S. (2023) Deep Gaussian process emulation using stochastic imputation. Technometrics. 65(2), 150-161) are available, but the software, and the practical and methodological challenges require investigations. There is great interest at the Met Office, and at UKAEA, since coupling of multiple models is a necessary step in these complex models. The community has not yet addressed this issue head-on, so this is a timely project to deliver some initial groundbreaking steps. The principal supervisor and collaborators have set up software platforms, within a prior project at the Alan Turing Institute, with the Multi-Output Gaussian Process Emulator (MOGP), and within the EPSRC project SEAVEA (2021-2025), a set-up that facilitates the design and organisation of runs. For applications to climate modelling the group of Prof Guillas has been working on the topic, but with only one submodel, with a paper now under submission: D. Giles, J. Briant, C. J. Morcrette, S. Guillas, “Sensitivity of simulated tropical precipitation in a coarse climate model to the inclusion of subgrid variability machine learnt from high resolution weather simulations”.\n\n\n\nThe objectives are to create surrogates of sub-models and link them within a network of models, merging some simulators, some emulators. The network can be viewed as a Deep Gaussian Process with partial exposure of the hidden layers (via individual runs of the simulators). Challenges include the dimensions to reduce to forward the right level of information across the network, the construction of uncertainties within such a hybrid set-up, and the design of such computer experiments. Campaigns of runs will be co-designed with the Met Office, which will provide computing access to novel facilities. ### Details of Software/Data Deliverables\n\n\n\nThe codes are now available to start with linked emulators and Deep Gaussian Processes (R and python: https://github.com/mingdeyu/DGP), and the dimension reduction and emulation of multi-outputs (MOGP in python: https://github.com/alan-turing-institute/mogp-emulator). Developments include managing and optimising the creation of the overall network of models, including uncertainties, with an emphasis on memory requirements due to large dimensions and the trade-off of computation v. accuracy, ultimately at the exascale."
  },
  {
    "objectID": "phd_projects/entries/Deisenroth_dataassimilation.html",
    "href": "phd_projects/entries/Deisenroth_dataassimilation.html",
    "title": "Machine Learning for Low-Cost Data Assimilation",
    "section": "",
    "text": "This project is about a machine learning approach to data assimilation. Specifically, we use message passing algorithms to infer a posterior distribution on the (weather) state given some observations. This would be a perspective on data assimilation that is different to currently used methods, such as 3DVar or inference in Gauss-Markov random fields using INLA. We have done some preliminary work in this space with encouraging results that are similar to 3DVar in terms of speed and accuracy. Currently, our results are limited to the spatial setting, and we only estimate the mean of the latent field.\n\n\n\nThe project will extend our previous work to include meaningful (marginal) uncertainty estimates plus the extension to the spatio-temporal setting where 4DVar and Ensemble Kalman Filters are the state of the art. In combination with a machine-learning model that faithfully emulates the numerical weather prediction model, we will be able to quickly arrive at a data assimilation solution that is a) parallelizable, b) distributed in computation, c) yields meaningful uncertainty estimates, d) has a small memory footprint, e) can be implemented on specialized hardware, such as Graphcores IPUs. Our approach is general in the sense that it can be applied to various areas, such as weather/climate, oceans or nuclear fusion.\n\n\n\nOur implementation is currently in JAX (including sparse linear algebra) and works on GPU and CPU. We will continue to develop our software to support multi-GPU systems. In terms of data, we currently use publicly available data. By the end of the project, software will be open-sourced and easy to use."
  },
  {
    "objectID": "phd_projects/entries/Deisenroth_dataassimilation.html#project-description",
    "href": "phd_projects/entries/Deisenroth_dataassimilation.html#project-description",
    "title": "Machine Learning for Low-Cost Data Assimilation",
    "section": "",
    "text": "This project is about a machine learning approach to data assimilation. Specifically, we use message passing algorithms to infer a posterior distribution on the (weather) state given some observations. This would be a perspective on data assimilation that is different to currently used methods, such as 3DVar or inference in Gauss-Markov random fields using INLA. We have done some preliminary work in this space with encouraging results that are similar to 3DVar in terms of speed and accuracy. Currently, our results are limited to the spatial setting, and we only estimate the mean of the latent field.\n\n\n\nThe project will extend our previous work to include meaningful (marginal) uncertainty estimates plus the extension to the spatio-temporal setting where 4DVar and Ensemble Kalman Filters are the state of the art. In combination with a machine-learning model that faithfully emulates the numerical weather prediction model, we will be able to quickly arrive at a data assimilation solution that is a) parallelizable, b) distributed in computation, c) yields meaningful uncertainty estimates, d) has a small memory footprint, e) can be implemented on specialized hardware, such as Graphcores IPUs. Our approach is general in the sense that it can be applied to various areas, such as weather/climate, oceans or nuclear fusion.\n\n\n\nOur implementation is currently in JAX (including sparse linear algebra) and works on GPU and CPU. We will continue to develop our software to support multi-GPU systems. In terms of data, we currently use publicly available data. By the end of the project, software will be open-sourced and easy to use."
  },
  {
    "objectID": "phd_projects/entries/Betcke_GPUarchitectures.html",
    "href": "phd_projects/entries/Betcke_GPUarchitectures.html",
    "title": "Fast Multipole Methods on modern architectures",
    "section": "",
    "text": "### Existing background work\nFast Multipole Methods (FMM) are one of the fundamental algorithms of computational sciences. They allow the fast approximate evaluation of interactions of N particles with each other in linear complexity instead of quadratic complexity when naive direct evaluation methods are being used. The FMM goes back to Rokhlin and Greengard in 1987 and has since undergone substantial algorithmic and computational advances. FMM on GPUs for example won the Gordon Bell Price in 2009. However, while advances continued over the last ten years little work has been done to exploit Fast Multipole Methods beyond classical GPU computing. In particular, modern designs such as unified memory architectures on Apple Silicone and Nvidia Grace Hopper, mixed precision computations, or the use of tensor units in modern accelerators have received little attention.\nWe have started building up in our group our own FMM expertise as part of the Bempp project. Using Rust as main driver language we have developed a CPU based FMM that is highly portable and competitive with other established Fast Multipole libraries. We are currently porting this effort over to MPI based clusters. Most of this work has been part of the PhD thesis of an existing student in Betcke’s group.\n\n\nBased on our existing experience with our CPU based FMM implementation we want to expand to modern compute architectures. In particular, we have the following objects:\n\nDevelop optimised FMM implementations for unified memory architectures. Within the FMM the key drivers of computational cost are the Particle To Particle Interactions (P2P) and the Multipole 2 Locale Interactions (M2L). For a shallow computation tree P2P will dominate. For a deeper tree M2L will dominate. P2P especially benefits from computation on GPU cores. M2L operations can be accelerated on GPUs but often have lower compute intensity than P2P. If CPU and GPU share a unified address space and fast overall memory accesses many of the implementational problems of full GPU based FMM fall away and we can flexibly decide between work on CPU and on GPU cores. We want to exploit this to optimise new FMM implementations that make full use of unified memory architectures.\nExploit mixed precision arithmetic computations. For many applications it is enough for an FMM to deliver 6 to 7 digits of accuracy, which is just about in the limit of FP32 computations. However, naive implementation on FP32 often leads to fewer correct digits. We want to investigate which operations to run on FP64, and which to accelerate via FP32 while still being able to maintain sufficiently high accuracy for the FMM.\nOptimise for matrix-multiplication operations.. Many modern FMM formulation can be written in terms of matrix-matrix products, which can make use of tensor cores and other specialised registers for AI computations. We want to investigate how these can be used as part of the FMM workflow and allow speed-up of the overall computation.\n\n\n\n\nThe student will take time to familiarise themselves with our existing Rust code base, interface C++ to access accelerator devices and develop FMM on unified architectures. We therefore think the first part of FMM on unified architectures will take roughly two years of project time. In parallel the student will slowly get started on mixed precision experiments, and we expect this part of the project to be around one year in length, once the student has already assembled more experience on developing FMM. The final part on tensor and other AI accelerators will then build on the codebase and research experience with mixed precision arithmetic that the student has built up and will mainly take part in the last 1-1.5 years of the PhD.\n\n\n\nWe are currently preparing for release our first Rust based FMM version. The student will build on this code and integrate compute kernels for accelerators within this code base. Rust itself has limited support for GPU compute. So much of this work will be in C++ and interfaced to the Rust driver codes for the FMM. A particular interest is also to use Apple Silicone as unified memory environment for FMM. While not being used directly in HPC, Apple Silicon is widely spread and a cost effective way to develop unified memory codes. For HPC we will target Nvidia’s Grace Hopper, meaning the student will need to write separate low-level kernel implementations for Apple Metal and Nvidia Cuda. For our current FMM codes we use a BSD 3-Clause license and will continue this license for this project."
  },
  {
    "objectID": "phd_projects/entries/Betcke_GPUarchitectures.html#project-description",
    "href": "phd_projects/entries/Betcke_GPUarchitectures.html#project-description",
    "title": "Fast Multipole Methods on modern architectures",
    "section": "",
    "text": "### Existing background work\nFast Multipole Methods (FMM) are one of the fundamental algorithms of computational sciences. They allow the fast approximate evaluation of interactions of N particles with each other in linear complexity instead of quadratic complexity when naive direct evaluation methods are being used. The FMM goes back to Rokhlin and Greengard in 1987 and has since undergone substantial algorithmic and computational advances. FMM on GPUs for example won the Gordon Bell Price in 2009. However, while advances continued over the last ten years little work has been done to exploit Fast Multipole Methods beyond classical GPU computing. In particular, modern designs such as unified memory architectures on Apple Silicone and Nvidia Grace Hopper, mixed precision computations, or the use of tensor units in modern accelerators have received little attention.\nWe have started building up in our group our own FMM expertise as part of the Bempp project. Using Rust as main driver language we have developed a CPU based FMM that is highly portable and competitive with other established Fast Multipole libraries. We are currently porting this effort over to MPI based clusters. Most of this work has been part of the PhD thesis of an existing student in Betcke’s group.\n\n\nBased on our existing experience with our CPU based FMM implementation we want to expand to modern compute architectures. In particular, we have the following objects:\n\nDevelop optimised FMM implementations for unified memory architectures. Within the FMM the key drivers of computational cost are the Particle To Particle Interactions (P2P) and the Multipole 2 Locale Interactions (M2L). For a shallow computation tree P2P will dominate. For a deeper tree M2L will dominate. P2P especially benefits from computation on GPU cores. M2L operations can be accelerated on GPUs but often have lower compute intensity than P2P. If CPU and GPU share a unified address space and fast overall memory accesses many of the implementational problems of full GPU based FMM fall away and we can flexibly decide between work on CPU and on GPU cores. We want to exploit this to optimise new FMM implementations that make full use of unified memory architectures.\nExploit mixed precision arithmetic computations. For many applications it is enough for an FMM to deliver 6 to 7 digits of accuracy, which is just about in the limit of FP32 computations. However, naive implementation on FP32 often leads to fewer correct digits. We want to investigate which operations to run on FP64, and which to accelerate via FP32 while still being able to maintain sufficiently high accuracy for the FMM.\nOptimise for matrix-multiplication operations.. Many modern FMM formulation can be written in terms of matrix-matrix products, which can make use of tensor cores and other specialised registers for AI computations. We want to investigate how these can be used as part of the FMM workflow and allow speed-up of the overall computation.\n\n\n\n\nThe student will take time to familiarise themselves with our existing Rust code base, interface C++ to access accelerator devices and develop FMM on unified architectures. We therefore think the first part of FMM on unified architectures will take roughly two years of project time. In parallel the student will slowly get started on mixed precision experiments, and we expect this part of the project to be around one year in length, once the student has already assembled more experience on developing FMM. The final part on tensor and other AI accelerators will then build on the codebase and research experience with mixed precision arithmetic that the student has built up and will mainly take part in the last 1-1.5 years of the PhD.\n\n\n\nWe are currently preparing for release our first Rust based FMM version. The student will build on this code and integrate compute kernels for accelerators within this code base. Rust itself has limited support for GPU compute. So much of this work will be in C++ and interfaced to the Rust driver codes for the FMM. A particular interest is also to use Apple Silicone as unified memory environment for FMM. While not being used directly in HPC, Apple Silicon is widely spread and a cost effective way to develop unified memory codes. For HPC we will target Nvidia’s Grace Hopper, meaning the student will need to write separate low-level kernel implementations for Apple Metal and Nvidia Cuda. For our current FMM codes we use a BSD 3-Clause license and will continue this license for this project."
  },
  {
    "objectID": "phd_projects/entries/hethernington2.html",
    "href": "phd_projects/entries/hethernington2.html",
    "title": "AI code generation and numerical codes",
    "section": "",
    "text": "A great deal of work is being done on automated code generation through LLMs, for example, through Github’s work on Copilot. As yet, though, little has been done to consider the implications of AI-assisted code generation for highly numerical code, for example, the parallel simulation codes used to develop digital twins of complex physical phenomena. We propose to study the behaviour of LLM-assisted coding for mathematically sophisticated floating-point software. This may lead to development of new domain specific languages closer to the natural language that would be used by mathematicians to describe a programme’s key characteristics, for example “A 2-d implementation of the wave equation on an adaptive mesh, parallelised with a Halo-Swap."
  },
  {
    "objectID": "phd_projects/entries/hethernington2.html#project-description",
    "href": "phd_projects/entries/hethernington2.html#project-description",
    "title": "AI code generation and numerical codes",
    "section": "",
    "text": "A great deal of work is being done on automated code generation through LLMs, for example, through Github’s work on Copilot. As yet, though, little has been done to consider the implications of AI-assisted code generation for highly numerical code, for example, the parallel simulation codes used to develop digital twins of complex physical phenomena. We propose to study the behaviour of LLM-assisted coding for mathematically sophisticated floating-point software. This may lead to development of new domain specific languages closer to the natural language that would be used by mathematicians to describe a programme’s key characteristics, for example “A 2-d implementation of the wave equation on an adaptive mesh, parallelised with a Halo-Swap."
  },
  {
    "objectID": "phd_projects/entries/Salvi_Gillespie.html",
    "href": "phd_projects/entries/Salvi_Gillespie.html",
    "title": "A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems",
    "section": "",
    "text": "The Gillespie algorithm is arguably one of the most well-known models for simulating chemical reactions, essentially acting as an Euler-Maruyama scheme applied to a jump process with a discrete state space. At each step, a random time is drawn from an exponential distribution (reflecting Poisson arrival events), and the discrete state is updated based on state-dependent transition probabilities. This project will focus on developing a rigorous mathematical theory and corresponding algorithm in JAX to calibrate parameterised and path-wise solutions of the Gillespie algorithm to data using automatic differentiation. By leveraging tools from rough path theory and stochastic automatic differentiation, we aim to compute exact gradients, which would improve on the approximate gradient methods currently in use. The work could have far-reaching implications for the fields of chemical kinetics, synthetic biology, and computational biology.\n\n\nThe principal supervisor’s group has extensive experience in both rough path theory and deep learning, particularly in deriving exact gradients for stochastic systems. For example, the work on “Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals” has laid the foundation for differentiating through stochastic and event-driven systems with discontinuities. Within the wider research community, a recent relevant contribution is the paper “A Differentiable Gillespie Algorithm for Simulating Chemical Kinetics, Parameter Estimation, and Designing Synthetic Biological Circuits.” However, this approach only provides approximate solutions and does not compute path-wise gradients, making it a target for improvement. Other works on stochastic automatic differentiation, such as “Automatic Differentiation of Programs with Discrete Randomness,” will also inform this project by providing a framework to handle the challenge of differentiating through inherently discrete random variables.\n\n\n\n\nDevelop higher order auto-differentiable solvers to efficiently compute path-wise solutions of the Gillespie algorithm, with a focus on computing exact path-wise gradients.\nStudy convergence/error and stability analysis of the proposed algorithms.\nCalibrate the algorithm to real-world data, showcasing its application in chemical kinetics and synthetic biology.\n\n\n\n\nThe project will deliver a JAX-based implementation of a differentiable Gillespie algorithm that computes exact gradients through discrete stochastic systems. This will involve: 1. The development of custom gradient routines that handle discrete random variables and jump processes. 2. A path-wise algorithm capable of calibrating chemical reaction networks to empirical data using automatic differentiation. 3. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. 4. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/Salvi_Gillespie.html#project-description",
    "href": "phd_projects/entries/Salvi_Gillespie.html#project-description",
    "title": "A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems",
    "section": "",
    "text": "The Gillespie algorithm is arguably one of the most well-known models for simulating chemical reactions, essentially acting as an Euler-Maruyama scheme applied to a jump process with a discrete state space. At each step, a random time is drawn from an exponential distribution (reflecting Poisson arrival events), and the discrete state is updated based on state-dependent transition probabilities. This project will focus on developing a rigorous mathematical theory and corresponding algorithm in JAX to calibrate parameterised and path-wise solutions of the Gillespie algorithm to data using automatic differentiation. By leveraging tools from rough path theory and stochastic automatic differentiation, we aim to compute exact gradients, which would improve on the approximate gradient methods currently in use. The work could have far-reaching implications for the fields of chemical kinetics, synthetic biology, and computational biology.\n\n\nThe principal supervisor’s group has extensive experience in both rough path theory and deep learning, particularly in deriving exact gradients for stochastic systems. For example, the work on “Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals” has laid the foundation for differentiating through stochastic and event-driven systems with discontinuities. Within the wider research community, a recent relevant contribution is the paper “A Differentiable Gillespie Algorithm for Simulating Chemical Kinetics, Parameter Estimation, and Designing Synthetic Biological Circuits.” However, this approach only provides approximate solutions and does not compute path-wise gradients, making it a target for improvement. Other works on stochastic automatic differentiation, such as “Automatic Differentiation of Programs with Discrete Randomness,” will also inform this project by providing a framework to handle the challenge of differentiating through inherently discrete random variables.\n\n\n\n\nDevelop higher order auto-differentiable solvers to efficiently compute path-wise solutions of the Gillespie algorithm, with a focus on computing exact path-wise gradients.\nStudy convergence/error and stability analysis of the proposed algorithms.\nCalibrate the algorithm to real-world data, showcasing its application in chemical kinetics and synthetic biology.\n\n\n\n\nThe project will deliver a JAX-based implementation of a differentiable Gillespie algorithm that computes exact gradients through discrete stochastic systems. This will involve: 1. The development of custom gradient routines that handle discrete random variables and jump processes. 2. A path-wise algorithm capable of calibrating chemical reaction networks to empirical data using automatic differentiation. 3. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. 4. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/mcewen_probabilistic.html",
    "href": "phd_projects/entries/mcewen_probabilistic.html",
    "title": "Differentiable probabilistic deep learning with generative denoising diffusion models",
    "section": "",
    "text": "Generative AI models for images, such as denoising diffusion models (e.g. Stable Diffusion), have recently demonstrated remarkable performance (Romback et al. 2022; https://arxiv.org/abs/2112.10752). Such generative models can be adapted to solve scientific inverse problems, such as recovering maps of the dark matter of the Universe. However, current approaches typically recover a single prediction, e.g. recover a single image. For robust scientific studies, however, single estimates are not sufficient and a principled statistical assessment is critical in order to quantify uncertainties. Embedding denoising diffusion models in a principled statistical framework for solving inverse problems remains a topical open problem in the field. A number of approximate solutions have been proposed (e.g. Chung et al 2023; https://arxiv.org/abs/2209.14687).\nMcEwen and collaborators have recently developed the proximal nested sampling framework (Cai et al. 2022; https://arxiv.org/abs/2106.03646) for principled statistical inference for high-dimensional inverse imaging problems with convex likelihoods (initial code available at https://github.com/astro-informatics/proxnest). Not only is the correct underlying posterior distribution targeted but the framework also supports computation of the marginal likelihood for principled Bayesian model comparison. Recently, the framework has been extended to support deep learned data-driven priors based on simple denoisers (McEwen et al. 2023; https://arxiv.org/abs/2307.00056), although not denoising diffusion models.\n\n\n\nIn this project we will develop a principled statistical framework to sample the posterior distribution of scientific inverse imaging problems that integrates the generative power of denoising diffusion models. This will be achieved by integrating denoising diffusion models into the proximal nested sampling framework. The resulting framework is expected to result in superior reconstruction performance due to the power of generative diffusion models, targets the correct underlying posterior distribution and also allows for Bayesian model comparison to assess different data-driven priors. The framework will be extended beyond convex likelihoods to handle general non-linear models by leveraging automatic differentiation and gradient-based likelihood constraints. Automatic differentiation will also be exploited to accelerate inference. While the focus will be mostly on theoretical methodological and code developments, the methods developed will be demonstrated on a number of inverse imaging problems in a range of fields.\n\n\n\nThe main deliverable with be an open-source code implementing the framework developed. Development will involve differentiable programming, generative denoising diffusion models, and Markov chain Monte Carlo (MCMC) techniques. A number of articles will be prepared as the research progresses, targeting the main deep learning venues (e.g. ICLR, ICML, NeurIPS)."
  },
  {
    "objectID": "phd_projects/entries/mcewen_probabilistic.html#project-description",
    "href": "phd_projects/entries/mcewen_probabilistic.html#project-description",
    "title": "Differentiable probabilistic deep learning with generative denoising diffusion models",
    "section": "",
    "text": "Generative AI models for images, such as denoising diffusion models (e.g. Stable Diffusion), have recently demonstrated remarkable performance (Romback et al. 2022; https://arxiv.org/abs/2112.10752). Such generative models can be adapted to solve scientific inverse problems, such as recovering maps of the dark matter of the Universe. However, current approaches typically recover a single prediction, e.g. recover a single image. For robust scientific studies, however, single estimates are not sufficient and a principled statistical assessment is critical in order to quantify uncertainties. Embedding denoising diffusion models in a principled statistical framework for solving inverse problems remains a topical open problem in the field. A number of approximate solutions have been proposed (e.g. Chung et al 2023; https://arxiv.org/abs/2209.14687).\nMcEwen and collaborators have recently developed the proximal nested sampling framework (Cai et al. 2022; https://arxiv.org/abs/2106.03646) for principled statistical inference for high-dimensional inverse imaging problems with convex likelihoods (initial code available at https://github.com/astro-informatics/proxnest). Not only is the correct underlying posterior distribution targeted but the framework also supports computation of the marginal likelihood for principled Bayesian model comparison. Recently, the framework has been extended to support deep learned data-driven priors based on simple denoisers (McEwen et al. 2023; https://arxiv.org/abs/2307.00056), although not denoising diffusion models.\n\n\n\nIn this project we will develop a principled statistical framework to sample the posterior distribution of scientific inverse imaging problems that integrates the generative power of denoising diffusion models. This will be achieved by integrating denoising diffusion models into the proximal nested sampling framework. The resulting framework is expected to result in superior reconstruction performance due to the power of generative diffusion models, targets the correct underlying posterior distribution and also allows for Bayesian model comparison to assess different data-driven priors. The framework will be extended beyond convex likelihoods to handle general non-linear models by leveraging automatic differentiation and gradient-based likelihood constraints. Automatic differentiation will also be exploited to accelerate inference. While the focus will be mostly on theoretical methodological and code developments, the methods developed will be demonstrated on a number of inverse imaging problems in a range of fields.\n\n\n\nThe main deliverable with be an open-source code implementing the framework developed. Development will involve differentiable programming, generative denoising diffusion models, and Markov chain Monte Carlo (MCMC) techniques. A number of articles will be prepared as the research progresses, targeting the main deep learning venues (e.g. ICLR, ICML, NeurIPS)."
  },
  {
    "objectID": "phd_projects/entries/CotterKalise_Fieldgames.html",
    "href": "phd_projects/entries/CotterKalise_Fieldgames.html",
    "title": "A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games",
    "section": "",
    "text": "Colin Cotter is Professor of Computational Mathematics at Imperial College. He has relevant interests in optimal transport applied to the semigeostrophic equations and to weather forecast verification, and in design, analysis and implementation of finite element methods.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed variational methods for mean field games and control which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nStarting from the fluid dynamics formulation of the Monge-Kantorovich mass transfer problem proposed by Benamou and Brenier, it is now well-understood that a wide class of problems including Optimal Transport (OT), Schroedinger Bridges (SB), Mean Field Control (MFC), and Variational Mean Field Games (MFG), can be seen as the solution of a PDE-constrained optimization problem where a convex cost if constrained to a continuity equation. While the numerical approximation of these problems has been extensively studied over the last decade due to their importance in statistical machine learning, computational methods are often developed for a particular problem of interest and fail to identified the underlying unifying structure.\nIn this project we will develop a unified solver for OT, SB, MFC and MFG, so that each instance arises after a suitable assignment of costs and constraints. The solver will be based on convex optimization methods (primal-dual algorithms), preconditioning, and structure-preserving discretizations of the continuity equation.\n\n\n\nSoftware deliverables will include: - A unified OT/SB/MFC/MFG solver which is not currently available. This will be built around Firedrake, building upon the work of Natale and Todeschi.\nNatale, Andrea, and Gabriele Todeschi. “A mixed finite element discretization of dynamical optimal transport.” Journal of Scientific Computing 91, no. 2 (2022): 38."
  },
  {
    "objectID": "phd_projects/entries/CotterKalise_Fieldgames.html#project-description",
    "href": "phd_projects/entries/CotterKalise_Fieldgames.html#project-description",
    "title": "A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games",
    "section": "",
    "text": "Colin Cotter is Professor of Computational Mathematics at Imperial College. He has relevant interests in optimal transport applied to the semigeostrophic equations and to weather forecast verification, and in design, analysis and implementation of finite element methods.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed variational methods for mean field games and control which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nStarting from the fluid dynamics formulation of the Monge-Kantorovich mass transfer problem proposed by Benamou and Brenier, it is now well-understood that a wide class of problems including Optimal Transport (OT), Schroedinger Bridges (SB), Mean Field Control (MFC), and Variational Mean Field Games (MFG), can be seen as the solution of a PDE-constrained optimization problem where a convex cost if constrained to a continuity equation. While the numerical approximation of these problems has been extensively studied over the last decade due to their importance in statistical machine learning, computational methods are often developed for a particular problem of interest and fail to identified the underlying unifying structure.\nIn this project we will develop a unified solver for OT, SB, MFC and MFG, so that each instance arises after a suitable assignment of costs and constraints. The solver will be based on convex optimization methods (primal-dual algorithms), preconditioning, and structure-preserving discretizations of the continuity equation.\n\n\n\nSoftware deliverables will include: - A unified OT/SB/MFC/MFG solver which is not currently available. This will be built around Firedrake, building upon the work of Natale and Todeschi.\nNatale, Andrea, and Gabriele Todeschi. “A mixed finite element discretization of dynamical optimal transport.” Journal of Scientific Computing 91, no. 2 (2022): 38."
  },
  {
    "objectID": "phd_projects/entries/Coveney_dynamicalsystems.html",
    "href": "phd_projects/entries/Coveney_dynamicalsystems.html",
    "title": "Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems",
    "section": "",
    "text": "Floating point pathologies in the simulation of chaotic dynamics on digital computers have been uncovered in the case of some remarkably simple chaotic maps and ordinary differential equations. In particular, the generalised Bernoulli map and the Lorenz 96 system exhibit certain behaviours where the numerical solutions generate incorrect results and related behaviour which is not understood. [1,2]\nA fundamental aspect of chaotic dynamics is the presence of unstable periodic orbits (UPOs), the enumeration of which provides the skeleton of chaos. However, the floating point numbers are unable to exactly identify the UPOs, with the result that substantial numbers of these orbits are missed. Moreover, the period of these orbits grows exponentially with the dimension of the differential equations underpinning them. Given that the statistical properties of these chaotic systems are determined by the spectrum of their UPOs, those properties are compromised by their absence.\n\n\n\nIn this research, the detailed way in which UPOs are excluded will be investigated theoretically as well as numerically in order to understand why, for higher dimensional versions of the Lorenz 96 system (N ~ 500), half, single and double precision floating point numbers produce closely similar statistical properties of the system.\nAn additional line of investigation will be undertaken with a view to implementing these dynamical systems on analogue computers, where no such floating point pathologies should arise. In principle, analogue solutions should be a much closer approximation to the true continuum behaviour of these systems and will throw further light on the floating point pathologies encountered on digital computers. It may well be necessary to extend the analysis to handle analogue systems incorporating noise.\n\n\n\nWe shall need to develop fast and highly efficient methods for identifying unstable periodic orbits to assess how many of them are missed using floating point numbers. A major problem in addressing difficulties caused by floats at present is that all conventional computers have IEEE floating point numbers on them; new numbering systems have not yet been deployed in any wide ranging manner on such devices. The most promising route to overcome these floating point pathologies is stochastic rounding, but while it can overcome some of these pathologies, there remains the intractability of computing the very large period orbits. The possibility of programming analogue computers is also relevant in order to test the accuracy of digital solutions.\n\nB. M. Boghosian, P. V. Coveney H. Wang, “A New Pathology in the Simulation of Chaotic Dynamical Systems on Digital Computers”, Advanced Theory and Simulations, 1900125 (2019), DOI:10.1002/adts.201900125\nM. Klöwer, P. V. Coveney, E. A. Paxton, T. N. Palmer, “Periodic orbits in chaotic systems simulated at low precision”, Nature Scientific Reports (2023) DOI: 10.1038/s41598-023-37004-4"
  },
  {
    "objectID": "phd_projects/entries/Coveney_dynamicalsystems.html#project-description",
    "href": "phd_projects/entries/Coveney_dynamicalsystems.html#project-description",
    "title": "Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems",
    "section": "",
    "text": "Floating point pathologies in the simulation of chaotic dynamics on digital computers have been uncovered in the case of some remarkably simple chaotic maps and ordinary differential equations. In particular, the generalised Bernoulli map and the Lorenz 96 system exhibit certain behaviours where the numerical solutions generate incorrect results and related behaviour which is not understood. [1,2]\nA fundamental aspect of chaotic dynamics is the presence of unstable periodic orbits (UPOs), the enumeration of which provides the skeleton of chaos. However, the floating point numbers are unable to exactly identify the UPOs, with the result that substantial numbers of these orbits are missed. Moreover, the period of these orbits grows exponentially with the dimension of the differential equations underpinning them. Given that the statistical properties of these chaotic systems are determined by the spectrum of their UPOs, those properties are compromised by their absence.\n\n\n\nIn this research, the detailed way in which UPOs are excluded will be investigated theoretically as well as numerically in order to understand why, for higher dimensional versions of the Lorenz 96 system (N ~ 500), half, single and double precision floating point numbers produce closely similar statistical properties of the system.\nAn additional line of investigation will be undertaken with a view to implementing these dynamical systems on analogue computers, where no such floating point pathologies should arise. In principle, analogue solutions should be a much closer approximation to the true continuum behaviour of these systems and will throw further light on the floating point pathologies encountered on digital computers. It may well be necessary to extend the analysis to handle analogue systems incorporating noise.\n\n\n\nWe shall need to develop fast and highly efficient methods for identifying unstable periodic orbits to assess how many of them are missed using floating point numbers. A major problem in addressing difficulties caused by floats at present is that all conventional computers have IEEE floating point numbers on them; new numbering systems have not yet been deployed in any wide ranging manner on such devices. The most promising route to overcome these floating point pathologies is stochastic rounding, but while it can overcome some of these pathologies, there remains the intractability of computing the very large period orbits. The possibility of programming analogue computers is also relevant in order to test the accuracy of digital solutions.\n\nB. M. Boghosian, P. V. Coveney H. Wang, “A New Pathology in the Simulation of Chaotic Dynamical Systems on Digital Computers”, Advanced Theory and Simulations, 1900125 (2019), DOI:10.1002/adts.201900125\nM. Klöwer, P. V. Coveney, E. A. Paxton, T. N. Palmer, “Periodic orbits in chaotic systems simulated at low precision”, Nature Scientific Reports (2023) DOI: 10.1038/s41598-023-37004-4"
  },
  {
    "objectID": "phd_projects/entries/ThomasShahrezaei_inference.html",
    "href": "phd_projects/entries/ThomasShahrezaei_inference.html",
    "title": "Inference of gene regulatory networks from single cell data",
    "section": "",
    "text": "Interpretation of single-cell transcriptomics data is a tremendous mathematical challenge. Existing statistical modelling tools often lack interpretability and can introduce ambiguity introducing unquantified errors that feed into downstream analyses. By inferring mechanistic mathematical models of stochastic gene expression and gene regulatory networks, the project aims to utilise distributional information to enhance the interpretability of predictions and shed light on the cellular processes underlying transcriptional regulation.\n\n\n\nOur major innovation is model-driven leveraging distributional information of transcriptomic signatures hidden behind averages to learn about transcriptional regulation in living cells. Such distributional information exists across cells, genes and modalities but is often aggregated, averaged, or ignored by current tools. We will utilise mechanistic stochastic models as interpretable generative models for single genes to produce an atlas of burst kinetics parameters across cell types, tissues, organisms and align differentiation trajectories. Extending our model to multiple genes allows us to leverage multi-modal distributional information to infer gene regulatory networks.\n\n\n\nOur model-driven approach will provide easy-to-use computational tools capable of Bayesian parameter estimation and model selection. The tools utilise amortised inference that efficiently scales to genomic data and will be used to create single-cell atlases of transcriptional regulation across several organisms. The availability of these computational pipelines is expected to enhance downstream analyses in applications, such as normalisation and data integration, providing lasting benefits to the field of single-cell transcriptomics. Overall, the project aims to significantly improve the interpretability, reliability, and computational efficiency of transcriptomic analyses, boosting our understanding of cellular processes and their disruption in disease.\n\n\n\nModelling capture efficiency of single cell RNA-sequencing data improves inference of transcriptome-wide burst kinetics W Tang, ACS Jorgensen, S Marguerat, P Thomas, V Shahrezaei Bioinformatics, Volume 39, Issue 7, July 2023, btad395\nGlobal transcription regulation revealed from dynamical correlations in time-resolved single-cell RNA-sequencing D Volteras, V Shahrezaei, P Thomas bioRxiv, 2023.10. 24.563709"
  },
  {
    "objectID": "phd_projects/entries/ThomasShahrezaei_inference.html#project-description",
    "href": "phd_projects/entries/ThomasShahrezaei_inference.html#project-description",
    "title": "Inference of gene regulatory networks from single cell data",
    "section": "",
    "text": "Interpretation of single-cell transcriptomics data is a tremendous mathematical challenge. Existing statistical modelling tools often lack interpretability and can introduce ambiguity introducing unquantified errors that feed into downstream analyses. By inferring mechanistic mathematical models of stochastic gene expression and gene regulatory networks, the project aims to utilise distributional information to enhance the interpretability of predictions and shed light on the cellular processes underlying transcriptional regulation.\n\n\n\nOur major innovation is model-driven leveraging distributional information of transcriptomic signatures hidden behind averages to learn about transcriptional regulation in living cells. Such distributional information exists across cells, genes and modalities but is often aggregated, averaged, or ignored by current tools. We will utilise mechanistic stochastic models as interpretable generative models for single genes to produce an atlas of burst kinetics parameters across cell types, tissues, organisms and align differentiation trajectories. Extending our model to multiple genes allows us to leverage multi-modal distributional information to infer gene regulatory networks.\n\n\n\nOur model-driven approach will provide easy-to-use computational tools capable of Bayesian parameter estimation and model selection. The tools utilise amortised inference that efficiently scales to genomic data and will be used to create single-cell atlases of transcriptional regulation across several organisms. The availability of these computational pipelines is expected to enhance downstream analyses in applications, such as normalisation and data integration, providing lasting benefits to the field of single-cell transcriptomics. Overall, the project aims to significantly improve the interpretability, reliability, and computational efficiency of transcriptomic analyses, boosting our understanding of cellular processes and their disruption in disease.\n\n\n\nModelling capture efficiency of single cell RNA-sequencing data improves inference of transcriptome-wide burst kinetics W Tang, ACS Jorgensen, S Marguerat, P Thomas, V Shahrezaei Bioinformatics, Volume 39, Issue 7, July 2023, btad395\nGlobal transcription regulation revealed from dynamical correlations in time-resolved single-cell RNA-sequencing D Volteras, V Shahrezaei, P Thomas bioRxiv, 2023.10. 24.563709"
  },
  {
    "objectID": "phd_projects/entries/Cotter_digitaltwin.html",
    "href": "phd_projects/entries/Cotter_digitaltwin.html",
    "title": "PDE-driven Digital Twins",
    "section": "",
    "text": "There are many definitions for digital twins but here we define a digital twin as a computational model of a physical (natural, engineered, social or hybrid) system combined with algorithms for assimilating observed data into the model and for controlling the physical system so that there is bidirectional interaction updated in real time. Some applications include: flood forecasting with river control, vertical farming, renewable energy devices, traffic pollution control in cities, etc.\nThis topic is for PhD researchers who are interested in numerical discretisations for PDEs and their embedding inside algorithms that couple them with data.\n\n\nThe lead supervisor is an expert in data assimilation (having written a textbook with Sebastian Reich) and also in finite element discretisations of PDEs.\n\n\n\nIn this project topic we will focus on the algorithmic challenges arising when the computational model involves the numerical solution of PDEs. In this case, to realise the full potential uses of digital twins, we need to develop new mathematics and algorithms to address the challenge to computational resources that otherwise exists. This includes: multiscale and multifidelity modelling, reduced order modelling, empirical surrogates (including generative models) and hybrid empirical/mechanistic models, data assimilation and control algorithms, all of which may need to be federated into a fully functional digital twin.\nProjects in this topic will focus in one or more of this algorithmic areas, developed in the context of finite element methods for partial differential equations, with the application area open depending on the interests and capabilities of the PhD researcher.\n\n\n\nThe project will deliver software for the chosen algorithms, so that they can be evaluated and extended. The software will take the form of a general purpose tool that can be applied to a range of models implemented using the Firedrake framework."
  },
  {
    "objectID": "phd_projects/entries/Cotter_digitaltwin.html#project-description",
    "href": "phd_projects/entries/Cotter_digitaltwin.html#project-description",
    "title": "PDE-driven Digital Twins",
    "section": "",
    "text": "There are many definitions for digital twins but here we define a digital twin as a computational model of a physical (natural, engineered, social or hybrid) system combined with algorithms for assimilating observed data into the model and for controlling the physical system so that there is bidirectional interaction updated in real time. Some applications include: flood forecasting with river control, vertical farming, renewable energy devices, traffic pollution control in cities, etc.\nThis topic is for PhD researchers who are interested in numerical discretisations for PDEs and their embedding inside algorithms that couple them with data.\n\n\nThe lead supervisor is an expert in data assimilation (having written a textbook with Sebastian Reich) and also in finite element discretisations of PDEs.\n\n\n\nIn this project topic we will focus on the algorithmic challenges arising when the computational model involves the numerical solution of PDEs. In this case, to realise the full potential uses of digital twins, we need to develop new mathematics and algorithms to address the challenge to computational resources that otherwise exists. This includes: multiscale and multifidelity modelling, reduced order modelling, empirical surrogates (including generative models) and hybrid empirical/mechanistic models, data assimilation and control algorithms, all of which may need to be federated into a fully functional digital twin.\nProjects in this topic will focus in one or more of this algorithmic areas, developed in the context of finite element methods for partial differential equations, with the application area open depending on the interests and capabilities of the PhD researcher.\n\n\n\nThe project will deliver software for the chosen algorithms, so that they can be evaluated and extended. The software will take the form of a general purpose tool that can be applied to a range of models implemented using the Firedrake framework."
  },
  {
    "objectID": "phd_projects/entries/ham_compiler.html",
    "href": "phd_projects/entries/ham_compiler.html",
    "title": "Domain-specific Compiler Technology for Finite Element Simulation on Tensor Hardware",
    "section": "",
    "text": "The finite element method is a powerful mathematical abstraction for simulations of continuous physical systems. It is employed in applications as diverse as weather prediction, fusion energy design, the stability of massive structures or the development of optimal cooling for high performance microchips, to name but a few.\nFiredrake is a software tool that automates the finite element method. It generates high-performance code for laptops, GPUs and supercomputers. Firedrake is used by a large and growing community of users worldwide, in universities, government institutions and companies. Firedrake exploits the mathematical abstraction of the finite element method to replace hand-written code with a series of specialist compiler layers that transform the user’s mathematical description of a simulation task into a high-performance parallel computation.\nFrom a user perspective, this provides a highly productive simulation capability which avoids the laborious, technical and error-prone task of writing low-level parallel code. From a computer science perspective, Firedrake delivers a representation of the simulation task which is higher level and more structured than low-level code. This provides the opportunity to exploit this structure to generate high performance implementations without the complex and fragile analysis needed to discover that structure in lower-level code\nThe availability of CPUs and GPUs with parallel instructions for not just vectors, but also matrices, creates new opportunities. Nvidia’s tensor cores provide a large performance advantage, and similar instruction set extensions are in the pipeline for Intel, AMD and ARM CPUs. The finite element method is often dominated by an assembly phase, essential a tensor contraction. The challenge in this project is to map this onto tensor/matrix instructions.\nThe Multi-Level Intermediate Representation (MLIR) is a framework for compiler architecture, building on the open-source LLVM infrastructure – with huge industrial support. MLIR’s dialect mechanism enables domain-specific compilers to be structured in a layered way – we aim to adopt MLIR (or the Python-based XDSL) - and to gain not only from multiple available compiler backends for diverse hardware, but also community support for tensor and graph/mesh optimisations.\n\n\nXDSL is a Python version of MLIR, interoperable with the main MLIR codebase, specifically designed to support DSLs in HPC; see GitHub - xdslproject/xdsl: A Python Compiler Design Toolkit), It is used, for example in [2404.02218] A shared compilation stack for distributed-memory parallelism in stencil DSLs\nThe Firedrake project will be the basis and host for this project.\n\n\n\nThe new compiler architecture delivered in this project will provide a new compiler back end to the Firedrake project and will be released as a part of that project. This will place it in immediate use by scientists and engineers worldwide. The student working on the project will have the opportunity to form collaborations with users tackling the most challenging of applications, and hence to take their work far further than the highly simplified test cases that often typify new technology developments."
  },
  {
    "objectID": "phd_projects/entries/ham_compiler.html#project-description",
    "href": "phd_projects/entries/ham_compiler.html#project-description",
    "title": "Domain-specific Compiler Technology for Finite Element Simulation on Tensor Hardware",
    "section": "",
    "text": "The finite element method is a powerful mathematical abstraction for simulations of continuous physical systems. It is employed in applications as diverse as weather prediction, fusion energy design, the stability of massive structures or the development of optimal cooling for high performance microchips, to name but a few.\nFiredrake is a software tool that automates the finite element method. It generates high-performance code for laptops, GPUs and supercomputers. Firedrake is used by a large and growing community of users worldwide, in universities, government institutions and companies. Firedrake exploits the mathematical abstraction of the finite element method to replace hand-written code with a series of specialist compiler layers that transform the user’s mathematical description of a simulation task into a high-performance parallel computation.\nFrom a user perspective, this provides a highly productive simulation capability which avoids the laborious, technical and error-prone task of writing low-level parallel code. From a computer science perspective, Firedrake delivers a representation of the simulation task which is higher level and more structured than low-level code. This provides the opportunity to exploit this structure to generate high performance implementations without the complex and fragile analysis needed to discover that structure in lower-level code\nThe availability of CPUs and GPUs with parallel instructions for not just vectors, but also matrices, creates new opportunities. Nvidia’s tensor cores provide a large performance advantage, and similar instruction set extensions are in the pipeline for Intel, AMD and ARM CPUs. The finite element method is often dominated by an assembly phase, essential a tensor contraction. The challenge in this project is to map this onto tensor/matrix instructions.\nThe Multi-Level Intermediate Representation (MLIR) is a framework for compiler architecture, building on the open-source LLVM infrastructure – with huge industrial support. MLIR’s dialect mechanism enables domain-specific compilers to be structured in a layered way – we aim to adopt MLIR (or the Python-based XDSL) - and to gain not only from multiple available compiler backends for diverse hardware, but also community support for tensor and graph/mesh optimisations.\n\n\nXDSL is a Python version of MLIR, interoperable with the main MLIR codebase, specifically designed to support DSLs in HPC; see GitHub - xdslproject/xdsl: A Python Compiler Design Toolkit), It is used, for example in [2404.02218] A shared compilation stack for distributed-memory parallelism in stencil DSLs\nThe Firedrake project will be the basis and host for this project.\n\n\n\nThe new compiler architecture delivered in this project will provide a new compiler back end to the Firedrake project and will be released as a part of that project. This will place it in immediate use by scientists and engineers worldwide. The student working on the project will have the opportunity to form collaborations with users tackling the most challenging of applications, and hence to take their work far further than the highly simplified test cases that often typify new technology developments."
  },
  {
    "objectID": "phd_projects/entries/Briol_simulation.html",
    "href": "phd_projects/entries/Briol_simulation.html",
    "title": "Simulation-based Inference for Expensive Scientific Simulators",
    "section": "",
    "text": "In many domains of science and engineering, statistical inference is challenging due to the unavailability of the likelihood associated to the scientific model of interest. Instead, it is often possible to simulate from these models, and to use these simulations to perform approximate inference. One challenge in this context is that these procedures typically rely on a large numbers of model simulations, which is a challenge when the simulator is computationally costly to run. Examples of such models include tsunami models based on non-linear shallow water equations, which take several minutes per run, or large eddy simulations of wind farms, which can take tens of hours per run. In these settings, statistical inference can be either extremely computationally demanding, or sometimes simply unfeasible.\n\n\n\nThe main aim of this project will be to develop novel simulation-based inference algorithms which are particularly suitable for inference with expensive scientific simulators. Different directions will be explored, including combining multi-fidelity, cost-aware and amortised methods. These should lead to novel algorithms which require far fewer expensive simulations, or where each simulation is cheaper than for existing methods.\n\n\n\nThe main development of software will be implementations in Python of these algorithms. The aim will be to contribute this code to existing software packages for simulation-based inference, including the pytorch-based package “sbi” and the package “BayesFlow”."
  },
  {
    "objectID": "phd_projects/entries/Briol_simulation.html#project-description",
    "href": "phd_projects/entries/Briol_simulation.html#project-description",
    "title": "Simulation-based Inference for Expensive Scientific Simulators",
    "section": "",
    "text": "In many domains of science and engineering, statistical inference is challenging due to the unavailability of the likelihood associated to the scientific model of interest. Instead, it is often possible to simulate from these models, and to use these simulations to perform approximate inference. One challenge in this context is that these procedures typically rely on a large numbers of model simulations, which is a challenge when the simulator is computationally costly to run. Examples of such models include tsunami models based on non-linear shallow water equations, which take several minutes per run, or large eddy simulations of wind farms, which can take tens of hours per run. In these settings, statistical inference can be either extremely computationally demanding, or sometimes simply unfeasible.\n\n\n\nThe main aim of this project will be to develop novel simulation-based inference algorithms which are particularly suitable for inference with expensive scientific simulators. Different directions will be explored, including combining multi-fidelity, cost-aware and amortised methods. These should lead to novel algorithms which require far fewer expensive simulations, or where each simulation is cheaper than for existing methods.\n\n\n\nThe main development of software will be implementations in Python of these algorithms. The aim will be to contribute this code to existing software packages for simulation-based inference, including the pytorch-based package “sbi” and the package “BayesFlow”."
  },
  {
    "objectID": "phd_projects/entries/bertrand2.html",
    "href": "phd_projects/entries/bertrand2.html",
    "title": "Accumulation and absorption of active particles at surfaces",
    "section": "",
    "text": "Active matter provides a powerful quantitative framework for understanding complex biological processes by examining the interplay between self-organizing, energy-consuming particles and their surrounding environment. Systems such as motile bacteria, self-propelled colloids, or cytoskeletal filaments exemplify this paradigm. Canonical models include run-and-tumble particles (RTPs), which change direction through discrete reorientations, and active Brownian particles (ABPs), whose motion combines constant propulsion speed with rotational diffusion. While the local energy consumption puts these systems inherently out-of-equilibrium, in isolation, active particles seen at long enough time and large enough distances remain diffusive; true nonequilibrium features stem from the interactions of active particles with their environment.\nFor instance, when confined within a channel, active particles tend to accumulate at the channel walls, even in the absence of inter-particle interactions. This is in clear contradiction with equilibrium Boltzmann distributions. Each particle pushes against the wall until a tumbling event or rotational diffusion redirects its motion enough that they can scatter off; this makes the wall behave like a sticky boundary. At the multi-particle level this results in a pressure being exerted on the confining walls. This behavior can also be described in terms of so-called sticky boundary condition: upon colliding with the wall, a particle remains attached for a random time governed by its tumbling dynamics. The degree of stickiness is characterized by the escape time back into the bulk; it spans from totally reflecting boundaries (instantaneous escape) to totally absorbing ones (permanent adhesion), with intermediate cases characterized by partial retention. Sticky boundary conditions are also relevant in understanding biological phenomena such as the dynamics of growing and shrinking polymer filaments.\nExtending this concept, partially permeable walls introduce another layer of complexity. Particles interacting with sticky boundaries may either re-enter the bulk or escape permanently, leading to a distinct set of behaviors compared to impermeable walls. In this scenario, the system lacks a steady-state density for particle position and orientation, and attention shifts to dynamic quantities like the mean first passage time (MFPT) for permanent absorption and its higher-order moments. These features underscore how the interactions between active particles and their environments drive nonequilibrium phenomena central to active matter systems.\n\n\nThe main goal of this project is to combine nonequilibrium statistical physics, mean field theory, and multi-scale computation to investigate the accumulationof particles. Recent studies have started to extend the equilibrium theory of wetting to systems of active particles showing that the stiffness of the wall controls a transition to wetting. We will here similarly study the condition of emergence of a wetting transition as a function of the absorption behaviour of the wall.\n• First-passage statistics – At the particle level, our study will also focus on determining important first-passage statistics including the mean first-passage time for single-particle absorption at a permeable wall as well as the extremal statistics of absorption in the case of multiple particles, quantifying for instance, first absorption times.\n• Theory of particle-surface interactions – We will develop a microscopic theory of particle-surface interactions and how this affects the accumulation and absorption of particles, including for flexible and active interfaces (modelling for instance a biological membrane).\n• Breakdown of mean-field – Throughout the project, we will compare large-scale particle-based simulations and mean-field analytical arguments. We will then investigate the breakdown of mean field theory due to the absorption and removal of particles from the population.\n\n\n\nThe success of this project will rely on the development of a number of advanced numerical simulations:\n\nnumerical algorithms for a large-scale computational exploration of a variety of minimal systems in statistical mechanics including efficient sampling techniques to explore rare events, extremal statistics and first-passage time statistics;\ndevelopment of efficient numerical algorithms for systems of coupled SDEs;\npurpose-built, scalable and adaptable software implementing advanced numerical solutions to highly nonlinear systems of PDEs and SPDEs to solve our mean-field models."
  },
  {
    "objectID": "phd_projects/entries/bertrand2.html#project-description",
    "href": "phd_projects/entries/bertrand2.html#project-description",
    "title": "Accumulation and absorption of active particles at surfaces",
    "section": "",
    "text": "Active matter provides a powerful quantitative framework for understanding complex biological processes by examining the interplay between self-organizing, energy-consuming particles and their surrounding environment. Systems such as motile bacteria, self-propelled colloids, or cytoskeletal filaments exemplify this paradigm. Canonical models include run-and-tumble particles (RTPs), which change direction through discrete reorientations, and active Brownian particles (ABPs), whose motion combines constant propulsion speed with rotational diffusion. While the local energy consumption puts these systems inherently out-of-equilibrium, in isolation, active particles seen at long enough time and large enough distances remain diffusive; true nonequilibrium features stem from the interactions of active particles with their environment.\nFor instance, when confined within a channel, active particles tend to accumulate at the channel walls, even in the absence of inter-particle interactions. This is in clear contradiction with equilibrium Boltzmann distributions. Each particle pushes against the wall until a tumbling event or rotational diffusion redirects its motion enough that they can scatter off; this makes the wall behave like a sticky boundary. At the multi-particle level this results in a pressure being exerted on the confining walls. This behavior can also be described in terms of so-called sticky boundary condition: upon colliding with the wall, a particle remains attached for a random time governed by its tumbling dynamics. The degree of stickiness is characterized by the escape time back into the bulk; it spans from totally reflecting boundaries (instantaneous escape) to totally absorbing ones (permanent adhesion), with intermediate cases characterized by partial retention. Sticky boundary conditions are also relevant in understanding biological phenomena such as the dynamics of growing and shrinking polymer filaments.\nExtending this concept, partially permeable walls introduce another layer of complexity. Particles interacting with sticky boundaries may either re-enter the bulk or escape permanently, leading to a distinct set of behaviors compared to impermeable walls. In this scenario, the system lacks a steady-state density for particle position and orientation, and attention shifts to dynamic quantities like the mean first passage time (MFPT) for permanent absorption and its higher-order moments. These features underscore how the interactions between active particles and their environments drive nonequilibrium phenomena central to active matter systems.\n\n\nThe main goal of this project is to combine nonequilibrium statistical physics, mean field theory, and multi-scale computation to investigate the accumulationof particles. Recent studies have started to extend the equilibrium theory of wetting to systems of active particles showing that the stiffness of the wall controls a transition to wetting. We will here similarly study the condition of emergence of a wetting transition as a function of the absorption behaviour of the wall.\n• First-passage statistics – At the particle level, our study will also focus on determining important first-passage statistics including the mean first-passage time for single-particle absorption at a permeable wall as well as the extremal statistics of absorption in the case of multiple particles, quantifying for instance, first absorption times.\n• Theory of particle-surface interactions – We will develop a microscopic theory of particle-surface interactions and how this affects the accumulation and absorption of particles, including for flexible and active interfaces (modelling for instance a biological membrane).\n• Breakdown of mean-field – Throughout the project, we will compare large-scale particle-based simulations and mean-field analytical arguments. We will then investigate the breakdown of mean field theory due to the absorption and removal of particles from the population.\n\n\n\nThe success of this project will rely on the development of a number of advanced numerical simulations:\n\nnumerical algorithms for a large-scale computational exploration of a variety of minimal systems in statistical mechanics including efficient sampling techniques to explore rare events, extremal statistics and first-passage time statistics;\ndevelopment of efficient numerical algorithms for systems of coupled SDEs;\npurpose-built, scalable and adaptable software implementing advanced numerical solutions to highly nonlinear systems of PDEs and SPDEs to solve our mean-field models."
  },
  {
    "objectID": "phd_projects/entries/jones.html",
    "href": "phd_projects/entries/jones.html",
    "title": "Single Cell Ageing and mtDNA: learning and simulation",
    "section": "",
    "text": "The aim of this project is o understand how mtDNA mutations accumulate in our cells, and what their effects are, using a mix of simulation, inference and machine learning.\n\n\nWe have been investigating (genetic) variation in cellular power stations (mitochondria) using a mix of ideas from stochastic population processes/statistical genetics, statistical physics, statistical inference/machine learning and control. Our goal is to understand human ageing and to inform therapies to combat disease. We believe this is a particularly exciting area to study for two reasons. Not only does 1) mitochondrial (dys)function have deep connections to therapies for conditions like Parkinsons, Diabetes, ageing and Cancer (and we are working on these) it is 2) a topic that, though poorly understood, is susceptible to the very basic (though mathematically nuanced) models that one constructs in theoretical and mathematical physics/ statistical genetics. This is an area where students can explore a new scientific direction since the medical promise and scientific opportunities easily exceed the number of theorists. Students can develop new evolutionary theory while also contributing to therapeutic design. We place a substantial emphasis on single-cell sequencing and single-cell transcriptomics (these are among the most active areas of modern biomedicine): this involves aspects of machine learning and bioinformatics. We build stochastic models that link to the results of our inference from the transcriptomics. We also link information about cellular state to cellular mutational state using tools from (Bayesian) Machine Learning. The applicant will have an opportunity to participate in a large project with the Cambridge experimental groups of Profs Patrick Chinnery and Maria Spillantini on Parkinson’s disease and mitochondria. Our work will build on our recent paper: ‘Cryptic mitochondrial ageing coincides with mid-late life and is pathophysiologically informative in single cells across tissues and species’ https://www.biorxiv.org/content/10.1101/2023.07.04.547509v1\n\n\n\nExemplar objectives would be to:\nSearch for evidence for selection of mutated mtDNA\nDiscover whether epigenetics or mtDNA mutations are better associated with ageing\nDevelop causal inference tools linking mtDNA mutations to gene-expression\nSimulate mutation accumulation in single cells and in tissues\n\n\n\nData analysis: machine learning linking mtDNA mutation and gene expression; causal inference linking mutations and genes; (Approximate) Bayesian inference fitting stochastic models for mutations\nSimulation: forward-simulating mutation accumulation in cellular populations and simulations."
  },
  {
    "objectID": "phd_projects/entries/jones.html#project-description",
    "href": "phd_projects/entries/jones.html#project-description",
    "title": "Single Cell Ageing and mtDNA: learning and simulation",
    "section": "",
    "text": "The aim of this project is o understand how mtDNA mutations accumulate in our cells, and what their effects are, using a mix of simulation, inference and machine learning.\n\n\nWe have been investigating (genetic) variation in cellular power stations (mitochondria) using a mix of ideas from stochastic population processes/statistical genetics, statistical physics, statistical inference/machine learning and control. Our goal is to understand human ageing and to inform therapies to combat disease. We believe this is a particularly exciting area to study for two reasons. Not only does 1) mitochondrial (dys)function have deep connections to therapies for conditions like Parkinsons, Diabetes, ageing and Cancer (and we are working on these) it is 2) a topic that, though poorly understood, is susceptible to the very basic (though mathematically nuanced) models that one constructs in theoretical and mathematical physics/ statistical genetics. This is an area where students can explore a new scientific direction since the medical promise and scientific opportunities easily exceed the number of theorists. Students can develop new evolutionary theory while also contributing to therapeutic design. We place a substantial emphasis on single-cell sequencing and single-cell transcriptomics (these are among the most active areas of modern biomedicine): this involves aspects of machine learning and bioinformatics. We build stochastic models that link to the results of our inference from the transcriptomics. We also link information about cellular state to cellular mutational state using tools from (Bayesian) Machine Learning. The applicant will have an opportunity to participate in a large project with the Cambridge experimental groups of Profs Patrick Chinnery and Maria Spillantini on Parkinson’s disease and mitochondria. Our work will build on our recent paper: ‘Cryptic mitochondrial ageing coincides with mid-late life and is pathophysiologically informative in single cells across tissues and species’ https://www.biorxiv.org/content/10.1101/2023.07.04.547509v1\n\n\n\nExemplar objectives would be to:\nSearch for evidence for selection of mutated mtDNA\nDiscover whether epigenetics or mtDNA mutations are better associated with ageing\nDevelop causal inference tools linking mtDNA mutations to gene-expression\nSimulate mutation accumulation in single cells and in tissues\n\n\n\nData analysis: machine learning linking mtDNA mutation and gene expression; causal inference linking mutations and genes; (Approximate) Bayesian inference fitting stochastic models for mutations\nSimulation: forward-simulating mutation accumulation in cellular populations and simulations."
  },
  {
    "objectID": "phd_projects/entries/Smears_mean_field.html",
    "href": "phd_projects/entries/Smears_mean_field.html",
    "title": "Numerical methods for Mean Field Games",
    "section": "",
    "text": "Mean field games are game theoretical models for large populations of players subject to stochastic dynamics, and find application in many fields of industrial, societal, and economic interest. Mathematically, the Nash equilibria of the game are characterized by a coupled system of nonlinear partial differential equations, involving a Hamilton–Jacobi–Bellman (HJB) equation coupled with a Kolmogorov–Fokker–Planck (KFP) equation. The mathematical theory of these problems is incredibly rich by tying together PDE theory, numerical analysis, game theory, convex analysis, stochastic processes and control theory. MFG have attracted significant interest owing to the combination of several analytical and numerical challenges, including the lack of a variational formulation for general couplings, the lack of coercivity or monotonicity properties of the operators, the nonlocality of the couplings, the coupled forward-backward structure of the time-dependent problem.\nThe group of the principal supervisor has made significant progress in the development and analysis of numerical methods for nonlinear partial differential equations, including Hamilton-Jacobi-Bellman equations and Mean Field Game systems, for a range of numerical methods including conforming and nonconforming methods, stabilized, high-order and/or adaptive methods. A recent advance in the field has been the discovery that, in general, the nonuniqueness of the optimal controls for the players of the game can lead to Nash equilibria with more complex structures, in particular asymmetry in player strategies despite the symmetry of the game. In such cases, the MFG system can no longer be understood as a partial differential equation but instead must be understood as a partial differential inclusion for nonlinear set-valued differential operators. In recent work we have developed finite element methods with a complete convergence proof for the full space-time problem with nonlocal and nonlinear couplings. In the case of steady-state problems with regular Hamiltonians, we have also given the first proof of quasi-optimal convergence for any family of numerical methods.\nEven with recent progress, there are many important challenges challenges remaining that can form the basis for the project. These include the need for developing novel nonlinear stabilization/flux limiting techniques that guarantee structure preservation and also dualities between components of the system. There is also the need for novel solution algorithms for the fully coupled space-time system. Finally there is a need for novel methods including regularization and approximations for handling the set-valued operators in the inclusions.\n\n\nThe software deliverables include the development of novel discretization methods and efficient solvers, and integration into numerical software packages such as Firedrake, NGSolve, etc, or as standalone open-source packages in a suitable language. The software would provide the numerical community with novel discretization techniques, and also provide the wider scientific community with effective new tools for modelling and simulating new and important applications of MFG."
  },
  {
    "objectID": "phd_projects/entries/Smears_mean_field.html#project-description",
    "href": "phd_projects/entries/Smears_mean_field.html#project-description",
    "title": "Numerical methods for Mean Field Games",
    "section": "",
    "text": "Mean field games are game theoretical models for large populations of players subject to stochastic dynamics, and find application in many fields of industrial, societal, and economic interest. Mathematically, the Nash equilibria of the game are characterized by a coupled system of nonlinear partial differential equations, involving a Hamilton–Jacobi–Bellman (HJB) equation coupled with a Kolmogorov–Fokker–Planck (KFP) equation. The mathematical theory of these problems is incredibly rich by tying together PDE theory, numerical analysis, game theory, convex analysis, stochastic processes and control theory. MFG have attracted significant interest owing to the combination of several analytical and numerical challenges, including the lack of a variational formulation for general couplings, the lack of coercivity or monotonicity properties of the operators, the nonlocality of the couplings, the coupled forward-backward structure of the time-dependent problem.\nThe group of the principal supervisor has made significant progress in the development and analysis of numerical methods for nonlinear partial differential equations, including Hamilton-Jacobi-Bellman equations and Mean Field Game systems, for a range of numerical methods including conforming and nonconforming methods, stabilized, high-order and/or adaptive methods. A recent advance in the field has been the discovery that, in general, the nonuniqueness of the optimal controls for the players of the game can lead to Nash equilibria with more complex structures, in particular asymmetry in player strategies despite the symmetry of the game. In such cases, the MFG system can no longer be understood as a partial differential equation but instead must be understood as a partial differential inclusion for nonlinear set-valued differential operators. In recent work we have developed finite element methods with a complete convergence proof for the full space-time problem with nonlocal and nonlinear couplings. In the case of steady-state problems with regular Hamiltonians, we have also given the first proof of quasi-optimal convergence for any family of numerical methods.\nEven with recent progress, there are many important challenges challenges remaining that can form the basis for the project. These include the need for developing novel nonlinear stabilization/flux limiting techniques that guarantee structure preservation and also dualities between components of the system. There is also the need for novel solution algorithms for the fully coupled space-time system. Finally there is a need for novel methods including regularization and approximations for handling the set-valued operators in the inclusions.\n\n\nThe software deliverables include the development of novel discretization methods and efficient solvers, and integration into numerical software packages such as Firedrake, NGSolve, etc, or as standalone open-source packages in a suitable language. The software would provide the numerical community with novel discretization techniques, and also provide the wider scientific community with effective new tools for modelling and simulating new and important applications of MFG."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "",
    "text": "Professor Alexandros Beskos has made fundamental research contributions regarding:\n\nthe development of computationally intensive Sequential Monte-Carlo (SMC) algorithms that can overcome the curse-of-dimensionality that characterises standard approaches [1];\nthe extension of the applicability of SMC algorithms on high-dimensional models, across a spectrum of application fields, including: physics-driven Data Assimilation problems in atmospheric sciences [6]; whole-genome applications in biomedicine [5]; multivariate Graphical models in finance [3]. The above research has been carried out in collaboration with expert colleagues, in the designated fields (Professor Dan Crisan, Dr Nikolas Kantas at Imperial College, UK; Professor Ajay Jasra at the Chinese University of Hong Kong in Shenzhen, China; Professor Maria de Iorio at the National University of Singapore; Prof Stephan Beck at the Cancer Institute, UCL)."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#project-description",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#project-description",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "",
    "text": "Professor Alexandros Beskos has made fundamental research contributions regarding:\n\nthe development of computationally intensive Sequential Monte-Carlo (SMC) algorithms that can overcome the curse-of-dimensionality that characterises standard approaches [1];\nthe extension of the applicability of SMC algorithms on high-dimensional models, across a spectrum of application fields, including: physics-driven Data Assimilation problems in atmospheric sciences [6]; whole-genome applications in biomedicine [5]; multivariate Graphical models in finance [3]. The above research has been carried out in collaboration with expert colleagues, in the designated fields (Professor Dan Crisan, Dr Nikolas Kantas at Imperial College, UK; Professor Ajay Jasra at the Chinese University of Hong Kong in Shenzhen, China; Professor Maria de Iorio at the National University of Singapore; Prof Stephan Beck at the Cancer Institute, UCL)."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-atmospheric-sciences",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-atmospheric-sciences",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of atmospheric sciences:",
    "text": "Direction of atmospheric sciences:\nSMC methods – in particular Particle Filters (PFs) – have only very recently started being attempted and used in high-dimensional Data Assimilation applications. In contrast to ensemble Kalman Filters (EnKFs) which are the particle-based algorithms typically used in Data Assimilation and are based on Gaussian approximations, PFs make use of the correct model and can recover strong non-linearities characterising dynamical systems, especially at high resolutions [7]. However, standard PF algorithms are known to suffer from the curse-of-dimensionality.\nRecent research has focused on the development of a new generation of PFs that can be effective for Data Assimilation applications in high dimensions. Such PFs are termed “Localised Particle Filters” (LPFs). LPFs can overcome the curse-of-dimensionality by replacing a single high-dimensional global importance sampling step – across the whole domain of the signal and for all set of arriving observations – with a number of ‘local’ importance sampling steps (at a lower dimension) that only make use of the subset of data that are informative for chosen subdomains. The localised approach is based on the simple remark that observations obtained at a given position of the domain of the signal contain minimal information for parts of the domain which are far from such a position.\nSuch methodological advances have led to LPFs very recently been tested in operational settings in Numerical Weather Forecasting (NWF) [8], where states can be of dimension of O(10^9) or higher."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-biomedicine",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-biomedicine",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of biomedicine:",
    "text": "Direction of biomedicine:\nThere is a need for the development of statistical models for genome-wide epigenetics data and of the accompanying computational methods for their calibration. Such computations can involve the use of state-of-the-art scalable SMC algorithms and High-Performance Computing (HPC), as datasizes can be of order of 10^8 or more. Appropriate models recently proposed in the literature include, e.g., change-point dynamics [5] that track DNA methylation patterns jointly over cases/controls, across the whole genome, and can identify particular positions along the genome where cases and controls have different patterns.\nFull Bayesian inference for genome-wide models can provide much more information for underlying patterns than competing methods. So far Biologists have relied on out-of-shelf approaches when the cost for producing the data in the lab is enormous, so maximised extraction of information from available data is of high significance. In general, there appears to be lack of expertise in the development of suitable statistical models in this field and in corresponding scalable computational methods for fitting these models."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-finance",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-finance",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of finance:",
    "text": "Direction of finance:\nHigh-dimensional applications, involving dynamical models, are abundant in finance, see e.g. [9]. Recent advances in generic computational SMC methodology have yet to be fully appreciated by researchers focused on applications in this area."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#main-objectives-of-the-project",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\nThe main objectives of the project will depend on the particular application domain of interest, and can be summarised as follows:\n\nInvestigate state-of-the-art SMC algorithms. Such a study is important given several new methods proposed in recent research in the field of Data Assimilation (see e.g. [4]) or in a general context in (see e.g. [2]).\nExplore key strengths/weaknesses of SMC algorithms and contrast approaches against alternative biased methods (e.g. EnKFs in atmospheric sciences).\nStudy performance of advanced SMC methods across a number of benchmark model scenarios.\nDevelop novel SMC algorithms which make use of expert methodology available in the SMC community, but which has yet to be fully used by researchers focused on particular application domains. Algorithmic tools already shown to greatly improve performance of SMC involve e.g.: tempering accompanied by jittering of particles to better disperse the latter across space; adaptation techniques; data-driven improved proposals for particles. Such procedures aim to generate particles within a Monte-Carlo framework which represent effectively the hidden signal. This latter signal can be modelled, e.g., via PDE/SPDE dynamics in atmospheric sciences, change point models in genome-wide applications in biomedicine, and time-evolving Graphical models in finance."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#details-of-softwaredata-deliverables",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Details of Software/Data deliverables",
    "text": "Details of Software/Data deliverables\nThere is an apparent lack of software for implementation of advanced SMC methods in all three areas of application highlighted above (and beyond). Provision of such software will be a key output delivered by the PhD project, given the chosen application field. Indicatively, in the field of atmospheric sciences, the comprehensive review work in [4] provides a package in GitHub (https://github.com/thiery-lab/data-assimilation) but the purpose of this software seems to be the reproducibility of the results in the accompanying paper rather than a general-purpose software. In the case of genome-wide applications in biomedicine, Professor Alexandros Beskos has been involved in the development of the package in https://github.com/ucl-medical-genomics/hygeia, but this is still at its infancy. Thus, the proposed project will provide an opportunity for a trainee in Computational Modelling to develop a software that will cover a gap in an important application field – selected based on the applicant’s interests. As such, the produced software can have a large impact. By their very nature, SMC algorithms require state-of-the-art parallelisation and High-Performance Computing (HPC) implementations. Computations will be carried out within CPU or GPU architectures, depending on the problem at hand, or on the Cloud."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#references",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#references",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "References",
    "text": "References\n[1] Beskos, A., Crisan D., Jasra, A. (2014). On the stability of sequential Monte Carlo methods in high dimensions. Annals of Applied Probability, 24, 1396-1445.\n[2] Finke, A., Thiery, A. (2023). Conditional sequential Monte Carlo in high dimensions. Annals of Statistics 51, 437-463.\n[3] Franzolini, B., Beskos, A., De Iorio, M., Poklewski Koziell, W., Grzeszkiewicz, K. (2024). Change point detection in dynamic Gaussian graphical models: the impact of COVID-19 pandemic on the US stock market. Annals of Applied Statistics 18, 555-584.\n[4] Graham, M., Thiery, A. (2019). A scalable optimal-transport based local particle filter. arXiv preprint arXiv:1906.00507."
  },
  {
    "objectID": "phd_projects/entries/Briol_Bayesianinference.html",
    "href": "phd_projects/entries/Briol_Bayesianinference.html",
    "title": "Robust Bayesian Inference at Scale",
    "section": "",
    "text": "### Existing background work\nRobustness refers to the ability of a model to perform well on unseen data, or data that is different from the data it was trained on. It is an ever-evolving challenge for practitioners of statistical and machine learning methods, who need to deal with large, complex, and un-curated data sets and build tools that are reliable in uncontrolled environments. It is also particularly important in critical applications, such as medical diagnosis or weather prediction, where a lack of robustness can have a severe impact. In this project, we will focus on the robustness of Bayesian machine learning. In Bayesian inference, we start with a prior belief about a quantity of interest, and then update this belief based on our model of the world and new evidence in the form of data. This allows us to formally describe our uncertainty and make reliable predictions. However, a crucial assumption is that the model can truly represent the data-generating process. When this assumption is violated, the model is called misspecified, and our predictions become unreliable. To remedy this issue, a wide range of approaches have been proposed to make Bayesian methods more robust, including modifications of standard likelihoods and generalised Bayesian approaches.\n\n\nThe main aim of this project will be to develop novel robust alternative to existing probabilistic machine learning algorithms which are used across the physical sciences, ranging from statistical emulators such as Gaussian processes to algorithms for large-scale filtering such as the Kalman filter. The focus will be on carefully selecting the generalised Bayesian update rules to ensure these algorithms have the same, or even lower, computational complexity than their existing counter-parts based on Bayesian updates, and that they can be efficiently implemented on modern scientific computing hardware and infrastructure.\n\n\n\nThe main development of software will be a package for robust Bayesian methods in Python. The aim will be to make the algorithms developed as part of this project easily available to the scientific community, and to make it straightforward for algorithms to be rigorously and fairly compared against existing competitors."
  },
  {
    "objectID": "phd_projects/entries/Briol_Bayesianinference.html#project-description",
    "href": "phd_projects/entries/Briol_Bayesianinference.html#project-description",
    "title": "Robust Bayesian Inference at Scale",
    "section": "",
    "text": "### Existing background work\nRobustness refers to the ability of a model to perform well on unseen data, or data that is different from the data it was trained on. It is an ever-evolving challenge for practitioners of statistical and machine learning methods, who need to deal with large, complex, and un-curated data sets and build tools that are reliable in uncontrolled environments. It is also particularly important in critical applications, such as medical diagnosis or weather prediction, where a lack of robustness can have a severe impact. In this project, we will focus on the robustness of Bayesian machine learning. In Bayesian inference, we start with a prior belief about a quantity of interest, and then update this belief based on our model of the world and new evidence in the form of data. This allows us to formally describe our uncertainty and make reliable predictions. However, a crucial assumption is that the model can truly represent the data-generating process. When this assumption is violated, the model is called misspecified, and our predictions become unreliable. To remedy this issue, a wide range of approaches have been proposed to make Bayesian methods more robust, including modifications of standard likelihoods and generalised Bayesian approaches.\n\n\nThe main aim of this project will be to develop novel robust alternative to existing probabilistic machine learning algorithms which are used across the physical sciences, ranging from statistical emulators such as Gaussian processes to algorithms for large-scale filtering such as the Kalman filter. The focus will be on carefully selecting the generalised Bayesian update rules to ensure these algorithms have the same, or even lower, computational complexity than their existing counter-parts based on Bayesian updates, and that they can be efficiently implemented on modern scientific computing hardware and infrastructure.\n\n\n\nThe main development of software will be a package for robust Bayesian methods in Python. The aim will be to make the algorithms developed as part of this project easily available to the scientific community, and to make it straightforward for algorithms to be rigorously and fairly compared against existing competitors."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Leadership Team",
    "section": "",
    "text": "Prof Timo Betcke is Centre Director. He is Professor of Computational Mathematics at UCL.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Colin Cotter is Co-Lead for the CDT. He is Professor of Computational Mathematics at Imperial College.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Marta Betcke is Co-Director for Research and Partnership. She is Professor of Scientific Computing at UCL.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Ruth Misener is Co-Director for Research and Partnership. She is Professor in Computational Optimisation at Imperial College.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Serge Guillas is Co-Director for Training. He is the Met Office Joint Chair in Data Sciences for Weather and Climate at UCL.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Dr Dante Kalise is Co-Director for Training. He is a Reader in Computational Optimisation and Control at Imperial College.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Hao Ni is Co-Director for Admissions. She is Professor of Mathematics at UCL and a Turing Fellow at the Alan Turing Institute.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Dr Vahid Shahrezaei is Co-Director for Admissions. He is a Reader in Biomathematics at Imperial College.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Laura Beer is Centre Manager for the CCMI CDT. She is a Professional Research Investment & Strategy Manager at UCL ARC.\n                \n                \n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Matthew Scroggs is Teaching and Training lead for the CCMI CDT. He is a Senior Research Software Engineer at UCL ARC.\n                \n                Homepage\n                \n                \n                Orcid\n                \n                \n                GitHub\n                \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/ccmi_start.html",
    "href": "blog/posts/ccmi_start.html",
    "title": "CCMI is about to start",
    "section": "",
    "text": "The first CCMI cohort is about to start. For UCL students term starts tomorrow, 22 September, while our Imperial students arrive one week later. The last year has seen a flurry of preparations, from two recruitment rounds to organising facilities, making contracts with our partners and many more.\nI would like to share the Director’s Welcome in our Student Handbook. It reminds of what CCMI stands for and what our values are.\nDear students,\nA warm welcome to those of you who are new and congratulations on making the CCMI CDT your programme of choice.\nCCMI is a unique and novel approach to Doctoral Training. The traditional view is that there are the computational and data sciences on one side, and software engineering as a professional service for these sciences on the other side. At CCMI we believe that research and software are deeply interlinked and should not be viewed as separate from each other. This is the mission of our CDT. You will receive world leading training on the interface of the computational and data science and research software engineering, enabling you to develop tools and methodologies to solve the big challenges of the 21st century.\nNot only will you be involved in cutting edge research and software, you will be part of something special, a cohort of like-minded PhD students who will be there with you for all your successes but also for the stressful moments and the days when nothing seems to work. You will make lifelong bonds in this CDT, building networks of friends and colleagues who will be there for you long after you complete your PhD.\nFinally, I would like to emphasise to you our commitment to Equality, Diversity, and Inclusion. At its core the message of EDI is very simple, treat each other with kindness, respect, and decency. In an uncertain world, where even these simple values are under attack, you have the chance to make a difference, not just through your research but by helping to build a better, more inclusive, and tolerant world.\nWith this said, all of us from the CCMI Team wish you a wonderful start and many happy memories and successes over the next four years.\nTimo Betcke"
  },
  {
    "objectID": "blog/posts/modern_hybrid_working.html",
    "href": "blog/posts/modern_hybrid_working.html",
    "title": "To be or not to be in the office",
    "section": "",
    "text": "In central London universities the question of remote vs in office working is a difficult one. Before the 2020 pandemic in office working was the standard for most staff and PhD students. Then the pandemic happened and suddenly for a long time remote working became the norm.\nFrom 2022 onwards as things slowly normalised again there was at universities like at many other work places no general return to office and we now live in a hybrid work environment. This has a lot of advantages, such as better adjustments for childcare, reduction of high commuting costs (especially in London), and generally achieving a better work/life balance.\nBut how should CDTs handle in-office vs remote working? There is widespread consensus across academics that the fully remote working conditions for PhD students during the pandemic was detrimental to their professional development. They missed out on day to day research interactions over tea, did not have the same networking experience as students before them and had to deal with the loneliness of doing an individual research project cut-off from their peers.\nAt the same time, unless required through e.g. certain types of lab works, it is unreasonable to expect of PhD students that they spend every day in the office when all around them staff are working mostly hybrid. Moreover, high commuting costs in places like London mean that coming to the office is not cheap.\nA personal rule of thumb is that students should aim to spend more than half of a week physically among their peers unless there is important reasons not to (e.g. health, care responsibilities, etc.). But getting students in the office cannot come through enforcement alone. There must be real benefit of being in the office. And there the cohort based training vision of a CDT is important. From spontaneous lunches to encouraging project collaborations among students there is a lot we can do to make a CDT an attractive place to be physically present.\nThis is what we try to do at CCMI. We are only at the beginning of this journey and will make mistakes along the way. But thinking back to the pandemic and the loneliness of PhD students at the time it is a worthwile endeavour to recreate an environment in which physical interactions are the norm and not the exception."
  },
  {
    "objectID": "interview.html",
    "href": "interview.html",
    "title": "Interview preparation",
    "section": "",
    "text": "This page collects some information for applicants who have been shortlisted to the interview stage.\n\nTime and Location\nThe next round of interviews takes place 12/13 February. Applicants will be sent an online invite for a Zoom interview with a timeslot in those two days. All interviews are online.\n\n\nInterview panel\nThe panel for each interview will consist of two to three academic members of staff across UCL and Imperial.\n\n\nThe setup of the interview\nThe interview will be roughly 30-35 minutes in length and consist of three parts.\n\nA technical part to assess the technical knowledge of applicants. Below we have a list of publications. We ask you to select one of these publications and give a 5 minutes presentation about the key results of the publication. We will then spend 10 minutes to ask you more topical questions about the area of the publication. So that you can prepare we have collected relevant topic areas for each publication below. Please note that we will be probing your knowledge. But we do not expect that you can give in-depth answers to everything.\nA discussion on your fit to the CDT (around 7 minutes). We will ask you about your motivation to join the CDT, how you would contribute to the software journey, and how your previous experience helps you with the software journey. Detailed info about the software journey programme of the CDT is given below.\nFinally, we discuss relevant research areas for you (around 8 minutes). This is to get an indication about the kind of PhD projects that would be most suitable for you. We want to know what areas you are most interested in and why. We will ask you how a PhD helps your desired career path, and how your previous education has prepared you to do research on a PhD level in your desired research area.\n\n\n\nWhat to expect after the interview?\nWe are aiming to make decisions within a few days after your interview. If after the interview you are shortlisted for project selection there will be a two weeks getting to know phase, in which you can talk to supervisors, discuss projects in detail with us, and can get to know potential supervisors at open events. By the end of this phase we will ask you to submit your preferred PhD topics. We will aim to accommodate those and guide you to a PhD project that is most suitable for you. Once you and a supervisor are matched for a PhD project you will need to submit a pro-forma application to either UCL or Imperial so that the universities can do a proper document check, confirm your fee status and create an offer for a place in the CDT. Note that a place in the CDT is guaranteed only once the offer is sent out.\nIf you have not been shortlisted after interview you may be put on a reserve list. We will provide regular updates about reserve list status and make you aware of a final decision as soon as possible. Not all formal offers for a PhD will be taken up by students, so there is a good chance that those on the reserve list may be offered a place later.\nIf you are neither shortlisted nor on the reserve list we are unfortunately not able to offer you a place in the CDT for this year. But you are very welcome to apply for future entry. If requested we will be very happy to try and provide feedback for your interview so that you have advice for other PhD interviews in the future.\n\n\nPreparation Material for the Interview\nWe will ask you to give a 5 minutes presentation about one of the following papers. We have added some topical markers about what you may want to focus on.\n\nLiberty et. al. Randomized Algorithms for the low-rank approximation of matrices. This paper suggests two randomized algorithms to compute the Interpolative Decomposition of matrices. Questions to consider include: What is the Singular Value Decomposition and what is the Interpolative Decomposition (ID). Why is the ID useful? How is the ID computed by the two algorithms? How can we compute an SVD from this?\nTurner, An introduction to Transformers. This very recent paper describes the mathematical ideas of transformer neural networks. You are asked to describe the basic ideas behind transformers, their precise definition and how they are used in neural networks.\nE, A Proposal on Machine Learning via Dynamical Systems. This paper discusses connections of dynamical systmems and deep learning. You should know how a dynamical system can be used to define a learning model, and be able to describe the optimal control in this context. You should also have a good intuition on how this connects to deep neural networks.\n\nFor your 5 minutes presentation please make sure that:\n\nYou stay exactly within your 5 minutes. Practice your talk beforehand and time it. The best is to have a timer visible on your mobile phone or otherwise nearby.\nThe presentation setup works, i.e. you can share your screen (especially on Mac make sure that under the Privacy Settings for Mac OS screen sharing is eanbled for Zoom.) Test your setup beforehand.\nYou do not have too many slides. A typical beginner mistake is to have too many slides. You want to present the key ideas on a few slides. 5 minutes is not enough for a deep dive but gives you enough time to explain the main ideas.\nPlease feel free to blur the background or add a background picture on your zoom call by changing the zoom settings should you wish.\n\nFor your interview you will also be asked how to contribute to the Software Journey in the CDT. The software journey has the following two key components.\n\nA software bazaar in which students, academics, and external partners can offer software projects, ranging from small extensions of existing software to larger open-source projects. Anything is possible. The conditions are that it is useful for a larger community and that it is defined with realistic goals for a small team (typically two to three) to work on.\nA termly software week. Once a term we have a software week in which members of the CDT present project pitches, their peers review proposals for MVPs (minimum viable products) and discuss the corresponding timelines. There will be joint sprint sessions to push projects forward, technical discussions and presentations to and talks from external partners.\n\nStudents will be mentored by professional research software engineers from UCL’s Advanced Research Computing Centre for their own software journey. The expectation is that all students actively engage in projects and produce software not only as part of their PhD thesis but also contribute to other projects they are interested in.\nSoftware projects will be actively advertised on the CDT homepage, at relevant conferences and showcased once a year at a CDT event for the broader community.\n\n\nExtenuating circumstances\nIf you have extenuating circumstances that require reasonable adjustments for the interview please let us know as soon as possible so that we can plan accordingly. If you are feeling ill on the day of the interview please also let us know straight away so we can work out alternative arrangements with you.\n\n\nA word of encouragement\nAll of us have gone through important interviews and all of us have been nervous and we are still getting nervous ourselves when interviewed for e.g. important grant applications or promotions. All of this is normal and does not work against you. So if you feel overwhelmed in the interview just stop for a moment and take a breather. We all understand this and it is perfectly fine. Should you feel too nervous in the interview we will try to give you break and continue later in the day if possible. Create an environment that helps you relax and always remember, the interview is there to get to know you. You have already managed the hurdle of shortlisting. You have a lot to offer and you can be proud of it."
  }
]