[
  {
    "objectID": "apply.html",
    "href": "apply.html",
    "title": "How to apply",
    "section": "",
    "text": "UCL Japanese Garden\n\n\nThe online application system for 2025 entry is not yet open. Application deadlines and links to the application system will be communicated in the near future. In the meantime please use our sign-up form to receive updates on when our application systems are ready.\n\nApplication Procedure\nDatabase of PhD Projects\n\n\nApplication Procedure\nYou apply by submitting a written application in our online submission system. If shortlisted this is followed by a formal interview. If you pass the interview we will match you with a suitable research project before a formal offer is made. Details can be found here.\n\n\nPhD Projects\nWe offer PhD projects across a broad range of technical areas from UCL and Imperial College. The common thread of these projects is the relevance to modern computational and data sciences. As part of the application procedure you will be matched with a suitable PhD Project and associated supervisor. Some PhD projects may involve external partners and expected internships at the external partner institutions. Such additional requirements are specified within the project descriptions.\n\n\nEligibility and language requirements.\nA UK Master’s degree in a relevant discipline with Merit, or a minimum of an upper second-class UK Bachelor’s degree in a relevant discipline, or an overseas qualification of an equivalent standard. Work experience may also be taken into account. The required English level for this Programme is Level 1.\nRelevant disciplines are broadly STEM subjects (Science, Technology, Engineering, Mathematics). But other related disciplines on the interface to STEM may be suitable too. During the application stage we will assess the suitability of your technical background."
  },
  {
    "objectID": "phd_projects/entries/hetherington1.html",
    "href": "phd_projects/entries/hetherington1.html",
    "title": "Model correlation and the version control tree",
    "section": "",
    "text": "When we consider the behaviour of numerical models of complex systems, it is relatively common practice to treat the predictions of different models of the same phenomena as independent and uncorrelated. This approach was, for example, taken during the Covid-19 pandemic to uncertainty quantification for the national R number, based on the outputs of an ensemble of models taken from a variety of UK universities. Yet this approach misses a key element: models form a tree of inheritance, similar to that of a phylogenetic tree in biology. Models share traits with their parents. Models borrow from each other. Models developed in physical proximity interbreed - the labs and companies they are developed in. This interrelatedness is hard to characterise and control - a key challenge for reliable UQ when using ensembles of models. We propose initially to use the version control tree implied by Github, as a proxy for this, and look at model output correlation properties conditioned on graph distance in the version control tree."
  },
  {
    "objectID": "phd_projects/entries/hetherington1.html#project-description",
    "href": "phd_projects/entries/hetherington1.html#project-description",
    "title": "Model correlation and the version control tree",
    "section": "",
    "text": "When we consider the behaviour of numerical models of complex systems, it is relatively common practice to treat the predictions of different models of the same phenomena as independent and uncorrelated. This approach was, for example, taken during the Covid-19 pandemic to uncertainty quantification for the national R number, based on the outputs of an ensemble of models taken from a variety of UK universities. Yet this approach misses a key element: models form a tree of inheritance, similar to that of a phylogenetic tree in biology. Models share traits with their parents. Models borrow from each other. Models developed in physical proximity interbreed - the labs and companies they are developed in. This interrelatedness is hard to characterise and control - a key challenge for reliable UQ when using ensembles of models. We propose initially to use the version control tree implied by Github, as a proxy for this, and look at model output correlation properties conditioned on graph distance in the version control tree."
  },
  {
    "objectID": "phd_projects/entries/Kovalchuk_healthcare.html",
    "href": "phd_projects/entries/Kovalchuk_healthcare.html",
    "title": "Drift detection in graph streams and its applications in healthcare",
    "section": "",
    "text": "Graphs have become a useful tool for representing information in many application domains. Social, computer, sensor and transport networks; molecular structures and business processes – all can be represented as attributed graphs. One of the characteristics of such graphs is dynamism – the graph structure, as well as the attributes of nodes and edges can change over time. The accuracy of predictive and inference models built over dynamic graphs depends on the ability of the models to adapt to these changes. This project will propose novel methods for detecting changes in graphs over time (also known as drifts) and demonstrate their usefulness in downstream machine learning and process mining tasks performed over dynamic graphs. The work will build upon the methods recently proposed by the principal supervisor based on graphs and deep learning for process mining https://doi.org/10.1109/ACCESS.2020.3025999 and drift detection in business processes https://doi.org/10.3390/e24070910. The PhD student will take this previous work as a basis to both advance the theoretical approach to drift detection in graph streams and demonstrate its generalizability by applying to a new domain, namely, discovering disease trajectories. Disease trajectories represented as graphs can help predict disease progression, risk of developing comorbid disorders and patient outcomes more accurately Kusuma et al. Existing solutions for discovering disease trajectories are based on statistical analysis https://doi.org/10.1038/ncomms5022 and knowledge graphs https://doi.org/10.1186/s13326-020-00228-8, thus computationally expensive and not scalable. Furthermore, these solutions are not capable of adapting to changes over time (e.g. changes in disease progression due to events such as the coronavirus pandemic or introducing a new drug/vaccination). Finally, there is currently no solution exists based on the UK population data. The methods built in this project will be applied to Hospital Episode Statistics (HES) data, thus revealing the healthcare picture of the entire UK population.\n\n\n\n\nDevelop novel methods for drift detection in graph streams to outperform the state-of-the-art methods according to at least one metric: accuracy, scalability, computational time.\nDemonstrate the generalizability of the developed methods by applying them to several domains such as detecting drifts in business processes and disease trajectories, aiming to outperform the state-of-the-art methods in respective areas according to at least one metric: accuracy, scalability, computational time.\nDevelop software allowing non-domain experts query process graphs to answer a range of research/business questions.\n\n\n\n\nThis project will deliver a disease trajectory browser, similar to the one proposed in https://doi.org/10.1038/s41467-020-18682-4 but based on more advanced process discovery and drift detection in graph streams methods developed as part of this project and leveraging UK population data. The proposed process discovery and drift detection methods will be developed in Python. The discovered graphs will be sorted in a graph database such as Neo4j. The frontend will be built using a range of JavaScript libraries, including Cytoscape.js and Dagre.js to represent graphs. The browser will be released as an open-source software under the MIT license."
  },
  {
    "objectID": "phd_projects/entries/Kovalchuk_healthcare.html#project-description",
    "href": "phd_projects/entries/Kovalchuk_healthcare.html#project-description",
    "title": "Drift detection in graph streams and its applications in healthcare",
    "section": "",
    "text": "Graphs have become a useful tool for representing information in many application domains. Social, computer, sensor and transport networks; molecular structures and business processes – all can be represented as attributed graphs. One of the characteristics of such graphs is dynamism – the graph structure, as well as the attributes of nodes and edges can change over time. The accuracy of predictive and inference models built over dynamic graphs depends on the ability of the models to adapt to these changes. This project will propose novel methods for detecting changes in graphs over time (also known as drifts) and demonstrate their usefulness in downstream machine learning and process mining tasks performed over dynamic graphs. The work will build upon the methods recently proposed by the principal supervisor based on graphs and deep learning for process mining https://doi.org/10.1109/ACCESS.2020.3025999 and drift detection in business processes https://doi.org/10.3390/e24070910. The PhD student will take this previous work as a basis to both advance the theoretical approach to drift detection in graph streams and demonstrate its generalizability by applying to a new domain, namely, discovering disease trajectories. Disease trajectories represented as graphs can help predict disease progression, risk of developing comorbid disorders and patient outcomes more accurately Kusuma et al. Existing solutions for discovering disease trajectories are based on statistical analysis https://doi.org/10.1038/ncomms5022 and knowledge graphs https://doi.org/10.1186/s13326-020-00228-8, thus computationally expensive and not scalable. Furthermore, these solutions are not capable of adapting to changes over time (e.g. changes in disease progression due to events such as the coronavirus pandemic or introducing a new drug/vaccination). Finally, there is currently no solution exists based on the UK population data. The methods built in this project will be applied to Hospital Episode Statistics (HES) data, thus revealing the healthcare picture of the entire UK population.\n\n\n\n\nDevelop novel methods for drift detection in graph streams to outperform the state-of-the-art methods according to at least one metric: accuracy, scalability, computational time.\nDemonstrate the generalizability of the developed methods by applying them to several domains such as detecting drifts in business processes and disease trajectories, aiming to outperform the state-of-the-art methods in respective areas according to at least one metric: accuracy, scalability, computational time.\nDevelop software allowing non-domain experts query process graphs to answer a range of research/business questions.\n\n\n\n\nThis project will deliver a disease trajectory browser, similar to the one proposed in https://doi.org/10.1038/s41467-020-18682-4 but based on more advanced process discovery and drift detection in graph streams methods developed as part of this project and leveraging UK population data. The proposed process discovery and drift detection methods will be developed in Python. The discovered graphs will be sorted in a graph database such as Neo4j. The frontend will be built using a range of JavaScript libraries, including Cytoscape.js and Dagre.js to represent graphs. The browser will be released as an open-source software under the MIT license."
  },
  {
    "objectID": "phd_projects/entries/burman_inverse.html",
    "href": "phd_projects/entries/burman_inverse.html",
    "title": "Computational solution of inverse problems using large datasets of low rank",
    "section": "",
    "text": "A major challenge in computational science is how to use information from large datasets to improve computations of the large scale severly ill-posed problems that appear in inverse problems and data assimilation. Here we will explore how large datasets characterising different aspects of the solution can be used to improve approximation. The approach is to first use machine learning techniques to find a finite dimensional manifold characterising the dataset. Then we can apply recent stability results for inverse problems with solution in a finite dimensional space, and design finite element methods for which we prove rigorous error estimates up to perturbations of the data and the accuracy of the approximate manifold. Finally, we train neural networks to map from the manifold into the finite element solution space to obtain a reduced order model that can be used for the efficient iterative solution of the inverse problem. The project will give rise to three pieces of software that can be applied together or individually."
  },
  {
    "objectID": "phd_projects/entries/burman_inverse.html#project-description",
    "href": "phd_projects/entries/burman_inverse.html#project-description",
    "title": "Computational solution of inverse problems using large datasets of low rank",
    "section": "",
    "text": "A major challenge in computational science is how to use information from large datasets to improve computations of the large scale severly ill-posed problems that appear in inverse problems and data assimilation. Here we will explore how large datasets characterising different aspects of the solution can be used to improve approximation. The approach is to first use machine learning techniques to find a finite dimensional manifold characterising the dataset. Then we can apply recent stability results for inverse problems with solution in a finite dimensional space, and design finite element methods for which we prove rigorous error estimates up to perturbations of the data and the accuracy of the approximate manifold. Finally, we train neural networks to map from the manifold into the finite element solution space to obtain a reduced order model that can be used for the efficient iterative solution of the inverse problem. The project will give rise to three pieces of software that can be applied together or individually."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "",
    "text": "Partial differential equations (PDEs) in non-divergence form are fundamental in a wide range of scientific and technological fields, playing a pivotal role in optimal control and game theory through the Hamilton-Jacobi-Bellman and Isaacs PDEs. Despite recent advancements in numerical methods for these PDEs (see the unified a priori analysis in [1]), their application remains confined to specialists. This limitation arises because there are no widely accessible software packages that offer the user-friendliness and computational power of state-of-the-art libraries for divergence-form equations, such as Fenics or Firedrake.\nFenics and Firedrake have succeeded primarily due to the development of the Unified Form Language, which allows users to define a wide range of PDEs in divergence form succinctly and in a manner that aligns with their mathematical structure. This project seeks to extend the Unified Form Language to accommodate PDEs in non-divergence form and implement this extension in Firedrake.\nA proof of concept has already been demonstrated in [2], where numerical experiments for the fully nonlinear Hamilton-Jacobi-Bellman/Monge-Ampère equation were conducted using integral components of the Fenics library in a non-divergence form setting (noting that Firedrake and Fenics share the same Unified Form Language)."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html#project-description",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html#project-description",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "",
    "text": "Partial differential equations (PDEs) in non-divergence form are fundamental in a wide range of scientific and technological fields, playing a pivotal role in optimal control and game theory through the Hamilton-Jacobi-Bellman and Isaacs PDEs. Despite recent advancements in numerical methods for these PDEs (see the unified a priori analysis in [1]), their application remains confined to specialists. This limitation arises because there are no widely accessible software packages that offer the user-friendliness and computational power of state-of-the-art libraries for divergence-form equations, such as Fenics or Firedrake.\nFenics and Firedrake have succeeded primarily due to the development of the Unified Form Language, which allows users to define a wide range of PDEs in divergence form succinctly and in a manner that aligns with their mathematical structure. This project seeks to extend the Unified Form Language to accommodate PDEs in non-divergence form and implement this extension in Firedrake.\nA proof of concept has already been demonstrated in [2], where numerical experiments for the fully nonlinear Hamilton-Jacobi-Bellman/Monge-Ampère equation were conducted using integral components of the Fenics library in a non-divergence form setting (noting that Firedrake and Fenics share the same Unified Form Language)."
  },
  {
    "objectID": "phd_projects/entries/Jensen_unifiedformlanguage.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Jensen_unifiedformlanguage.html#details-of-softwaredata-deliverables",
    "title": "A Unified Form Language for PDEs in non-divergence form",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe project will develop and implement an extension of the Unified Form Language to non-divergence form equations within Firedrake, focusing on semi-Lagrangian methods and incorporating mixed boundary conditions [3]. The implementation will leverage existing Firedrake functionality to solve systems of equations that combine terms in non-divergence and divergence forms. An example of such a system is mean-field games. The project license will likely be a GNU Lesser General Public License, with Python as the primary programming language. Initially, the target platforms are Ubuntu and macOS (via Homebrew).\n\n[1] Debrabant, K. & Jakobsen, E. R. Semi-Lagrangian schemes for linear and fully nonlinear diffusion equations. Mathematics of Computation 82, 1433-1462 (2012).\n[2] Feng, X. & Jensen, M. Convergent Semi-Lagrangian Methods for the Monge–Ampère Equation on Unstructured Grids. SIAM Journal on Numerical Analysis 55, 691-712 (2017).\n[3] Jaroszkowski, B. & Jensen, M. Finite element approximation of Hamilton–Jacobi–Bellman equations with nonlinear mixed boundary conditions. IMA Journal of Numerical Analysis (2023)."
  },
  {
    "objectID": "phd_projects/entries/Hewett_fractaldomains.html",
    "href": "phd_projects/entries/Hewett_fractaldomains.html",
    "title": "Numerical methods for PDEs in fractal domains",
    "section": "",
    "text": "PDEs are a fundamental tool in the mathematical modelling of physical processes, and obtaining accurate approximate numerical solutions to PDE problems is a key area within scientific computing. Many numerical methods are available, but most are designed for the case where the PDE is to be solved in a domain with a smooth boundary (typically, Lipschitz or smoother). However, many real-world problems involve domains with non-smooth boundaries, which may possess detail on multiple length scales. Fractals provide a mathematical model for such boundaries, and in this project the student will develop numerical methods for solving PDEs in domains with fractal boundaries. This is an exciting and emerging field, with lots of challenging open mathematical and computational questions to investigate.\nThe principal supervisor’s group have already made significant progress in the study of acoustic scattering problems involving fractal scatterers, via integral equation methods. They have investigated two main approaches: (i) conventional discretizations on a smoother “pre-fractal” approximation to the domain, and (ii) novel discretizations on the true fractal domain. Computationally, for approach (i) we used the open-source Bempp software of Timo Betcke on polygonal or polyhedral prefractal approximations, and for approach (ii) we developed our own open-source integral equations package called IFSIntegrals, implementing the novel numerical quadrature rules developed recently within the group for singular integration over fractal domains. So far our investigations have been limited to acoustic simulations, and mostly to problems with Dirichlet boundary conditions, but the results are very promising both theoretically and computationally.\n\n\n\nThe objectives of the project could include: - extending the applicability of our integral equation methods to new PDEs (e.g. Maxwell equations) and/or new boundary conditions (e.g. Neumann, impedance) - the development of novel discontinuous Galerkin (DG) methods on meshes with fractal elements - the development of novel quadrature rules involving singular and/or oscillatory integrands over fractal domains - the application of the developed methods to shape optimization problems relevant to the design of fractal wave absorbers/reflectors/ transmitters.\n\n\n\nThe software deliverable would be an implementation of the numerical methods developed within the project. Depending on the nature of the methods developed, this might take the form of new modules within the Bempp or IFSIntegrals packages, which are written in Python/Rust and Julia respectively, or a standalone open-source code written in a language to be determined by the project team. The software would provide the numerical PDE community with a valuable tool with which to simulate PDEs in domains with fractal boundaries, which currently does not exist, opening up the prospect of simulating new and important problems in acoustic and electromagnetic scattering, metamaterial design and imaging technologies."
  },
  {
    "objectID": "phd_projects/entries/Hewett_fractaldomains.html#project-description",
    "href": "phd_projects/entries/Hewett_fractaldomains.html#project-description",
    "title": "Numerical methods for PDEs in fractal domains",
    "section": "",
    "text": "PDEs are a fundamental tool in the mathematical modelling of physical processes, and obtaining accurate approximate numerical solutions to PDE problems is a key area within scientific computing. Many numerical methods are available, but most are designed for the case where the PDE is to be solved in a domain with a smooth boundary (typically, Lipschitz or smoother). However, many real-world problems involve domains with non-smooth boundaries, which may possess detail on multiple length scales. Fractals provide a mathematical model for such boundaries, and in this project the student will develop numerical methods for solving PDEs in domains with fractal boundaries. This is an exciting and emerging field, with lots of challenging open mathematical and computational questions to investigate.\nThe principal supervisor’s group have already made significant progress in the study of acoustic scattering problems involving fractal scatterers, via integral equation methods. They have investigated two main approaches: (i) conventional discretizations on a smoother “pre-fractal” approximation to the domain, and (ii) novel discretizations on the true fractal domain. Computationally, for approach (i) we used the open-source Bempp software of Timo Betcke on polygonal or polyhedral prefractal approximations, and for approach (ii) we developed our own open-source integral equations package called IFSIntegrals, implementing the novel numerical quadrature rules developed recently within the group for singular integration over fractal domains. So far our investigations have been limited to acoustic simulations, and mostly to problems with Dirichlet boundary conditions, but the results are very promising both theoretically and computationally.\n\n\n\nThe objectives of the project could include: - extending the applicability of our integral equation methods to new PDEs (e.g. Maxwell equations) and/or new boundary conditions (e.g. Neumann, impedance) - the development of novel discontinuous Galerkin (DG) methods on meshes with fractal elements - the development of novel quadrature rules involving singular and/or oscillatory integrands over fractal domains - the application of the developed methods to shape optimization problems relevant to the design of fractal wave absorbers/reflectors/ transmitters.\n\n\n\nThe software deliverable would be an implementation of the numerical methods developed within the project. Depending on the nature of the methods developed, this might take the form of new modules within the Bempp or IFSIntegrals packages, which are written in Python/Rust and Julia respectively, or a standalone open-source code written in a language to be determined by the project team. The software would provide the numerical PDE community with a valuable tool with which to simulate PDEs in domains with fractal boundaries, which currently does not exist, opening up the prospect of simulating new and important problems in acoustic and electromagnetic scattering, metamaterial design and imaging technologies."
  },
  {
    "objectID": "phd_projects/entries/Bravi_cancerimmune.html",
    "href": "phd_projects/entries/Bravi_cancerimmune.html",
    "title": "A machine learning-informed computational model of cancer-immune interactions",
    "section": "",
    "text": "The immune system is a major constraint on cancer evolution and an important therapeutic target. However, the molecular determinants and the dynamics of immune response in cancer remain poorly understood. The goal of this project is to develop efficient computational simulation frameworks and machine learning methods to characterise and predict quantitatively immune responses in cancer, focussing on colorectal cancer data. The combination of computational modelling and machine learning predictions will allow us to uncover many characteristics of tumour-immune coevolution, which is key to the design of anti-cancer immunotherapies.\n\n\nCells of the immune system called T cells can recognize and kill cancer cells. Such recognition occurs via a specific binding between the receptors of T cells and protein fragments carrying cancer-specific mutations exposed on the cancer cells’ surface (called neoantigens). In this project, we will develop a data-driven computational model of cancer evolution under the effect of the response by T cells, which will provide evidence to test different hypotheses on cancer-immune co-evolution and will be useful to suggest molecular targets for immunotherapy design. The work will involve two complementary tasks:\n\nTo design a mathematical model describing the stochastic dynamics of co-evolution of cancer and T cells which is driven by the molecular processes of cancer recognition by T cells (i.e., the binding between cancer neoantigens and T cell receptors), and set up a simulation framework for this model. The model will be calibrated on data that sample longitudinally T cell receptors and putative neoantigens from primary cancer to metastasis in colorectal cancers from several patients.\nTo develop a machine learning method to detect and predict quantitatively the molecular interaction between receptors and neoantigens driving the cancer-immune co-evolutionary dynamics. Such a method will build upon recent advances in generative language models for proteins and techniques of transfer learning, and will be trained on publicly available datasets on T cell response to neoantigens. Evaluating the predictions of this method on the data from colorectal cancer samples will allow us to calibrate the receptor-neoantigen binding term in the computational model, hence to incorporate in an innovative way all the available information on the molecular species involved in the cancer-immune joint dynamics.\n\n\n\n\nThe software/code development in this project consists of two parts:\n\nSoftware for a machine learning model that predicts probabilistic scores of binding between the proteins involved in cancer-immune co-evolution (cancer neoantigens and T-cell receptors). This model will build upon the recent advances in generative language models and their application to protein modelling (e.g. by adopting a transformer architecture, see Meynard-Piganeau et al., biorxiv.org/content/10.1101/2023.07.19.549669, 2023). It will implement strategies of transfer learning on top of pre-trained models to capture the specificity of neoantigen-receptor binding (as a special class of protein-protein interactions).\nSoftware for the computational model of the co-evolution of cancer and immune cells. It will be based on Gillespie simulations of its stochastic dynamics and will include routines for the statistical inference of the model’s parameters from longitudinal data.\n\n\n\n\nOn mathematical modelling: Lakatos, …, and Graham. Evolutionary dynamics of neoantigens in growing tumors, Nature Genetics (2020). Almeida et al. Discrete and continuum models for the coevolutionary dynamics between CD8+ cytotoxic T lymphocytes and tumour cells, Mathematical Medicine and Biology: A Journal of the IMA (2023). On machine learning methods: Bravi et al. A transfer-learning approach to predict antigen immunogenicity and T-cell receptor specificity, eLife (2023). Meynard-Piganeau et al. TULIP - a transformer based unsupervised language model for interacting peptides and T-cell receptors that generalizes to unseen epitopes, bioRxiv https://doi.org/10.1101/2023.07.19.549669 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Bravi_cancerimmune.html#project-description",
    "href": "phd_projects/entries/Bravi_cancerimmune.html#project-description",
    "title": "A machine learning-informed computational model of cancer-immune interactions",
    "section": "",
    "text": "The immune system is a major constraint on cancer evolution and an important therapeutic target. However, the molecular determinants and the dynamics of immune response in cancer remain poorly understood. The goal of this project is to develop efficient computational simulation frameworks and machine learning methods to characterise and predict quantitatively immune responses in cancer, focussing on colorectal cancer data. The combination of computational modelling and machine learning predictions will allow us to uncover many characteristics of tumour-immune coevolution, which is key to the design of anti-cancer immunotherapies.\n\n\nCells of the immune system called T cells can recognize and kill cancer cells. Such recognition occurs via a specific binding between the receptors of T cells and protein fragments carrying cancer-specific mutations exposed on the cancer cells’ surface (called neoantigens). In this project, we will develop a data-driven computational model of cancer evolution under the effect of the response by T cells, which will provide evidence to test different hypotheses on cancer-immune co-evolution and will be useful to suggest molecular targets for immunotherapy design. The work will involve two complementary tasks:\n\nTo design a mathematical model describing the stochastic dynamics of co-evolution of cancer and T cells which is driven by the molecular processes of cancer recognition by T cells (i.e., the binding between cancer neoantigens and T cell receptors), and set up a simulation framework for this model. The model will be calibrated on data that sample longitudinally T cell receptors and putative neoantigens from primary cancer to metastasis in colorectal cancers from several patients.\nTo develop a machine learning method to detect and predict quantitatively the molecular interaction between receptors and neoantigens driving the cancer-immune co-evolutionary dynamics. Such a method will build upon recent advances in generative language models for proteins and techniques of transfer learning, and will be trained on publicly available datasets on T cell response to neoantigens. Evaluating the predictions of this method on the data from colorectal cancer samples will allow us to calibrate the receptor-neoantigen binding term in the computational model, hence to incorporate in an innovative way all the available information on the molecular species involved in the cancer-immune joint dynamics.\n\n\n\n\nThe software/code development in this project consists of two parts:\n\nSoftware for a machine learning model that predicts probabilistic scores of binding between the proteins involved in cancer-immune co-evolution (cancer neoantigens and T-cell receptors). This model will build upon the recent advances in generative language models and their application to protein modelling (e.g. by adopting a transformer architecture, see Meynard-Piganeau et al., biorxiv.org/content/10.1101/2023.07.19.549669, 2023). It will implement strategies of transfer learning on top of pre-trained models to capture the specificity of neoantigen-receptor binding (as a special class of protein-protein interactions).\nSoftware for the computational model of the co-evolution of cancer and immune cells. It will be based on Gillespie simulations of its stochastic dynamics and will include routines for the statistical inference of the model’s parameters from longitudinal data.\n\n\n\n\nOn mathematical modelling: Lakatos, …, and Graham. Evolutionary dynamics of neoantigens in growing tumors, Nature Genetics (2020). Almeida et al. Discrete and continuum models for the coevolutionary dynamics between CD8+ cytotoxic T lymphocytes and tumour cells, Mathematical Medicine and Biology: A Journal of the IMA (2023). On machine learning methods: Bravi et al. A transfer-learning approach to predict antigen immunogenicity and T-cell receptor specificity, eLife (2023). Meynard-Piganeau et al. TULIP - a transformer based unsupervised language model for interacting peptides and T-cell receptors that generalizes to unseen epitopes, bioRxiv https://doi.org/10.1101/2023.07.19.549669 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Burman_heterogeneouscoupling.html",
    "href": "phd_projects/entries/Burman_heterogeneouscoupling.html",
    "title": "A computational framework for heterogeneous coupling in large scale computations",
    "section": "",
    "text": "With the advent of high performance computing and new computer architectures there is renewed interest in the efficient coupling of different partial differential equation based models and their associated solvers. Indeed, it is often the case that for a given application or computational methods there exists a highly optimized solver. However in complex applications many different models are connected and it is then attractive to connect such optimized solvers for the subproblem to obtain a solution of the global problem. The common approach in this framework is to discretize the global continuous problem into a, possibly huge, algebraic system, and then split the latter as a system of coupled algebraic subsystems. In this case the coupling condition often consists in the identification of unknowns in the subsystems corresponding to the same global unknown. This approach forms the basis of the dominating coupling softwares available today (see for example [3,4]).\nAn alternative approach, which is lately gaining increasing interest, is, instead, to decompose the original problem already at the continuous/infinite dimensional level, thus obtaining a system of coupled infinite dimensional subproblems, to be successively discretized. The advantage of the latter approach compared to the former is that it allows for a detailed mathematical analysis of the resulting couplings under precise conditions on the local solvers. Moreover, it allows for the design of optimal preconditioners of the coupling method. An abstract framework for heterogeneous coupling including stability and error analysis as well as the design of preconditioners was recently proposed by the principal supervisor in [1]. It is shown that a variety of different couplings enter the framework including interface coupling, volume coupling, FEM-BEM coupling, bulk-surface pde systems, or networks of fractures. This theoretical work was inspired by the principal supervisors extensive work on coupling methods for heterogeneous interface problems [3], fluid-structure or fluid-fluid interaction [4,5], or FEM- BEM coupling [2,6].\n\n\n\nDescribe the main objectives of the project.\nThe main objective of the project is to further develop the ideas of [1] along the following lines:\n\nTheory. The examples of [1] are restricted to scalar problems. Here the objective of the thesis would be to show how the framework can be applied to systems, with special focus on the linearized Navier-Stokes’ equations of fluid mechanics and the Maxwell’s equations of electro magnetics. For these cases a complete analysis will be derived including stability and error analysis. Preconditioners for the coupling will also be designed. If time allows more heterogeneous systems will then be considered, such as for example free flow coupled to fractured porous media or magneto-hydrodynamics. Another more adventurous potential research strand is to explore using network approximation to learn optimal coupling spaces satisfying the conditions of [1].\nComputational aspects. The framework will then be realised in a computational software where the merits of different coupling conditions or formulations can be assessed. Here special care will be taken to make the computational software agnostic (to the furthest possible extent) to the solvers of the subproblems in the spirit of [1].\n\n\n\n\nThe software deliverable should form a significant component of the PhD project. It must be a substantial piece of software that can disseminated to the wider community. For the details of the software deliverable please describe existing software (if available), community need for the software, licensing and technical aspects (programming language, target platforms, etc.). The license by default should be an open-source license. For other types of licenses please discuss with the CDT leadership team.\nThe objective is to design and implement a software library allowing for heterogeneous coupling of different partial differential equations and their associated methods. This library would allow for interface coupling, volume coupling, mixed dimensional coupling. Assuming that the different solvers of the subproblems are optimized the library will handle the preconditioning of the coupling using the general FETI type preconditioner designed and analysed in [1]. Contrary to the libraries of [7,8] this library would have a solid mathematical foundation that will allow for a highly optimized and reliable computational software.\n[1] Bertoluzza, Silvia ; Burman, Erik. An abstract framework for heterogeneous coupling: stability, approximation and applications. arXiv:2312.11733, 2023. [2] Betcke, Timo ; Bosy, Michał ; Burman, Erik . Hybrid coupling of finite element and boundary element methods using Nitsche’s method and the Calderon projection. Numer. Algorithms 91 (2022), no. 3, 997–1019. [3] Burman, Erik ; Elfverson, Daniel ; Hansbo, Peter ; Larson, Mats G. ; Larsson, Karl . Hybridized CutFEM for elliptic interface problems. SIAM J. Sci. Comput. 41 (2019), no. 5, A3354–A3380. [4] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel A. ; Guzmán, Johnny . Fully discrete loosely coupled Robin-Robin scheme for incompressible fluid-structure interaction: stability and error analysis. Numer. Math. 151 (2022), no. 4, 807–840. [5] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel ; Guzmán, Johnny . Loosely coupled, non- iterative time-splitting scheme based on Robin-Robin coupling: unified analysis for parabolic/parabolic and parabolic/hyperbolic problems. J. Numer. Math. 31 (2023), no. 1, 59–77. [6] Bosy, Michal; Scroggs, Matthew W.; Betcke, Timo; Burman, Erik ; Cooper, Christopher D. Coupling finite and boundary element methods to solve the Poisson–Boltzmann equation for electrostatics in molecular solvation. Journal of Computational Chemistry. https://doi.org/10.1002/jcc.27262, 2023. [7] Bungartz, Hans-Joachim ; Lindner, Florian ; Gatzhammer, Bernhard ; Mehl, Miriam ; Scheufele, Klaudius ; Shukaev, Alexander ; Uekermann, Benjamin . preCICE—a fully parallel library for multi- physics surface coupling. Comput. & Fluids 141 (2016), 250–258. [8] Tang, Yu-Hang ; Kudo, Shuhei ; Bian, Xin ; Li, Zhen ; Karniadakis, George Em . Multiscale universal interface: a concurrent framework for coupling heterogeneous solvers. J. Comput. Phys. 297 (2015), 13–31."
  },
  {
    "objectID": "phd_projects/entries/Burman_heterogeneouscoupling.html#project-description",
    "href": "phd_projects/entries/Burman_heterogeneouscoupling.html#project-description",
    "title": "A computational framework for heterogeneous coupling in large scale computations",
    "section": "",
    "text": "With the advent of high performance computing and new computer architectures there is renewed interest in the efficient coupling of different partial differential equation based models and their associated solvers. Indeed, it is often the case that for a given application or computational methods there exists a highly optimized solver. However in complex applications many different models are connected and it is then attractive to connect such optimized solvers for the subproblem to obtain a solution of the global problem. The common approach in this framework is to discretize the global continuous problem into a, possibly huge, algebraic system, and then split the latter as a system of coupled algebraic subsystems. In this case the coupling condition often consists in the identification of unknowns in the subsystems corresponding to the same global unknown. This approach forms the basis of the dominating coupling softwares available today (see for example [3,4]).\nAn alternative approach, which is lately gaining increasing interest, is, instead, to decompose the original problem already at the continuous/infinite dimensional level, thus obtaining a system of coupled infinite dimensional subproblems, to be successively discretized. The advantage of the latter approach compared to the former is that it allows for a detailed mathematical analysis of the resulting couplings under precise conditions on the local solvers. Moreover, it allows for the design of optimal preconditioners of the coupling method. An abstract framework for heterogeneous coupling including stability and error analysis as well as the design of preconditioners was recently proposed by the principal supervisor in [1]. It is shown that a variety of different couplings enter the framework including interface coupling, volume coupling, FEM-BEM coupling, bulk-surface pde systems, or networks of fractures. This theoretical work was inspired by the principal supervisors extensive work on coupling methods for heterogeneous interface problems [3], fluid-structure or fluid-fluid interaction [4,5], or FEM- BEM coupling [2,6].\n\n\n\nDescribe the main objectives of the project.\nThe main objective of the project is to further develop the ideas of [1] along the following lines:\n\nTheory. The examples of [1] are restricted to scalar problems. Here the objective of the thesis would be to show how the framework can be applied to systems, with special focus on the linearized Navier-Stokes’ equations of fluid mechanics and the Maxwell’s equations of electro magnetics. For these cases a complete analysis will be derived including stability and error analysis. Preconditioners for the coupling will also be designed. If time allows more heterogeneous systems will then be considered, such as for example free flow coupled to fractured porous media or magneto-hydrodynamics. Another more adventurous potential research strand is to explore using network approximation to learn optimal coupling spaces satisfying the conditions of [1].\nComputational aspects. The framework will then be realised in a computational software where the merits of different coupling conditions or formulations can be assessed. Here special care will be taken to make the computational software agnostic (to the furthest possible extent) to the solvers of the subproblems in the spirit of [1].\n\n\n\n\nThe software deliverable should form a significant component of the PhD project. It must be a substantial piece of software that can disseminated to the wider community. For the details of the software deliverable please describe existing software (if available), community need for the software, licensing and technical aspects (programming language, target platforms, etc.). The license by default should be an open-source license. For other types of licenses please discuss with the CDT leadership team.\nThe objective is to design and implement a software library allowing for heterogeneous coupling of different partial differential equations and their associated methods. This library would allow for interface coupling, volume coupling, mixed dimensional coupling. Assuming that the different solvers of the subproblems are optimized the library will handle the preconditioning of the coupling using the general FETI type preconditioner designed and analysed in [1]. Contrary to the libraries of [7,8] this library would have a solid mathematical foundation that will allow for a highly optimized and reliable computational software.\n[1] Bertoluzza, Silvia ; Burman, Erik. An abstract framework for heterogeneous coupling: stability, approximation and applications. arXiv:2312.11733, 2023. [2] Betcke, Timo ; Bosy, Michał ; Burman, Erik . Hybrid coupling of finite element and boundary element methods using Nitsche’s method and the Calderon projection. Numer. Algorithms 91 (2022), no. 3, 997–1019. [3] Burman, Erik ; Elfverson, Daniel ; Hansbo, Peter ; Larson, Mats G. ; Larsson, Karl . Hybridized CutFEM for elliptic interface problems. SIAM J. Sci. Comput. 41 (2019), no. 5, A3354–A3380. [4] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel A. ; Guzmán, Johnny . Fully discrete loosely coupled Robin-Robin scheme for incompressible fluid-structure interaction: stability and error analysis. Numer. Math. 151 (2022), no. 4, 807–840. [5] Burman, Erik ; Durst, Rebecca ; Fernández, Miguel ; Guzmán, Johnny . Loosely coupled, non- iterative time-splitting scheme based on Robin-Robin coupling: unified analysis for parabolic/parabolic and parabolic/hyperbolic problems. J. Numer. Math. 31 (2023), no. 1, 59–77. [6] Bosy, Michal; Scroggs, Matthew W.; Betcke, Timo; Burman, Erik ; Cooper, Christopher D. Coupling finite and boundary element methods to solve the Poisson–Boltzmann equation for electrostatics in molecular solvation. Journal of Computational Chemistry. https://doi.org/10.1002/jcc.27262, 2023. [7] Bungartz, Hans-Joachim ; Lindner, Florian ; Gatzhammer, Bernhard ; Mehl, Miriam ; Scheufele, Klaudius ; Shukaev, Alexander ; Uekermann, Benjamin . preCICE—a fully parallel library for multi- physics surface coupling. Comput. & Fluids 141 (2016), 250–258. [8] Tang, Yu-Hang ; Kudo, Shuhei ; Bian, Xin ; Li, Zhen ; Karniadakis, George Em . Multiscale universal interface: a concurrent framework for coupling heterogeneous solvers. J. Comput. Phys. 297 (2015), 13–31."
  },
  {
    "objectID": "phd_projects/entries/zhang_reinforcement_learning.html",
    "href": "phd_projects/entries/zhang_reinforcement_learning.html",
    "title": "Optimization methods for high-dimensional reinforcement learning",
    "section": "",
    "text": "This PhD project aims to develop efficient zero-order optimization methods for high-dimensional reinforcement learning problems. Reinforcement learning, which is crucial for decision-making tasks in fields such as robotics, finance, and artificial intelligence, often requires solving high-dimensional optimization problems where gradients are noisy or infeasible to compute. Zero-order methods, which rely solely on function evaluations rather than gradient information, are well-suited for these black-box optimization scenarios. However, existing zero-order optimization methods suffer from the curse of dimensionality. This project seeks to leverage recent advancements in high-dimensional statistics, probability, and optimization theory to develop scalable reinforcement learning methods for high-dimensional problems.\nAlthough various reinforcement learning and zero-order optimization algorithms have been proposed in the literature, they do not explicitly exploit the intrinsic low-dimensional structures that naturally arise in many large-scale decision-making problems. As a result, existing reinforcement learning algorithms struggle to scale effectively in high-dimensional settings, leading to inefficiencies. The development of efficient RL algorithms that leverage low-dimensional structures for high-dimensional problems, along with a rigorous performance analysis, remains largely unexplored in the current literature.\nThe principal supervisor’s group has focused on developing provably convergent reinforcement learning algorithms, with an emphasis on rigorous theoretical foundations to ensure reliable performance. These works have primarily been applied to low-dimensional settings. For high-dimensional problems, the wider research community has explored complex function approximation techniques, such as neural networks, to address the curse of dimensionality. Despite their promise, these approaches often lack strong theoretical guarantees, particularly in terms of convergence and robustness, which limits their applicability in high-stakes large-scale decision-making problems.\n\n\nThe main objective of this project is to leverage recent advances in high-dimensional statistics and probability theory to learn the intricate low-dimensional structures and develop more efficient, theoretically grounded algorithms for high-dimensional reinforcement learning problems. Specifically, the project aims to:\n• Develop novel zero-order optimization methods that exploit the intrinsic low-dimensional structure of high-dimensional decision-making problems. • Design reinforcement learning algorithms that can scale efficiently in high-dimensional environments while providing provable theoretical guarantees for convergence and performance. • Investigate the use of high-dimensional statistical techniques and mean-field approximation to improve the scalability and robustness of these algorithms. • Conduct a comprehensive performance analysis of the developed algorithms, comparing them with existing approaches in the literature, particularly in terms of scalability, convergence, and computational efficiency.\n\n\n\n\nZero-Order Optimization Library: A software package implementing novel zero-order optimization methods, designed to efficiently scale in high-dimensional decision-making problems.\nReinforcement Learning Algorithms: A suite of scalable RL algorithms that exploit the intrinsic low-dimensional structure of high-dimensional problems and leverage advanced statistical techniques, with provable convergence guarantees.\nAlgorithm Implementation: Development of zero-order optimization and RL algorithms in Python, focusing on scalability for high-dimensional problems.\nOptimization Library: Creation of a flexible, scalable zero-order optimization library that integrates easily into existing machine learning frameworks.\nData Simulations: Generation of synthetic and real-world data (e.g., finance, robotics) for testing and validating the algorithms.\nPerformance Benchmarking: Development of tools to benchmark algorithm performance, focusing on scalability, efficiency, and convergence rates.\nOpen-Source Release: All code will be open-sourced with comprehensive documentation for ease of use and reproducibility by the research community."
  },
  {
    "objectID": "phd_projects/entries/zhang_reinforcement_learning.html#project-description",
    "href": "phd_projects/entries/zhang_reinforcement_learning.html#project-description",
    "title": "Optimization methods for high-dimensional reinforcement learning",
    "section": "",
    "text": "This PhD project aims to develop efficient zero-order optimization methods for high-dimensional reinforcement learning problems. Reinforcement learning, which is crucial for decision-making tasks in fields such as robotics, finance, and artificial intelligence, often requires solving high-dimensional optimization problems where gradients are noisy or infeasible to compute. Zero-order methods, which rely solely on function evaluations rather than gradient information, are well-suited for these black-box optimization scenarios. However, existing zero-order optimization methods suffer from the curse of dimensionality. This project seeks to leverage recent advancements in high-dimensional statistics, probability, and optimization theory to develop scalable reinforcement learning methods for high-dimensional problems.\nAlthough various reinforcement learning and zero-order optimization algorithms have been proposed in the literature, they do not explicitly exploit the intrinsic low-dimensional structures that naturally arise in many large-scale decision-making problems. As a result, existing reinforcement learning algorithms struggle to scale effectively in high-dimensional settings, leading to inefficiencies. The development of efficient RL algorithms that leverage low-dimensional structures for high-dimensional problems, along with a rigorous performance analysis, remains largely unexplored in the current literature.\nThe principal supervisor’s group has focused on developing provably convergent reinforcement learning algorithms, with an emphasis on rigorous theoretical foundations to ensure reliable performance. These works have primarily been applied to low-dimensional settings. For high-dimensional problems, the wider research community has explored complex function approximation techniques, such as neural networks, to address the curse of dimensionality. Despite their promise, these approaches often lack strong theoretical guarantees, particularly in terms of convergence and robustness, which limits their applicability in high-stakes large-scale decision-making problems.\n\n\nThe main objective of this project is to leverage recent advances in high-dimensional statistics and probability theory to learn the intricate low-dimensional structures and develop more efficient, theoretically grounded algorithms for high-dimensional reinforcement learning problems. Specifically, the project aims to:\n• Develop novel zero-order optimization methods that exploit the intrinsic low-dimensional structure of high-dimensional decision-making problems. • Design reinforcement learning algorithms that can scale efficiently in high-dimensional environments while providing provable theoretical guarantees for convergence and performance. • Investigate the use of high-dimensional statistical techniques and mean-field approximation to improve the scalability and robustness of these algorithms. • Conduct a comprehensive performance analysis of the developed algorithms, comparing them with existing approaches in the literature, particularly in terms of scalability, convergence, and computational efficiency.\n\n\n\n\nZero-Order Optimization Library: A software package implementing novel zero-order optimization methods, designed to efficiently scale in high-dimensional decision-making problems.\nReinforcement Learning Algorithms: A suite of scalable RL algorithms that exploit the intrinsic low-dimensional structure of high-dimensional problems and leverage advanced statistical techniques, with provable convergence guarantees.\nAlgorithm Implementation: Development of zero-order optimization and RL algorithms in Python, focusing on scalability for high-dimensional problems.\nOptimization Library: Creation of a flexible, scalable zero-order optimization library that integrates easily into existing machine learning frameworks.\nData Simulations: Generation of synthetic and real-world data (e.g., finance, robotics) for testing and validating the algorithms.\nPerformance Benchmarking: Development of tools to benchmark algorithm performance, focusing on scalability, efficiency, and convergence rates.\nOpen-Source Release: All code will be open-sourced with comprehensive documentation for ease of use and reproducibility by the research community."
  },
  {
    "objectID": "phd_projects/entries/salvi_backpropagation.html",
    "href": "phd_projects/entries/salvi_backpropagation.html",
    "title": "Backpropagation through rough differential equations",
    "section": "",
    "text": "In this project, we will build on the work of the two supervisors [https://arxiv.org/abs/2009.08295, https://arxiv.org/abs/2201.07566] to study backpropagation through rough differential equations (RDEs), a rough analysis generalisation of SDEs including driving noises possibly rougher than Brownian motion. We will be particularly interested in designing algebraically reversible solvers for Neural RDEs, with enjoy the advantages of both discretise-then-optimise and optimise-then-discretise methods. A key tool we will use are Butcher series expansion, allowing to express the pathwise solution of an RDE in terms of trees representing certain iterated integrals of the driving noise. This expansion allows to capture symmetries to be imposed on the resulting numerical scheme such as algebraic reversibility.\n\n\nNeural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. The reversible Heun method for SDEs https://arxiv.org/abs/2105.13493, built from the analogous scheme for ODEs https://arxiv.org/abs/2102.04668, is to the best of our knowledge, the only algebraically reversible SDE solver to have been developed.\n\n\n\nDevelop higher order algebraically reversible solvers for RDEs. Study convergence/error and stability analysis of the proposed algorithms. Calibrate the algorithm to real-world data, showcasing its application in financial market simulations.\n\n\n\nThe integration of symbolic computations needed for the Butcher series expansion in Diffrax. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/salvi_backpropagation.html#project-description",
    "href": "phd_projects/entries/salvi_backpropagation.html#project-description",
    "title": "Backpropagation through rough differential equations",
    "section": "",
    "text": "In this project, we will build on the work of the two supervisors [https://arxiv.org/abs/2009.08295, https://arxiv.org/abs/2201.07566] to study backpropagation through rough differential equations (RDEs), a rough analysis generalisation of SDEs including driving noises possibly rougher than Brownian motion. We will be particularly interested in designing algebraically reversible solvers for Neural RDEs, with enjoy the advantages of both discretise-then-optimise and optimise-then-discretise methods. A key tool we will use are Butcher series expansion, allowing to express the pathwise solution of an RDE in terms of trees representing certain iterated integrals of the driving noise. This expansion allows to capture symmetries to be imposed on the resulting numerical scheme such as algebraic reversibility.\n\n\nNeural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. The reversible Heun method for SDEs https://arxiv.org/abs/2105.13493, built from the analogous scheme for ODEs https://arxiv.org/abs/2102.04668, is to the best of our knowledge, the only algebraically reversible SDE solver to have been developed.\n\n\n\nDevelop higher order algebraically reversible solvers for RDEs. Study convergence/error and stability analysis of the proposed algorithms. Calibrate the algorithm to real-world data, showcasing its application in financial market simulations.\n\n\n\nThe integration of symbolic computations needed for the Butcher series expansion in Diffrax. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#existing-background-work",
    "href": "phd_projects/entries/Coveney_HemeLB.html#existing-background-work",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Existing background work",
    "text": "Existing background work\nHemeLB is a highly scalable lattice-Boltzmann code which manifests strong scaling on all CPU and GPU platforms where it is currently deployed. These include, inter alia, Frontier (currently the world’s only exascale computer), Summit, Aurora, LUMI, and Archer2. It is designed to model and simulate personalised blood flow throughout the entire human vasculature, from head to toe. A great deal of software engineering has been invested in getting the code to this performance level, which is almost unrivalled on a global basis. It may well feature as a science driver for the post-exascale world and to ensure that is the case we must continue to support and develop the code base. HemeLB has been developed over more than 15 years within Prof Peter Coveney’s group at the Centre for Computational Science. Its applications are wide-ranging, from smaller scale investigations of blood flow in aneurysms and arteriovenous malformations through to the latest simulations in which HemeLB is coupled to a state of the art model of the human heart (Alya from Barcelona Supercomputing Center). [1,2,3]"
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/Coveney_HemeLB.html#main-objectives-of-the-project",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\nIn this research, we will be involved in globally leading edge developments of the HemeLB code which will be influenced by discussions about the way supercomputer are going to be designed for the post exascale era – that is, a form of genuinely interactive co-design with our collaborators in the DoE Leadership Computing Computing Facilities at Oak Ridge and Argonne National Laboraties and associated computing companies.\nThe applications will continue to grow around modelling and simulating the virtual human. These applications will evolve in terms of optimising and enhancing the resolution of the simulations, rendering and visualising the output in situ on GPUs and providing a computational steering facility which can be used by both scientists and clinicians when running the code, so as to most effectively assess numerous “what if?” scenarios. The overall field of computational biomedicine is currently evolving fast in the direction of human digital twins (HDTs) and we expect the HemeLB software to be integral to these endeavours within research and clinical applications in the near future too. [4, 5]."
  },
  {
    "objectID": "phd_projects/entries/Coveney_HemeLB.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Coveney_HemeLB.html#details-of-softwaredata-deliverables",
    "title": "HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe student will need to become familiar with the substantial existing code base of HemeLB and its various development branches. Through this route, we would like to make available versions of the code that run effectively on Isambard-AI, Dawn and other UK based platforms, as well as on Frontier and Aurora and a number of European machines including LUMI and with our collaborators at Leibniz Rechenzentrum in Munich. Versions of the code which are suitable for inclusion in development of and support for computational steering and visualisation will be developed in collaboration with NVIDIA, particularly in the context of Isambard-AI, which will enter its production phase in the second half of 2024. HDT applications which engage with end-users, particularly clinicians and physiologists, will put a premium on the usability and ease of access of the code from remote machines which may well include clouds.\n\n[1] I. Zacharoudiou, J. W. S. McCullough, P. V. Coveney, “Development and performance of a HemeLB GPU code for human-scale blood flow simulation”, Computer Physics Communications, 282, 108548 (2023) DOI:10.1016/j.cpc.2022.108548\n[2] J. W. S. McCullough, R. A. Richardson, A. Patronis, R. Halver, R. Marshall, M. Ruefenacht, B. J. N. Wylie, T. Odaker, M. Wiedemann, B. Lloyd, E. Neufeld, G. Sutmann, A. Skjellum, D. Kranzlmüller and P. V. Coveney, “Towards blood flow in the virtual human: efficient self-coupling of HemeLB”, J R Soc Interface Focus 11, 20190119 (2020), DOI:10.1098/rsfs.2019.0119\n[3] A. Patronis, R. A. Richardson, S. Schmieschek, B. J. Wylie, R. W. Nash and P. V. Coveney, “Modelling Patient-Specific Magnetic Drug Targeting within the Intracranial Vasculature”, Frontiers of Physiology, 9:331 (2018), DOI: 10.3389/fphys.2018.00331\n[4] C. A. Franco, M. Jones, I. Geudens, M. O. Bernabeu, A. Ragab, A. Lima, R. T. Collins, L. K. Phng, P. V. Coveney, H. Gerhardt, “Dynamic endothelial cell rearrangements drive developmental vessel regression”, PLoS Biology, 13(4), e1002125 (2015), DOI: 10.1371/journal.pbio.1002125\n[5] P. V. Coveney and R. R. Highfield, Virtual You: How Building Your Digital Twin Will Revolutionize Medicine and Change Your Life, Princeton University Press (2023) DOI:10.1515/9780691223407"
  },
  {
    "objectID": "phd_projects/entries/gelat.html",
    "href": "phd_projects/entries/gelat.html",
    "title": "Large-scale high-performance solver for therapeutic ultrasound applications",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney1, as well as osteoid osteoma2. OptimUS featured as part of an international software benchmarking exercise3 for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic institutions worldwide as well as a pedagogical tool for undergraduate and postgraduate students. OptimUS leverages the Bempp kernel developed by Prof Betcke.\nUsing OptimUS to solve large biomedical problems at high frequencies is currently challenging due to the large RAM consumption required, which is of the order of GigaByte to TeraByte. A dedicated high-frequency solver is therefore required to achieve realistic simulations at the operational MHz frequencies relevant to laboratory and clinical biomedical ultrasound applications. A promising kernel-independent Fast Multipole Method (FMM) BEM has recently been developed for the next release of the Bempp library and has been used to solve the Laplace equation. Research into dedicated high-frequency FMM implementation is needed to achieve BEM simulations on a larger scale than other numerical approaches. BEM has a distinct advantage over finite element and finite-difference time domain schemes as it suffers from only minimal numerical dispersion and pollution effects. It is therefore anticipated that the development of a high-performance FMM BEM formulation for Helmholtz kernels will be transformative in the field of biomedical ultrasound.\n\n\n\nThis project aims to develop an efficient FMM BEM formulation for high-frequency Helmholtz kernels. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from MRI/CT images. This formulation will also be used as a basis for treating the physics of weakly nonlinear wave propagation in tissue, which requires solving inhomogeneous Helmholtz equations for higher order harmonics. The performance of this new solver will be benchmarked against existing CPU implementations and will also be tested alongside the spectral element method solver recently developed by Prof Garth Wells’ team at University of Cambridge. It will also be benchmarked against other toolboxes used by the biomedical ultrasound community (e.g. k-Wave)\nThe successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as acoustic tomography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its wider adoption in clinical settings, where it could be used for personalised treatment plans based on anatomical models derived from MRI/CT scans, ultimately improving patient outcomes.\nThe supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art facilities required for the successful delivery of this project. These facilities include high-performance computing workstations as well as the UCL Research Computing Platforms Service.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of new BEM formulations for solving ultrasound waves in piecewise homogeneous and heterogeneous domains. The student will be actively involved in software development using Rust and Python, employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in focused ultrasound treatment platforms used in the clinic.\n\n\n\n\nHaqshenas, S.R., Gélat, P., van’t Wout, E., Betcke, T. and Saffari, N., 2021. A fast full-wave solver for calculating ultrasound propagation in the body. Ultrasonics, 110, p.106240.\nvan’t Wout, E., Haqshenas, S.R., Gélat, P., Betcke, T. and Saffari, N., 2022. Frequency-robust preconditioning of boundary integral equations for acoustic transmission. Journal of Computational Physics, 462, p.111229.\nAubry, J.F., Bates, O., Boehm, C., Butts Pauly, K., Christensen, D., Cueto, C., Gélat, P., Guasch, L., Jaros, J., Jing, Y. and Jones, R., 2022. Benchmark problems for transcranial ultrasound simulation: Intercomparison of compressional wave models. The Journal of the Acoustical Society of America, 152(2), pp.1003-1019."
  },
  {
    "objectID": "phd_projects/entries/gelat.html#project-description",
    "href": "phd_projects/entries/gelat.html#project-description",
    "title": "Large-scale high-performance solver for therapeutic ultrasound applications",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney1, as well as osteoid osteoma2. OptimUS featured as part of an international software benchmarking exercise3 for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic institutions worldwide as well as a pedagogical tool for undergraduate and postgraduate students. OptimUS leverages the Bempp kernel developed by Prof Betcke.\nUsing OptimUS to solve large biomedical problems at high frequencies is currently challenging due to the large RAM consumption required, which is of the order of GigaByte to TeraByte. A dedicated high-frequency solver is therefore required to achieve realistic simulations at the operational MHz frequencies relevant to laboratory and clinical biomedical ultrasound applications. A promising kernel-independent Fast Multipole Method (FMM) BEM has recently been developed for the next release of the Bempp library and has been used to solve the Laplace equation. Research into dedicated high-frequency FMM implementation is needed to achieve BEM simulations on a larger scale than other numerical approaches. BEM has a distinct advantage over finite element and finite-difference time domain schemes as it suffers from only minimal numerical dispersion and pollution effects. It is therefore anticipated that the development of a high-performance FMM BEM formulation for Helmholtz kernels will be transformative in the field of biomedical ultrasound.\n\n\n\nThis project aims to develop an efficient FMM BEM formulation for high-frequency Helmholtz kernels. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from MRI/CT images. This formulation will also be used as a basis for treating the physics of weakly nonlinear wave propagation in tissue, which requires solving inhomogeneous Helmholtz equations for higher order harmonics. The performance of this new solver will be benchmarked against existing CPU implementations and will also be tested alongside the spectral element method solver recently developed by Prof Garth Wells’ team at University of Cambridge. It will also be benchmarked against other toolboxes used by the biomedical ultrasound community (e.g. k-Wave)\nThe successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as acoustic tomography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its wider adoption in clinical settings, where it could be used for personalised treatment plans based on anatomical models derived from MRI/CT scans, ultimately improving patient outcomes.\nThe supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art facilities required for the successful delivery of this project. These facilities include high-performance computing workstations as well as the UCL Research Computing Platforms Service.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of new BEM formulations for solving ultrasound waves in piecewise homogeneous and heterogeneous domains. The student will be actively involved in software development using Rust and Python, employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in focused ultrasound treatment platforms used in the clinic.\n\n\n\n\nHaqshenas, S.R., Gélat, P., van’t Wout, E., Betcke, T. and Saffari, N., 2021. A fast full-wave solver for calculating ultrasound propagation in the body. Ultrasonics, 110, p.106240.\nvan’t Wout, E., Haqshenas, S.R., Gélat, P., Betcke, T. and Saffari, N., 2022. Frequency-robust preconditioning of boundary integral equations for acoustic transmission. Journal of Computational Physics, 462, p.111229.\nAubry, J.F., Bates, O., Boehm, C., Butts Pauly, K., Christensen, D., Cueto, C., Gélat, P., Guasch, L., Jaros, J., Jing, Y. and Jones, R., 2022. Benchmark problems for transcranial ultrasound simulation: Intercomparison of compressional wave models. The Journal of the Acoustical Society of America, 152(2), pp.1003-1019."
  },
  {
    "objectID": "phd_projects/entries/Berloff_Quasigeostrophic.html",
    "href": "phd_projects/entries/Berloff_Quasigeostrophic.html",
    "title": "Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model",
    "section": "",
    "text": "This Project would be ideal for a student who seeks to develop skills in software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science research. It aims for a major overhaul and upgrade of the existing Quasi-Geostrophic Coupled Model (Q-GCM) of the ocean-atmosphere system to convert this model into a versatile modular community code for extremely fast high-resolution climate modelling in arbitrary geometrical setups. The model’s ability to quickly produce global-scope multi-century climate simulations faithfully representing mesoscale ocean–atmosphere interactions would allow it to set the milestones for future research of fundamental climate processes that are currently out of reach for state-of-the-art coupled General Circulation Models (GCMs) due to prohibitive computational expenses of such simulations.\nAdvancing our understanding of multi-scale climate variability is at the heart of the Project. A particular focus here is on the internal variability of the ocean-atmosphere subsystem of the climate system, which can occur even in the absence of variations in the external forcing. Such variability can generally be classified to fall into one of the following categories:\n\ninternal variability of the oceans;\ninternal variability of the atmosphere; and\ncoupled ocean–atmosphere variability.\n\nOf course, other factors contribute to the climate variability modes on different levels (e.g., interaction with cryosphere, coupled land–atmosphere processes, and so forth). Most coupled ocean-atmosphere GCMs do not yet adequately discriminate between the scenarios (i)–(iii), because the model dynamics still lacks accurate representation of small-scale processes due to their insufficient horizontal resolutions. In particular, global coupled GCMs do not have the required capability to resolve routinely the oceanic weather represented by multi-scale ensembles of synoptic mesoscale eddies, which evolve in a complicated, spatially inhomogeneous and poorly understood way.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1: Left panel: Snapshot with results of FESOM1.4 (Wang et al. 2014) simulation on the global mesh with 4 km resolution in the full North Atlantic; white color shows upper-ocean flow speed; note the intense Gulf stream current with surrounding turbulence. Right panel: Snapshot of the upper-ocean potential vorticity anomaly as simulated by the idealized square-box quasigeostrophic ocean model coupled to the atmosphere; this model produces turbulent Gulf stream that couples to the underlying atmosphere.\n\n\n\nThe cutting edge high-resolution ocean modelling efforts, which involve enormous computational expenses https://fesom.de/models/fesom14/ typically produce a single realization worth of a few decades of simulation at most, with marginally accepted dynamical resolution of the eddies achieved at least for midlatitudes (note that at high latitudes the eddies and, hence, required resolutions become even smaller). Many such simulations are also run in the ocean-only context, despite the growing evidence that the mesoscale air-sea interaction affects, in major — and, perhaps, nonlinear ways, — the atmospheric large-scale low-frequency variability (Mathews et al. 2024). However, climate-type simulations not only require hundreds of multi-century runs for robust statistical ensemble predictions, but they also have to consider different environmental scenarios (e.g., for greenhouse gas emissions) and sensitivities to many physical factors. The ability to faithfully characterize the effects of the mesoscale ocean eddies and currents in a coupled, global setting is a major stumbling block in climate research, and is thus one of the grand research challenges of our time.\nOne way out of this deadlock is to develop accurate statistical–dynamical eddy parameterizations for the use in realistic models, which is a major task of its own. An alternative way, proposed here, is to make use of intermediate-complexity coupled ocean–atmosphere process models capable of accessing new and crucial knowledge about the processes involved, yet casting them in advanced settings that would permit direct comparisons with the real world’s climate variability.\nIndeed, driven by the surging demand of climate science, the last two decades witnessed development of idealized, intermediate-complexity, midlatitude, quasi-geostrophic (QG), ocean–atmosphere coupled models, which can routinely resolve oceanic mesoscale eddies (Hogg et al. 2003; Kravtsov et al. 2007; Berloff et al. 2007a). These models are at least 100 times more computationally efficient than the heavy-duty global coupled GCMs. The Q-GCM of Hogg et al. (2003), which is a starting point of this project, couples its oceanic and atmospheric subsystems via ageostrophic boundary layers of both fluids. Aside from the natural limitation of QG models to be formally accurate within the midlatitude belts, these models are cast in the simplest square-basin or channel geometries, which hinders their immediate application to interpreting the observed climate variability. Yet, Q-GCM is a powerful tool for simulating midlatitude coupled climate variability with fully resolved oceanic mesoscale turbulence.\nIdealized eddy-resolving ocean and coupled modelling thus far established not only existence and robustness of the intrinsic ocean-only variability dubbed as the rbulent Oscillator (e.g., Berloff et al. 2007b), but also the importance of this variability for the ocean-atmosphere coupled variability (Kravtsov et al. 2007; Kurashina and Berloff 2023a,b). Similar fundamental importance of the eddies for driving decadal variability has been established in the Southern Ocean (Hogg and Blundell 2006). Considering these coupled dynamics in progressively more realistic Q-GCM is of high priority, but this effort requires significant upgrades of the existing modelling capabilities.\n\n\nThe software development objective of the Project is very significant upgrade of the existing Q-GCM, including generation of modular geometrical setups, addition of new physics and incorporation of superior numerical algorithms, as well as the requisite updates to the post-processing tools and software library. The computational objective of the Project is to produce new multi-century simulations in the northern-hemisphere (NH) and southern-hemisphere (SH) model configurations. More specifically, we hypothesize that in the NH case the Atlantic and Pacific oceans will generate their own internal, large-scale decadal-to-interdecadal Turbulent Oscillator variability, which will be coupled through atmospheric teleconnections. In the SH case the situation is likely to be more complicated, as the midlatitude basins will be also connected via the Antarctic Circumpolar Current. The analysis objective of the Project is to gain dynamical understanding of the involved variabilities.\n\n\n\nThe starting point for the Project will be the most recent study of the Q-GCM idealized-ocean double-gyre coupling with the atmosphere, in which new, zonally asymmetric coupled variability modes have been discovered and understood (Kurashina and Berloff 2023a,b). The following Q-GCM model developments are envisioned:\n\nAdding the second (rectangular) ocean basin to represent Atlantic-Pacific teleconnections (NB: this configuration can be passed to a Master student for spin-off project);\nAdding capabilities for representing (arbitrary and) realistically shaped basins (this requires complete overhaul of the elliptic solver with the matrix capacitance method, and recoding the boundary conditions);\nUpgrading advection operators in both ocean and atmosphere components with the high-resolution, efficient CABARET advection scheme (Karabasov et al. 2009);\nAdding moist dynamics to the atmosphere (Kravtsov et al. 2022);\nDeveloping realistic NH and SH Q-GCM model configurations (i.e., with two isolated ocean basins; with three ocean basins connected by the circular Southern ocean) and obtaining the corresponding milestone solutions both with high- and low-resolution configurations (the latter will help to quantify effects of the small scales and serve as the basis for eddy parameterizations);\nDeveloping comprehensive post-processing library of numerical routines for both Eulerian and Lagrangian analyses and visualizations of the Q-GCM solutions;\nProviding initial analyses of the milestone solutions; disentangling causalities of the ocean-atmosphere and ocean-ocean couplings, as well as understanding the main mesoscale eddy effects and their mechanisms;\nDeveloping mixed-layer model for dynamics of floating tracers, such as plankton and pollutants; this will in effect prepare ground for the coupled ocean-atmosphere modelling with oceanic biochemistry and with global carbon cycle.\n\nThe student will benefit from the interdisciplinary nature of the Project that combines a great deal of original and creative research within the remit of software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science. Close interactions with external research partners will be a vital aspect of the Project, ensuring its optimal development and success: Prof. Sergey Kravtsov (University of Wisconsin, USA), Prof. William Dewar (Florida State University, USA) and Prof. Andrew Hogg (Australian National University, Australia). The Project will be a great opportunity for outreaching the climate science community and providing broad and practical impact.\n\n\n\n\nBerloff, P., A. Hogg, and W. Dewar, 2007b: The turbulent oscillator: A mechanism of low-frequency variability of the wind-driven ocean gyres. J. Phys. Oceanogr., 37, 2363–2386.\nBerloff, P., S. Kravtsov, W. Dewar, and J. McWilliams, 2007a: Ocean eddy dynamics in a coupled ocean-atmosphere model. J. Phys. Oceanogr., 37, 1103–1121.\nHogg, A., and J. Blundell, 2006: Interdecadal variability of the Southern Ocean. J. Phys. Oceanogr., 36, 1626-–1645.\nHogg, A., W. Dewar, P. Killworth et al., 2003: A quasi-geostrophic coupled model (Q-GCM). Monthly Weather Review, 131, 2261–2278.\nKarabasov, S., P. Berloff, and V. Goloviznin, 2009: CABARET in the ocean gyres. Ocean Modelling, 30, 155–168.\nKravtsov, S., I. Mastilovic, A. Hogg, W. Dewar, and J. Blundell, 2022: The Moist Quasi-Geostrophic Coupled Model: MQ-GCM 2.0. Geoscientific Model Development, 15, 7449-–7469.\nKravtsov, S., W. Dewar, P. Berloff, J. McWilliams, and M. Ghil, 2007: A highly nonlinear coupled mode of decadal variability in a midlatitude ocean-atmosphere model. Dyn. Atmos. Ocean., 43, 123–150.\nKurashina, R., and P. Berloff, 2023b: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part II: Ocean mechanisms. Climate Dynamics, doi:10.1007/s00382-023-06767-x.\nKurashina, R., and P. Berloff, 2023a: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part I: Anatomy. Climate Dynamics, doi:10.1007/s00382-023-06782-y.\nMathews, J. P., Czaja, A., Vitart, F., and Roberts, C., 2024: Gulf Stream moisture fluxes impact atmospheric blocks throughout the Northern Hemisphere. Geophysical Research Letters, 51, e2024GL108826. https://doi.org/10.1029/2024GL108826\nWang, Q., Danilov, S., Sidorenko, D., Timmermann, R., Wekerle, C., Wang, X., … and Schröter, J., 2014: The Finite Element Sea Ice-Ocean Model (FESOM) v. 1.4: formulation of an ocean general circulation model. Geoscientific Model Development, 7(2), 663–693."
  },
  {
    "objectID": "phd_projects/entries/Berloff_Quasigeostrophic.html#project-description",
    "href": "phd_projects/entries/Berloff_Quasigeostrophic.html#project-description",
    "title": "Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model",
    "section": "",
    "text": "This Project would be ideal for a student who seeks to develop skills in software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science research. It aims for a major overhaul and upgrade of the existing Quasi-Geostrophic Coupled Model (Q-GCM) of the ocean-atmosphere system to convert this model into a versatile modular community code for extremely fast high-resolution climate modelling in arbitrary geometrical setups. The model’s ability to quickly produce global-scope multi-century climate simulations faithfully representing mesoscale ocean–atmosphere interactions would allow it to set the milestones for future research of fundamental climate processes that are currently out of reach for state-of-the-art coupled General Circulation Models (GCMs) due to prohibitive computational expenses of such simulations.\nAdvancing our understanding of multi-scale climate variability is at the heart of the Project. A particular focus here is on the internal variability of the ocean-atmosphere subsystem of the climate system, which can occur even in the absence of variations in the external forcing. Such variability can generally be classified to fall into one of the following categories:\n\ninternal variability of the oceans;\ninternal variability of the atmosphere; and\ncoupled ocean–atmosphere variability.\n\nOf course, other factors contribute to the climate variability modes on different levels (e.g., interaction with cryosphere, coupled land–atmosphere processes, and so forth). Most coupled ocean-atmosphere GCMs do not yet adequately discriminate between the scenarios (i)–(iii), because the model dynamics still lacks accurate representation of small-scale processes due to their insufficient horizontal resolutions. In particular, global coupled GCMs do not have the required capability to resolve routinely the oceanic weather represented by multi-scale ensembles of synoptic mesoscale eddies, which evolve in a complicated, spatially inhomogeneous and poorly understood way.\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1: Left panel: Snapshot with results of FESOM1.4 (Wang et al. 2014) simulation on the global mesh with 4 km resolution in the full North Atlantic; white color shows upper-ocean flow speed; note the intense Gulf stream current with surrounding turbulence. Right panel: Snapshot of the upper-ocean potential vorticity anomaly as simulated by the idealized square-box quasigeostrophic ocean model coupled to the atmosphere; this model produces turbulent Gulf stream that couples to the underlying atmosphere.\n\n\n\nThe cutting edge high-resolution ocean modelling efforts, which involve enormous computational expenses https://fesom.de/models/fesom14/ typically produce a single realization worth of a few decades of simulation at most, with marginally accepted dynamical resolution of the eddies achieved at least for midlatitudes (note that at high latitudes the eddies and, hence, required resolutions become even smaller). Many such simulations are also run in the ocean-only context, despite the growing evidence that the mesoscale air-sea interaction affects, in major — and, perhaps, nonlinear ways, — the atmospheric large-scale low-frequency variability (Mathews et al. 2024). However, climate-type simulations not only require hundreds of multi-century runs for robust statistical ensemble predictions, but they also have to consider different environmental scenarios (e.g., for greenhouse gas emissions) and sensitivities to many physical factors. The ability to faithfully characterize the effects of the mesoscale ocean eddies and currents in a coupled, global setting is a major stumbling block in climate research, and is thus one of the grand research challenges of our time.\nOne way out of this deadlock is to develop accurate statistical–dynamical eddy parameterizations for the use in realistic models, which is a major task of its own. An alternative way, proposed here, is to make use of intermediate-complexity coupled ocean–atmosphere process models capable of accessing new and crucial knowledge about the processes involved, yet casting them in advanced settings that would permit direct comparisons with the real world’s climate variability.\nIndeed, driven by the surging demand of climate science, the last two decades witnessed development of idealized, intermediate-complexity, midlatitude, quasi-geostrophic (QG), ocean–atmosphere coupled models, which can routinely resolve oceanic mesoscale eddies (Hogg et al. 2003; Kravtsov et al. 2007; Berloff et al. 2007a). These models are at least 100 times more computationally efficient than the heavy-duty global coupled GCMs. The Q-GCM of Hogg et al. (2003), which is a starting point of this project, couples its oceanic and atmospheric subsystems via ageostrophic boundary layers of both fluids. Aside from the natural limitation of QG models to be formally accurate within the midlatitude belts, these models are cast in the simplest square-basin or channel geometries, which hinders their immediate application to interpreting the observed climate variability. Yet, Q-GCM is a powerful tool for simulating midlatitude coupled climate variability with fully resolved oceanic mesoscale turbulence.\nIdealized eddy-resolving ocean and coupled modelling thus far established not only existence and robustness of the intrinsic ocean-only variability dubbed as the rbulent Oscillator (e.g., Berloff et al. 2007b), but also the importance of this variability for the ocean-atmosphere coupled variability (Kravtsov et al. 2007; Kurashina and Berloff 2023a,b). Similar fundamental importance of the eddies for driving decadal variability has been established in the Southern Ocean (Hogg and Blundell 2006). Considering these coupled dynamics in progressively more realistic Q-GCM is of high priority, but this effort requires significant upgrades of the existing modelling capabilities.\n\n\nThe software development objective of the Project is very significant upgrade of the existing Q-GCM, including generation of modular geometrical setups, addition of new physics and incorporation of superior numerical algorithms, as well as the requisite updates to the post-processing tools and software library. The computational objective of the Project is to produce new multi-century simulations in the northern-hemisphere (NH) and southern-hemisphere (SH) model configurations. More specifically, we hypothesize that in the NH case the Atlantic and Pacific oceans will generate their own internal, large-scale decadal-to-interdecadal Turbulent Oscillator variability, which will be coupled through atmospheric teleconnections. In the SH case the situation is likely to be more complicated, as the midlatitude basins will be also connected via the Antarctic Circumpolar Current. The analysis objective of the Project is to gain dynamical understanding of the involved variabilities.\n\n\n\nThe starting point for the Project will be the most recent study of the Q-GCM idealized-ocean double-gyre coupling with the atmosphere, in which new, zonally asymmetric coupled variability modes have been discovered and understood (Kurashina and Berloff 2023a,b). The following Q-GCM model developments are envisioned:\n\nAdding the second (rectangular) ocean basin to represent Atlantic-Pacific teleconnections (NB: this configuration can be passed to a Master student for spin-off project);\nAdding capabilities for representing (arbitrary and) realistically shaped basins (this requires complete overhaul of the elliptic solver with the matrix capacitance method, and recoding the boundary conditions);\nUpgrading advection operators in both ocean and atmosphere components with the high-resolution, efficient CABARET advection scheme (Karabasov et al. 2009);\nAdding moist dynamics to the atmosphere (Kravtsov et al. 2022);\nDeveloping realistic NH and SH Q-GCM model configurations (i.e., with two isolated ocean basins; with three ocean basins connected by the circular Southern ocean) and obtaining the corresponding milestone solutions both with high- and low-resolution configurations (the latter will help to quantify effects of the small scales and serve as the basis for eddy parameterizations);\nDeveloping comprehensive post-processing library of numerical routines for both Eulerian and Lagrangian analyses and visualizations of the Q-GCM solutions;\nProviding initial analyses of the milestone solutions; disentangling causalities of the ocean-atmosphere and ocean-ocean couplings, as well as understanding the main mesoscale eddy effects and their mechanisms;\nDeveloping mixed-layer model for dynamics of floating tracers, such as plankton and pollutants; this will in effect prepare ground for the coupled ocean-atmosphere modelling with oceanic biochemistry and with global carbon cycle.\n\nThe student will benefit from the interdisciplinary nature of the Project that combines a great deal of original and creative research within the remit of software engineering for computational modelling, scientific computing, data science, geophysical fluid dynamics, and climate science. Close interactions with external research partners will be a vital aspect of the Project, ensuring its optimal development and success: Prof. Sergey Kravtsov (University of Wisconsin, USA), Prof. William Dewar (Florida State University, USA) and Prof. Andrew Hogg (Australian National University, Australia). The Project will be a great opportunity for outreaching the climate science community and providing broad and practical impact.\n\n\n\n\nBerloff, P., A. Hogg, and W. Dewar, 2007b: The turbulent oscillator: A mechanism of low-frequency variability of the wind-driven ocean gyres. J. Phys. Oceanogr., 37, 2363–2386.\nBerloff, P., S. Kravtsov, W. Dewar, and J. McWilliams, 2007a: Ocean eddy dynamics in a coupled ocean-atmosphere model. J. Phys. Oceanogr., 37, 1103–1121.\nHogg, A., and J. Blundell, 2006: Interdecadal variability of the Southern Ocean. J. Phys. Oceanogr., 36, 1626-–1645.\nHogg, A., W. Dewar, P. Killworth et al., 2003: A quasi-geostrophic coupled model (Q-GCM). Monthly Weather Review, 131, 2261–2278.\nKarabasov, S., P. Berloff, and V. Goloviznin, 2009: CABARET in the ocean gyres. Ocean Modelling, 30, 155–168.\nKravtsov, S., I. Mastilovic, A. Hogg, W. Dewar, and J. Blundell, 2022: The Moist Quasi-Geostrophic Coupled Model: MQ-GCM 2.0. Geoscientific Model Development, 15, 7449-–7469.\nKravtsov, S., W. Dewar, P. Berloff, J. McWilliams, and M. Ghil, 2007: A highly nonlinear coupled mode of decadal variability in a midlatitude ocean-atmosphere model. Dyn. Atmos. Ocean., 43, 123–150.\nKurashina, R., and P. Berloff, 2023b: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part II: Ocean mechanisms. Climate Dynamics, doi:10.1007/s00382-023-06767-x.\nKurashina, R., and P. Berloff, 2023a: Low-frequency variability enhancement of the midlatitude climate in an eddy-resolving, coupled ocean-atmosphere model. Part I: Anatomy. Climate Dynamics, doi:10.1007/s00382-023-06782-y.\nMathews, J. P., Czaja, A., Vitart, F., and Roberts, C., 2024: Gulf Stream moisture fluxes impact atmospheric blocks throughout the Northern Hemisphere. Geophysical Research Letters, 51, e2024GL108826. https://doi.org/10.1029/2024GL108826\nWang, Q., Danilov, S., Sidorenko, D., Timmermann, R., Wekerle, C., Wang, X., … and Schröter, J., 2014: The Finite Element Sea Ice-Ocean Model (FESOM) v. 1.4: formulation of an ocean general circulation model. Geoscientific Model Development, 7(2), 663–693."
  },
  {
    "objectID": "phd_projects/entries/BetckeM_inverseproblems.html",
    "href": "phd_projects/entries/BetckeM_inverseproblems.html",
    "title": "All-at-once deep learning methods for nonlinear PDE based inverse problems",
    "section": "",
    "text": "In inverse problems (IPs) a natural role for deep learning (DL) is to encode prior information contained in the training set e.g. anatomical and pathological similarities between patients. Learned reconstruction methods realise this in different ways learning a prior/proximal operator for use within an optimisation scheme or a post-processing correction or a pseudo-inverse e.g. via an unrolled scheme (see [1] for an overview of such methods, [2] for methods with convergence guarantees, [3, 4] for examples our group’s work in the context of Photoacoustic tomography). While general PDE solvers are designed to work for all coefficients (in an appropriate function space), for the PDEs modelling the forward operator of a nonlinear IP we may have additional information about the distribution of the coefficients which can be used to improve performance. This idea underpins approaches based on unrolling of an iterative forward solver e.g., specifically in the context of solution of high frequency Helmholtz equation, GMRES with the net acting as a (flexible) parametric preconditioner [5], or iterates of a learned Born series [6], and more generic neural operators which directly construct a neural network approximation to the Green’s function of a linear PDE as a function of its coefficients (see e.g. [7,8,9]).\n\n\n\nThis project will combine both these strands of research. We will seek to optimise the forward solver’s performance for the coefficients on the manifold of the prior which is akin to adaptivity in classical PDE solvers and combine it optimally with the leaned prior into an all-at-once inversion procedure. Possible approaches to construction of such an all-at-once framework include: Unrolling and parametrisation of a nonlinear solver, e.g. Gauss-Newton, for minimising a regularised nonlinear data fitting functional. The Fréchet derivative of the nonlinear data term involves solution of the forward PDE and its adjoint which can be replaced by neural operators (or forward unrolling). The regularisation functional can be replaced with an input convex neural network, or for non-smooth priors, the prox of the regularisation functional can be replaced with a neural network. The whole assembly can then be jointly trained on the same training set following one of the training paradigms e.g. variational, fix point or adversarial. Another formulation can be proposed based on a PDE constraint optimisation framework which opts to minimise the regularised linear data consistency term subject to PDE constraints. The first order optimality conditions contain the Fréchet derivative, the forward and adjoint PDE solves, and the derivative of the regularisation functional, which again can be parametrised similarly as described above and used e.g. as a direction within first order optimisation method. As a part of the project we will investigate and develop suitable neural network architectures e.g. for the forward/adjoint neural operator pairs, Fréchet derivatives, regularisation functionals or their proxes, and efficient training strategies.\n\n\n\nThe objective of the project is to spearhead the paradigm of hybrid learned and model based all-at-once methods paying equal attention to utilising available data sets to aid solution of both the forward and the inverse problem involved. To push the paradigm into the wider inverse problems community of researchers and practitioners a high-quality accompanying software package is necessary. Along the core functionality a.k.a. the all-at-once frameworks, the package will include implementations of state-of-the-art hybrid model and data based forward solvers interfacing to popular PDE software e.g., https://ngsolve.org/, https://fenicsproject.org/, http://www.k-wave.org/ to allow flexibility of the underlying PDEs and the method of their solution. We will include implementations of the state-of-the-art neural operators, learned and analytical proximal/regularisation operators and optimisation methods, and will investigate feasibility of interfacing to popular inverse problems packages such as ODL https://odlgroup.github.io/odl/.\n[1] Arridge, Simon, Maass, Peter, Ozan, Öktem, Schönlieb, Carola-Bibiane. (2019). Solving inverse problems using data-driven models. Acta Numerica. 28. 1-174.\n[2] S. Mukherjee, A. Hauptmann, O. Öktem, M. Pereyra and C. -B. Schönlieb, Learned Reconstruction Methods With Convergence Guarantees: A survey of concepts and applications. IEEE Signal Processing Magazine, vol. 40, no. 1, pp. 164-182, Jan. 2023.\n[3] Hauptmann A, Lucka F, Betcke M, Huynh N, Adler J, Cox B, Beard P, Ourselin S, Arridge S. Model-based learning for accelerated, limited-view 3-D photoacoustic tomography. IEEE Transactions on Medical Imaging. 2018 Mar 29;37(6):1382-93.\n[4] Bolin Pan and Marta M. Betcke, On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector, SIAM Journal on Imaging Sciences 2023 16:2, 770-801\n[5] Stanziola, A., Arridge, S. R., Cox, B. T., & Treeby, B. E. (2021). A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. Journal of Computational Physics, 441, 110430.\n[6] Antonio Stanziola, Simon Arridge, Ben T. Cox, Bradley E. Treeby; A learned Born series for highly-scattering media. JASA Express Lett. 1 May 2023; 3 (5): 052401.\n[7] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. ICLR International Conference on Learning Representations (2021). https://openreview.net/forum?id=c8P9NQVtmnO.\n[8] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, Andrew Stuart. Neural Operator: Graph Kernel Network for Partial Differential Equations. ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (2019), https://openreview.net/forum?id=fg2ZFmXFO3.\n[9] Boullé, N., Townsend, A. Learning Elliptic Partial Differential Equations with Randomized Linear Algebra. Found Comput Math 23, 709–739 (2023)."
  },
  {
    "objectID": "phd_projects/entries/BetckeM_inverseproblems.html#project-description",
    "href": "phd_projects/entries/BetckeM_inverseproblems.html#project-description",
    "title": "All-at-once deep learning methods for nonlinear PDE based inverse problems",
    "section": "",
    "text": "In inverse problems (IPs) a natural role for deep learning (DL) is to encode prior information contained in the training set e.g. anatomical and pathological similarities between patients. Learned reconstruction methods realise this in different ways learning a prior/proximal operator for use within an optimisation scheme or a post-processing correction or a pseudo-inverse e.g. via an unrolled scheme (see [1] for an overview of such methods, [2] for methods with convergence guarantees, [3, 4] for examples our group’s work in the context of Photoacoustic tomography). While general PDE solvers are designed to work for all coefficients (in an appropriate function space), for the PDEs modelling the forward operator of a nonlinear IP we may have additional information about the distribution of the coefficients which can be used to improve performance. This idea underpins approaches based on unrolling of an iterative forward solver e.g., specifically in the context of solution of high frequency Helmholtz equation, GMRES with the net acting as a (flexible) parametric preconditioner [5], or iterates of a learned Born series [6], and more generic neural operators which directly construct a neural network approximation to the Green’s function of a linear PDE as a function of its coefficients (see e.g. [7,8,9]).\n\n\n\nThis project will combine both these strands of research. We will seek to optimise the forward solver’s performance for the coefficients on the manifold of the prior which is akin to adaptivity in classical PDE solvers and combine it optimally with the leaned prior into an all-at-once inversion procedure. Possible approaches to construction of such an all-at-once framework include: Unrolling and parametrisation of a nonlinear solver, e.g. Gauss-Newton, for minimising a regularised nonlinear data fitting functional. The Fréchet derivative of the nonlinear data term involves solution of the forward PDE and its adjoint which can be replaced by neural operators (or forward unrolling). The regularisation functional can be replaced with an input convex neural network, or for non-smooth priors, the prox of the regularisation functional can be replaced with a neural network. The whole assembly can then be jointly trained on the same training set following one of the training paradigms e.g. variational, fix point or adversarial. Another formulation can be proposed based on a PDE constraint optimisation framework which opts to minimise the regularised linear data consistency term subject to PDE constraints. The first order optimality conditions contain the Fréchet derivative, the forward and adjoint PDE solves, and the derivative of the regularisation functional, which again can be parametrised similarly as described above and used e.g. as a direction within first order optimisation method. As a part of the project we will investigate and develop suitable neural network architectures e.g. for the forward/adjoint neural operator pairs, Fréchet derivatives, regularisation functionals or their proxes, and efficient training strategies.\n\n\n\nThe objective of the project is to spearhead the paradigm of hybrid learned and model based all-at-once methods paying equal attention to utilising available data sets to aid solution of both the forward and the inverse problem involved. To push the paradigm into the wider inverse problems community of researchers and practitioners a high-quality accompanying software package is necessary. Along the core functionality a.k.a. the all-at-once frameworks, the package will include implementations of state-of-the-art hybrid model and data based forward solvers interfacing to popular PDE software e.g., https://ngsolve.org/, https://fenicsproject.org/, http://www.k-wave.org/ to allow flexibility of the underlying PDEs and the method of their solution. We will include implementations of the state-of-the-art neural operators, learned and analytical proximal/regularisation operators and optimisation methods, and will investigate feasibility of interfacing to popular inverse problems packages such as ODL https://odlgroup.github.io/odl/.\n[1] Arridge, Simon, Maass, Peter, Ozan, Öktem, Schönlieb, Carola-Bibiane. (2019). Solving inverse problems using data-driven models. Acta Numerica. 28. 1-174.\n[2] S. Mukherjee, A. Hauptmann, O. Öktem, M. Pereyra and C. -B. Schönlieb, Learned Reconstruction Methods With Convergence Guarantees: A survey of concepts and applications. IEEE Signal Processing Magazine, vol. 40, no. 1, pp. 164-182, Jan. 2023.\n[3] Hauptmann A, Lucka F, Betcke M, Huynh N, Adler J, Cox B, Beard P, Ourselin S, Arridge S. Model-based learning for accelerated, limited-view 3-D photoacoustic tomography. IEEE Transactions on Medical Imaging. 2018 Mar 29;37(6):1382-93.\n[4] Bolin Pan and Marta M. Betcke, On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector, SIAM Journal on Imaging Sciences 2023 16:2, 770-801\n[5] Stanziola, A., Arridge, S. R., Cox, B. T., & Treeby, B. E. (2021). A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. Journal of Computational Physics, 441, 110430.\n[6] Antonio Stanziola, Simon Arridge, Ben T. Cox, Bradley E. Treeby; A learned Born series for highly-scattering media. JASA Express Lett. 1 May 2023; 3 (5): 052401.\n[7] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. ICLR International Conference on Learning Representations (2021). https://openreview.net/forum?id=c8P9NQVtmnO.\n[8] Anima Anandkumar, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Nikola Kovachki, Zongyi Li, Burigede Liu, Andrew Stuart. Neural Operator: Graph Kernel Network for Partial Differential Equations. ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations (2019), https://openreview.net/forum?id=fg2ZFmXFO3.\n[9] Boullé, N., Townsend, A. Learning Elliptic Partial Differential Equations with Randomized Linear Algebra. Found Comput Math 23, 709–739 (2023)."
  },
  {
    "objectID": "phd_projects/entries/hethernington2.html",
    "href": "phd_projects/entries/hethernington2.html",
    "title": "AI code generation and numerical codes",
    "section": "",
    "text": "A great deal of work is being done on automated code generation through LLMs, for example, through Github’s work on Copilot. As yet, though, little has been done to consider the implications of AI-assisted code generation for highly numerical code, for example, the parallel simulation codes used to develop digital twins of complex physical phenomena. We propose to study the behaviour of LLM-assisted coding for mathematically sophisticated floating-point software. This may lead to development of new domain specific languages closer to the natural language that would be used by mathematicians to describe a programme’s key characteristics, for example “A 2-d implementation of the wave equation on an adaptive mesh, parallelised with a Halo-Swap"
  },
  {
    "objectID": "phd_projects/entries/hethernington2.html#project-description",
    "href": "phd_projects/entries/hethernington2.html#project-description",
    "title": "AI code generation and numerical codes",
    "section": "",
    "text": "A great deal of work is being done on automated code generation through LLMs, for example, through Github’s work on Copilot. As yet, though, little has been done to consider the implications of AI-assisted code generation for highly numerical code, for example, the parallel simulation codes used to develop digital twins of complex physical phenomena. We propose to study the behaviour of LLM-assisted coding for mathematically sophisticated floating-point software. This may lead to development of new domain specific languages closer to the natural language that would be used by mathematicians to describe a programme’s key characteristics, for example “A 2-d implementation of the wave equation on an adaptive mesh, parallelised with a Halo-Swap"
  },
  {
    "objectID": "phd_projects/entries/Cox_neuralODE.html",
    "href": "phd_projects/entries/Cox_neuralODE.html",
    "title": "Time Reversal Imaging and Learned Physics with Neural ODEs",
    "section": "",
    "text": "Many imaging problems take the form of recovering the coefficients of a partial differential equation (PDE). Examples include Electrical Impedance Tomography (EIT), Diffuse Optical Tomography (DOT) and Magnetic Induction Tomography (MIT). These are categorised as non-linear. Ill-posed inverse problems and a classical approach to their solution is to iteratively solve a forward and adjoint problem by solving the PDE system explicitly. A particular challenge occurs for time-dependent problems such as Ultrasound Computed Tomography (USCT) or PhotoAcoustic Tomography (PAT) because of the extra time and memory complexity of the extra time dimension. In these problems the adjoint solution to the governing PDE has the physical interpretation of time-reversal, and has led to practical solutions for problems in several million unknowns with computation time of several hours. In the machine learning community there is an increasing interest in using deep learning techniques for solving forward and inverse problems involving PDEs, as well as for “discovering Physics” by finding appropriate polynomial combinations of differential operators to fit the observed behaviour of physical systems. For time dependent problems a key technology is the Neural ODE concept, wherein the time-dependent part of a model is evaluated by classical methods and a neural network is used at each time point to express the spatial derivatives of the operator. Prior work at UCL involving the supervisor team has developed the world leading k-Wave software for modelling acoustic propagation in a variety of setting, including heterogeneous sound speed, variable absorption. K-Wave .currently has 15.000 users from 70 countries. Recently several advances have been made in leveraging deep learning techniques including a learned Helmholtz [1], a differentiable wave simulator (j-wave) [2], and a learned Born series method [3].\n\n[1] A Stanziola, SR Arridge, BT Cox, BE Treeby, “A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound”, Journal of Computational Physics 441, 110430\n[2] A. Stanziola, S.R. Arridge, B.T. Cox, B.E. Treeby, “j-Wave: An open-source differentiable wave simulator”, SoftwareX 22, 101338\n[3] A Stanziola, S Arridge, BT Cox, BE Treeby, “A learned Born series for highly-scattering media”, JASA Express Letters 3 (5)\n\n\n\n\nIn this project we want to exploit the Neural ODE concept for inverse problems in ultrasound, including USCT and PAT. We will first tackle some established problems such as the initial pressure reconstruction in PAT and sound speed recovery in USCT. In these problems the spatial operator is known and we want to compare the efficiency and accuracy of Neural ODE forward adjoint solvers methods to existing solutions. Next we will use a convolutional neural net (CNN) in place of an explicit spatial operator to learn a non-linear PDE model for ultrasound, including extension to acoustic attenuation. Finally we will use methods of symbolic regression to learn back an interpretable physical model from the CNN as a method of discovery novel representations for non-linear acoustic propagation. Developed methods will be tested in close collaboration with the Biomedical Ultrasound Group at UCL dept Medical Physics and Biomedical Engineering, firstly on phantoms with controlled parameters and subsequently on in-vivo biological samples and human volunteers\n\n\n\nSoftware to be used :\n\nk-wave : kwave.org\njaxdf: https://github.com/ucl-bug/jaxdf\nTorchDiffEq: https://github.com/rtqichen/torchdiffeq/\nDiffrax: https://docs.kidger.site/diffrax\n\nMilestones 1. Year 1 : Integrate k-Wave as forward model within TorchDiff/Diffrax and train to reconstruct sound speed from a model 2D USCT problem 2. Year 2 : Implement a CNN to learn a non-linear spatial derivative operator for variable sound speed and acoustic attenuation 3. Year 3 : Develop symbolic regression methods to learn novel interpretable PDE models for acoustic propagation. Deliverables : New methods will be beta-released on Github and introduced in short courses/Summer schools where acoustic modelling is routinely presented. Results will be presented in conferences such as Acoustic Society of America (ASA) and Medical Ultrasound Tomography (MUST). Stable implementations will be disseminated with existing k-Wave/j-Wave platforms under rolling releases."
  },
  {
    "objectID": "phd_projects/entries/Cox_neuralODE.html#project-description",
    "href": "phd_projects/entries/Cox_neuralODE.html#project-description",
    "title": "Time Reversal Imaging and Learned Physics with Neural ODEs",
    "section": "",
    "text": "Many imaging problems take the form of recovering the coefficients of a partial differential equation (PDE). Examples include Electrical Impedance Tomography (EIT), Diffuse Optical Tomography (DOT) and Magnetic Induction Tomography (MIT). These are categorised as non-linear. Ill-posed inverse problems and a classical approach to their solution is to iteratively solve a forward and adjoint problem by solving the PDE system explicitly. A particular challenge occurs for time-dependent problems such as Ultrasound Computed Tomography (USCT) or PhotoAcoustic Tomography (PAT) because of the extra time and memory complexity of the extra time dimension. In these problems the adjoint solution to the governing PDE has the physical interpretation of time-reversal, and has led to practical solutions for problems in several million unknowns with computation time of several hours. In the machine learning community there is an increasing interest in using deep learning techniques for solving forward and inverse problems involving PDEs, as well as for “discovering Physics” by finding appropriate polynomial combinations of differential operators to fit the observed behaviour of physical systems. For time dependent problems a key technology is the Neural ODE concept, wherein the time-dependent part of a model is evaluated by classical methods and a neural network is used at each time point to express the spatial derivatives of the operator. Prior work at UCL involving the supervisor team has developed the world leading k-Wave software for modelling acoustic propagation in a variety of setting, including heterogeneous sound speed, variable absorption. K-Wave .currently has 15.000 users from 70 countries. Recently several advances have been made in leveraging deep learning techniques including a learned Helmholtz [1], a differentiable wave simulator (j-wave) [2], and a learned Born series method [3].\n\n[1] A Stanziola, SR Arridge, BT Cox, BE Treeby, “A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound”, Journal of Computational Physics 441, 110430\n[2] A. Stanziola, S.R. Arridge, B.T. Cox, B.E. Treeby, “j-Wave: An open-source differentiable wave simulator”, SoftwareX 22, 101338\n[3] A Stanziola, S Arridge, BT Cox, BE Treeby, “A learned Born series for highly-scattering media”, JASA Express Letters 3 (5)\n\n\n\n\nIn this project we want to exploit the Neural ODE concept for inverse problems in ultrasound, including USCT and PAT. We will first tackle some established problems such as the initial pressure reconstruction in PAT and sound speed recovery in USCT. In these problems the spatial operator is known and we want to compare the efficiency and accuracy of Neural ODE forward adjoint solvers methods to existing solutions. Next we will use a convolutional neural net (CNN) in place of an explicit spatial operator to learn a non-linear PDE model for ultrasound, including extension to acoustic attenuation. Finally we will use methods of symbolic regression to learn back an interpretable physical model from the CNN as a method of discovery novel representations for non-linear acoustic propagation. Developed methods will be tested in close collaboration with the Biomedical Ultrasound Group at UCL dept Medical Physics and Biomedical Engineering, firstly on phantoms with controlled parameters and subsequently on in-vivo biological samples and human volunteers\n\n\n\nSoftware to be used :\n\nk-wave : kwave.org\njaxdf: https://github.com/ucl-bug/jaxdf\nTorchDiffEq: https://github.com/rtqichen/torchdiffeq/\nDiffrax: https://docs.kidger.site/diffrax\n\nMilestones 1. Year 1 : Integrate k-Wave as forward model within TorchDiff/Diffrax and train to reconstruct sound speed from a model 2D USCT problem 2. Year 2 : Implement a CNN to learn a non-linear spatial derivative operator for variable sound speed and acoustic attenuation 3. Year 3 : Develop symbolic regression methods to learn novel interpretable PDE models for acoustic propagation. Deliverables : New methods will be beta-released on Github and introduced in short courses/Summer schools where acoustic modelling is routinely presented. Results will be presented in conferences such as Acoustic Society of America (ASA) and Medical Ultrasound Tomography (MUST). Stable implementations will be disseminated with existing k-Wave/j-Wave platforms under rolling releases."
  },
  {
    "objectID": "phd_projects/entries/Parpas_moleculardynamics.html",
    "href": "phd_projects/entries/Parpas_moleculardynamics.html",
    "title": "Efficient Computation of Transition States in Molecular Dynamics",
    "section": "",
    "text": "Transition states govern the outcomes of molecular reactions. Therefore, determining the transition states of molecules is a fundamental challenge encountered across diverse fields, including chemical physics, biology, material science, and chemical engineering. However, current numerical methods fall short of what is needed in practice because transition states are rare events within a high-dimensional stochastic dynamical system. As a result, the simulation of rare events is generally intractable and requires domain-specific knowledge of the underlying molecular system or extensive and time consuming trial-and-error computations.\n\n\n\nThe theoretical foundations of our methodology are provided by the remarkable fact that, under appropriate technical conditions, the eigenforms of a particular type of operator, known as the Witten Laplacian, concentrate on transition states. However, there is a catch: solving the associated Witten PDE in high dimensions is currently impractical. The objective of this project is to take of advantage of recent developments that showed how to represent the Witten PDE as a system of stochastic differential equations.[1] This opens the door to new numerical methods utilizing state-of-the-art stochastic simulation and machine learning methods. These advancements are especially promising for harnessing the power of massively parallel computing.\nThere is a deep connection between the stochastic dynamics in used to understand saddle points and gradient flows. Associated with the stochastic representation in [1] there is a partial differential equation, the Fokker-Planck equation, that describes the evolution of the probability density of the stochastic dynamics. In a seminal paper, the authors in [2] made a connection between gradient flows and the Fokker-Planck equation. Moreover, their proof is constructive and based on an implicit discretization scheme that is known as the JKO scheme. Recent works have shown how to leverage the connection between the Fokker-Planck equation and gradient flows to develop numerical methods to solve for the probability density function of the stochastic representation directly. Unfortunately, the implementation of the JKO scheme requires the solution of a large optimization problem at each iteration. To address this challenge, you will study advanced distributed optimization methods that can scale to large dimensions. A more speculative direction of research is to explore whether the Witten PDE can also be written as a gradient flow in an appropriate space. If successful, this direction will lead to a new class of algorithms for computing transition states.\n[1] T. Lelièvre, P. Parpas. Using Witten Laplacians to locate index-1 saddle points SIAM Journal on Scientific Computing, to appear, 2023. [2] Jordan R, Kinderlehrer D, Otto F. The variational formulation of the Fokker–Planck equation. SIAM journal on mathematical analysis. 1998 Jan;29(1):1-7.\n\n\n\nIt is expected that you will develop an open-source software library that can be used by the wider community. Initially, the focus will be on benchmark problems but the hope is that you will eventually be able to simulate realistic systems. You will build on an initial implementation available here: https://github.com/pp500/Stochastic-Saddle-Point-Dynamics"
  },
  {
    "objectID": "phd_projects/entries/Parpas_moleculardynamics.html#project-description",
    "href": "phd_projects/entries/Parpas_moleculardynamics.html#project-description",
    "title": "Efficient Computation of Transition States in Molecular Dynamics",
    "section": "",
    "text": "Transition states govern the outcomes of molecular reactions. Therefore, determining the transition states of molecules is a fundamental challenge encountered across diverse fields, including chemical physics, biology, material science, and chemical engineering. However, current numerical methods fall short of what is needed in practice because transition states are rare events within a high-dimensional stochastic dynamical system. As a result, the simulation of rare events is generally intractable and requires domain-specific knowledge of the underlying molecular system or extensive and time consuming trial-and-error computations.\n\n\n\nThe theoretical foundations of our methodology are provided by the remarkable fact that, under appropriate technical conditions, the eigenforms of a particular type of operator, known as the Witten Laplacian, concentrate on transition states. However, there is a catch: solving the associated Witten PDE in high dimensions is currently impractical. The objective of this project is to take of advantage of recent developments that showed how to represent the Witten PDE as a system of stochastic differential equations.[1] This opens the door to new numerical methods utilizing state-of-the-art stochastic simulation and machine learning methods. These advancements are especially promising for harnessing the power of massively parallel computing.\nThere is a deep connection between the stochastic dynamics in used to understand saddle points and gradient flows. Associated with the stochastic representation in [1] there is a partial differential equation, the Fokker-Planck equation, that describes the evolution of the probability density of the stochastic dynamics. In a seminal paper, the authors in [2] made a connection between gradient flows and the Fokker-Planck equation. Moreover, their proof is constructive and based on an implicit discretization scheme that is known as the JKO scheme. Recent works have shown how to leverage the connection between the Fokker-Planck equation and gradient flows to develop numerical methods to solve for the probability density function of the stochastic representation directly. Unfortunately, the implementation of the JKO scheme requires the solution of a large optimization problem at each iteration. To address this challenge, you will study advanced distributed optimization methods that can scale to large dimensions. A more speculative direction of research is to explore whether the Witten PDE can also be written as a gradient flow in an appropriate space. If successful, this direction will lead to a new class of algorithms for computing transition states.\n[1] T. Lelièvre, P. Parpas. Using Witten Laplacians to locate index-1 saddle points SIAM Journal on Scientific Computing, to appear, 2023. [2] Jordan R, Kinderlehrer D, Otto F. The variational formulation of the Fokker–Planck equation. SIAM journal on mathematical analysis. 1998 Jan;29(1):1-7.\n\n\n\nIt is expected that you will develop an open-source software library that can be used by the wider community. Initially, the focus will be on benchmark problems but the hope is that you will eventually be able to simulate realistic systems. You will build on an initial implementation available here: https://github.com/pp500/Stochastic-Saddle-Point-Dynamics"
  },
  {
    "objectID": "phd_projects/entries/Karin_tissue.html",
    "href": "phd_projects/entries/Karin_tissue.html",
    "title": "Agent-based simulation of tissue self-organization",
    "section": "",
    "text": "The project aims to develop a computational framework for agent-based simulations of tissue dynamics, encompassing self-organization and disease dysregulation. The primary goal is to create a software tool that facilitates the development of interpretable and predictive mechanistic simulations of the dynamic interaction network of cells within tissues. This tool will focus on how these interactions balance growth, removal, and differentiation. Understanding these processes is essential for studying healthy tissue function and the development of diseases such as cancer.\n\n\nThe framework will be based on the group’s work on effective mathematical models for cell fate decisions.\n[1] Simons BD, Karin O. Tuning of plasma cell lifespan by competition explains the longevity and heterogeneity of antibody persistence. Immunity. 2024 Mar 12;57(3):600-11.\n[2] Karin O. EnhancerNet: A predictive model of cell identity dynamics through enhancer selection. Development. 2024 Sep 17:dev-202997.\n\n\n\nSpecific applications may involve modelling stem cell dynamics in adult tissues or during embryonic development and could include collaboration with experimental groups. The project will be organized gradually, with early deliverables focusing on more limited modeling contexts. Throughout the project, the modelling framework will be tested against specific biological applications. The outcome will be an open-source software package that supports the modelling of complex self-organization in biological tissues.\n\n\n\nFrom a software development perspective, the project will utilize Python along with numerical computation libraries such as NumPy and SciPy and support the highly nonlinear nature of the underlying dynamics and possibly a large number of agents (cells). We aim to make the code as general as possible to facilitate the investigation of complex self-organizing behaviours, such as spatio-temporal patterning or mutant invasion."
  },
  {
    "objectID": "phd_projects/entries/Karin_tissue.html#project-description",
    "href": "phd_projects/entries/Karin_tissue.html#project-description",
    "title": "Agent-based simulation of tissue self-organization",
    "section": "",
    "text": "The project aims to develop a computational framework for agent-based simulations of tissue dynamics, encompassing self-organization and disease dysregulation. The primary goal is to create a software tool that facilitates the development of interpretable and predictive mechanistic simulations of the dynamic interaction network of cells within tissues. This tool will focus on how these interactions balance growth, removal, and differentiation. Understanding these processes is essential for studying healthy tissue function and the development of diseases such as cancer.\n\n\nThe framework will be based on the group’s work on effective mathematical models for cell fate decisions.\n[1] Simons BD, Karin O. Tuning of plasma cell lifespan by competition explains the longevity and heterogeneity of antibody persistence. Immunity. 2024 Mar 12;57(3):600-11.\n[2] Karin O. EnhancerNet: A predictive model of cell identity dynamics through enhancer selection. Development. 2024 Sep 17:dev-202997.\n\n\n\nSpecific applications may involve modelling stem cell dynamics in adult tissues or during embryonic development and could include collaboration with experimental groups. The project will be organized gradually, with early deliverables focusing on more limited modeling contexts. Throughout the project, the modelling framework will be tested against specific biological applications. The outcome will be an open-source software package that supports the modelling of complex self-organization in biological tissues.\n\n\n\nFrom a software development perspective, the project will utilize Python along with numerical computation libraries such as NumPy and SciPy and support the highly nonlinear nature of the underlying dynamics and possibly a large number of agents (cells). We aim to make the code as general as possible to facilitate the investigation of complex self-organizing behaviours, such as spatio-temporal patterning or mutant invasion."
  },
  {
    "objectID": "phd_projects/entries/Tennyson_TROVE.html",
    "href": "phd_projects/entries/Tennyson_TROVE.html",
    "title": "Re-griding the TROVE nuclear motion program",
    "section": "",
    "text": "Tennyson and Yurchenko have developed a series of programs for solving the quantum mechanical Schrodinger equation for the motion of nuclei for small molecules. These programs are highly efficient and widely used (eg they underpin the ERC funded ExoMol project). TROVE (Theoretical ROVibrational Energies, see https://spectrove.readthedocs.io) is the most flexible of these program as it can be extended to molecules of arbitrary size and complexity. However the number of degrees of freedom that need to be considered grows as 3N-6 where N is the number of atoms. At present TROVE us products of one-dimensional grids which becomes increasingly inefficient a N increases. In addition we have been systematically updating our methodology to treat processes which lead to continuum states of the molecule being occupied. Experience with small (N=2 or N=3) molecules shows that specialized grids, currently not implemented in TROVE. To tackled cases with N &gt; 3 will require regridding of TROVE.\n\n\n\nThe main objective will be to develop methods of using multidimensional grids in place of products on 1 dimension grids. Two versions of this will be developed: for bound state problems largely aimed at systems with N &gt; 6 and for problems which need to consider continuum states where at present we cannot address problems with N=4 despite many requests to work on such problems. Initial work focus on the possibility of using Smolyak grids but probably the project will explore other possibilities The main output will be a version of TROVE with greatly enhanced functionality. The student will have the opportunity to run calculations for key problems if they wish.\n\n\n\nTROVE has a github repository: https://github.com/Trovemaster/TROVE Original coding can be done independent of the main software as suitable grid algorithms are developed. The final version will then integrated in the main TROVE software package."
  },
  {
    "objectID": "phd_projects/entries/Tennyson_TROVE.html#project-description",
    "href": "phd_projects/entries/Tennyson_TROVE.html#project-description",
    "title": "Re-griding the TROVE nuclear motion program",
    "section": "",
    "text": "Tennyson and Yurchenko have developed a series of programs for solving the quantum mechanical Schrodinger equation for the motion of nuclei for small molecules. These programs are highly efficient and widely used (eg they underpin the ERC funded ExoMol project). TROVE (Theoretical ROVibrational Energies, see https://spectrove.readthedocs.io) is the most flexible of these program as it can be extended to molecules of arbitrary size and complexity. However the number of degrees of freedom that need to be considered grows as 3N-6 where N is the number of atoms. At present TROVE us products of one-dimensional grids which becomes increasingly inefficient a N increases. In addition we have been systematically updating our methodology to treat processes which lead to continuum states of the molecule being occupied. Experience with small (N=2 or N=3) molecules shows that specialized grids, currently not implemented in TROVE. To tackled cases with N &gt; 3 will require regridding of TROVE.\n\n\n\nThe main objective will be to develop methods of using multidimensional grids in place of products on 1 dimension grids. Two versions of this will be developed: for bound state problems largely aimed at systems with N &gt; 6 and for problems which need to consider continuum states where at present we cannot address problems with N=4 despite many requests to work on such problems. Initial work focus on the possibility of using Smolyak grids but probably the project will explore other possibilities The main output will be a version of TROVE with greatly enhanced functionality. The student will have the opportunity to run calculations for key problems if they wish.\n\n\n\nTROVE has a github repository: https://github.com/Trovemaster/TROVE Original coding can be done independent of the main software as suitable grid algorithms are developed. The final version will then integrated in the main TROVE software package."
  },
  {
    "objectID": "phd_projects/entries/Betcke_GPUarchitectures.html",
    "href": "phd_projects/entries/Betcke_GPUarchitectures.html",
    "title": "Fast Multipole Methods on modern architectures",
    "section": "",
    "text": "### Existing background work\nFast Multipole Methods (FMM) are one of the fundamental algorithms of computational sciences. They allow the fast approximate evaluation of interactions of N particles with each other in linear complexity instead of quadratic complexity when naive direct evaluation methods are being used. The FMM goes back to Rokhlin and Greengard in 1987 and has since undergone substantial algorithmic and computational advances. FMM on GPUs for example won the Gordon Bell Price in 2009. However, while advances continued over the last ten years little work has been done to exploit Fast Multipole Methods beyond classical GPU computing. In particular, modern designs such as unified memory architectures on Apple Silicone and Nvidia Grace Hopper, mixed precision computations, or the use of tensor units in modern accelerators have received little attention.\nWe have started building up in our group our own FMM expertise as part of the Bempp project. Using Rust as main driver language we have developed a CPU based FMM that is highly portable and competitive with other established Fast Multipole libraries. We are currently porting this effort over to MPI based clusters. Most of this work has been part of the PhD thesis of an existing student in Betcke’s group.\n\n\nBased on our existing experience with our CPU based FMM implementation we want to expand to modern compute architectures. In particular, we have the following objects:\n\nDevelop optimised FMM implementations for unified memory architectures. Within the FMM the key drivers of computational cost are the Particle To Particle Interactions (P2P) and the Multipole 2 Locale Interactions (M2L). For a shallow computation tree P2P will dominate. For a deeper tree M2L will dominate. P2P especially benefits from computation on GPU cores. M2L operations can be accelerated on GPUs but often have lower compute intensity than P2P. If CPU and GPU share a unified address space and fast overall memory accesses many of the implementational problems of full GPU based FMM fall away and we can flexibly decide between work on CPU and on GPU cores. We want to exploit this to optimise new FMM implementations that make full use of unified memory architectures.\nExploit mixed precision arithmetic computations. For many applications it is enough for an FMM to deliver 6 to 7 digits of accuracy, which is just about in the limit of FP32 computations. However, naive implementation on FP32 often leads to fewer correct digits. We want to investigate which operations to run on FP64, and which to accelerate via FP32 while still being able to maintain sufficiently high accuracy for the FMM.\nOptimise for matrix-multiplication operations.. Many modern FMM formulation can be written in terms of matrix-matrix products, which can make use of tensor cores and other specialised registers for AI computations. We want to investigate how these can be used as part of the FMM workflow and allow speed-up of the overall computation.\n\n\n\n\nThe student will take time to familiarise themselves with our existing Rust code base, interface C++ to access accelerator devices and develop FMM on unified architectures. We therefore think the first part of FMM on unified architectures will take roughly two years of project time. In parallel the student will slowly get started on mixed precision experiments, and we expect this part of the project to be around one year in length, once the student has already assembled more experience on developing FMM. The final part on tensor and other AI accelerators will then build on the codebase and research experience with mixed precision arithmetic that the student has built up and will mainly take part in the last 1-1.5 years of the PhD.\n\n\n\nWe are currently preparing for release our first Rust based FMM version. The student will build on this code and integrate compute kernels for accelerators within this code base. Rust itself has limited support for GPU compute. So much of this work will be in C++ and interfaced to the Rust driver codes for the FMM. A particular interest is also to use Apple Silicone as unified memory environment for FMM. While not being used directly in HPC, Apple Silicon is widely spread and a cost effective way to develop unified memory codes. For HPC we will target Nvidia’s Grace Hopper, meaning the student will need to write separate low-level kernel implementations for Apple Metal and Nvidia Cuda. For our current FMM codes we use a BSD 3-Clause license and will continue this license for this project."
  },
  {
    "objectID": "phd_projects/entries/Betcke_GPUarchitectures.html#project-description",
    "href": "phd_projects/entries/Betcke_GPUarchitectures.html#project-description",
    "title": "Fast Multipole Methods on modern architectures",
    "section": "",
    "text": "### Existing background work\nFast Multipole Methods (FMM) are one of the fundamental algorithms of computational sciences. They allow the fast approximate evaluation of interactions of N particles with each other in linear complexity instead of quadratic complexity when naive direct evaluation methods are being used. The FMM goes back to Rokhlin and Greengard in 1987 and has since undergone substantial algorithmic and computational advances. FMM on GPUs for example won the Gordon Bell Price in 2009. However, while advances continued over the last ten years little work has been done to exploit Fast Multipole Methods beyond classical GPU computing. In particular, modern designs such as unified memory architectures on Apple Silicone and Nvidia Grace Hopper, mixed precision computations, or the use of tensor units in modern accelerators have received little attention.\nWe have started building up in our group our own FMM expertise as part of the Bempp project. Using Rust as main driver language we have developed a CPU based FMM that is highly portable and competitive with other established Fast Multipole libraries. We are currently porting this effort over to MPI based clusters. Most of this work has been part of the PhD thesis of an existing student in Betcke’s group.\n\n\nBased on our existing experience with our CPU based FMM implementation we want to expand to modern compute architectures. In particular, we have the following objects:\n\nDevelop optimised FMM implementations for unified memory architectures. Within the FMM the key drivers of computational cost are the Particle To Particle Interactions (P2P) and the Multipole 2 Locale Interactions (M2L). For a shallow computation tree P2P will dominate. For a deeper tree M2L will dominate. P2P especially benefits from computation on GPU cores. M2L operations can be accelerated on GPUs but often have lower compute intensity than P2P. If CPU and GPU share a unified address space and fast overall memory accesses many of the implementational problems of full GPU based FMM fall away and we can flexibly decide between work on CPU and on GPU cores. We want to exploit this to optimise new FMM implementations that make full use of unified memory architectures.\nExploit mixed precision arithmetic computations. For many applications it is enough for an FMM to deliver 6 to 7 digits of accuracy, which is just about in the limit of FP32 computations. However, naive implementation on FP32 often leads to fewer correct digits. We want to investigate which operations to run on FP64, and which to accelerate via FP32 while still being able to maintain sufficiently high accuracy for the FMM.\nOptimise for matrix-multiplication operations.. Many modern FMM formulation can be written in terms of matrix-matrix products, which can make use of tensor cores and other specialised registers for AI computations. We want to investigate how these can be used as part of the FMM workflow and allow speed-up of the overall computation.\n\n\n\n\nThe student will take time to familiarise themselves with our existing Rust code base, interface C++ to access accelerator devices and develop FMM on unified architectures. We therefore think the first part of FMM on unified architectures will take roughly two years of project time. In parallel the student will slowly get started on mixed precision experiments, and we expect this part of the project to be around one year in length, once the student has already assembled more experience on developing FMM. The final part on tensor and other AI accelerators will then build on the codebase and research experience with mixed precision arithmetic that the student has built up and will mainly take part in the last 1-1.5 years of the PhD.\n\n\n\nWe are currently preparing for release our first Rust based FMM version. The student will build on this code and integrate compute kernels for accelerators within this code base. Rust itself has limited support for GPU compute. So much of this work will be in C++ and interfaced to the Rust driver codes for the FMM. A particular interest is also to use Apple Silicone as unified memory environment for FMM. While not being used directly in HPC, Apple Silicon is widely spread and a cost effective way to develop unified memory codes. For HPC we will target Nvidia’s Grace Hopper, meaning the student will need to write separate low-level kernel implementations for Apple Metal and Nvidia Cuda. For our current FMM codes we use a BSD 3-Clause license and will continue this license for this project."
  },
  {
    "objectID": "phd_projects/entries/Deisenroth_dataassimilation.html",
    "href": "phd_projects/entries/Deisenroth_dataassimilation.html",
    "title": "Machine Learning for Low-Cost Data Assimilation",
    "section": "",
    "text": "This project is about a machine learning approach to data assimilation. Specifically, we use message passing algorithms to infer a posterior distribution on the (weather) state given some observations. This would be a perspective on data assimilation that is different to currently used methods, such as 3DVar or inference in Gauss-Markov random fields using INLA. We have done some preliminary work in this space with encouraging results that are similar to 3DVar in terms of speed and accuracy. Currently, our results are limited to the spatial setting, and we only estimate the mean of the latent field.\n\n\n\nThe project will extend our previous work to include meaningful (marginal) uncertainty estimates plus the extension to the spatio-temporal setting where 4DVar and Ensemble Kalman Filters are the state of the art. In combination with a machine-learning model that faithfully emulates the numerical weather prediction model, we will be able to quickly arrive at a data assimilation solution that is a) parallelizable, b) distributed in computation, c) yields meaningful uncertainty estimates, d) has a small memory footprint, e) can be implemented on specialized hardware, such as Graphcores IPUs. Our approach is general in the sense that it can be applied to various areas, such as weather/climate, oceans or nuclear fusion.\n\n\n\nOur implementation is currently in JAX (including sparse linear algebra) and works on GPU and CPU. We will continue to develop our software to support multi-GPU systems. In terms of data, we currently use publicly available data. By the end of the project, software will be open-sourced and easy to use."
  },
  {
    "objectID": "phd_projects/entries/Deisenroth_dataassimilation.html#project-description",
    "href": "phd_projects/entries/Deisenroth_dataassimilation.html#project-description",
    "title": "Machine Learning for Low-Cost Data Assimilation",
    "section": "",
    "text": "This project is about a machine learning approach to data assimilation. Specifically, we use message passing algorithms to infer a posterior distribution on the (weather) state given some observations. This would be a perspective on data assimilation that is different to currently used methods, such as 3DVar or inference in Gauss-Markov random fields using INLA. We have done some preliminary work in this space with encouraging results that are similar to 3DVar in terms of speed and accuracy. Currently, our results are limited to the spatial setting, and we only estimate the mean of the latent field.\n\n\n\nThe project will extend our previous work to include meaningful (marginal) uncertainty estimates plus the extension to the spatio-temporal setting where 4DVar and Ensemble Kalman Filters are the state of the art. In combination with a machine-learning model that faithfully emulates the numerical weather prediction model, we will be able to quickly arrive at a data assimilation solution that is a) parallelizable, b) distributed in computation, c) yields meaningful uncertainty estimates, d) has a small memory footprint, e) can be implemented on specialized hardware, such as Graphcores IPUs. Our approach is general in the sense that it can be applied to various areas, such as weather/climate, oceans or nuclear fusion.\n\n\n\nOur implementation is currently in JAX (including sparse linear algebra) and works on GPU and CPU. We will continue to develop our software to support multi-GPU systems. In terms of data, we currently use publicly available data. By the end of the project, software will be open-sourced and easy to use."
  },
  {
    "objectID": "phd_projects/entries/CotterKalise_Fieldgames.html",
    "href": "phd_projects/entries/CotterKalise_Fieldgames.html",
    "title": "A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games",
    "section": "",
    "text": "Colin Cotter is Professor of Computational Mathematics at Imperial College. He has relevant interests in optimal transport applied to the semigeostrophic equations and to weather forecast verification, and in design, analysis and implementation of finite element methods.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed variational methods for mean field games and control which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nStarting from the fluid dynamics formulation of the Monge-Kantorovich mass transfer problem proposed by Benamou and Brenier, it is now well-understood that a wide class of problems including Optimal Transport (OT), Schroedinger Bridges (SB), Mean Field Control (MFC), and Variational Mean Field Games (MFG), can be seen as the solution of a PDE-constrained optimization problem where a convex cost if constrained to a continuity equation. While the numerical approximation of these problems has been extensively studied over the last decade due to their importance in statistical machine learning, computational methods are often developed for a particular problem of interest and fail to identified the underlying unifying structure.\nIn this project we will develop a unified solver for OT, SB, MFC and MFG, so that each instance arises after a suitable assignment of costs and constraints. The solver will be based on convex optimization methods (primal-dual algorithms), preconditioning, and structure-preserving discretizations of the continuity equation.\n\n\n\nSoftware deliverables will include: - A unified OT/SB/MFC/MFG solver which is not currently available. This will be built around Firedrake, building upon the work of Natale and Todeschi.\nNatale, Andrea, and Gabriele Todeschi. “A mixed finite element discretization of dynamical optimal transport.” Journal of Scientific Computing 91, no. 2 (2022): 38."
  },
  {
    "objectID": "phd_projects/entries/CotterKalise_Fieldgames.html#project-description",
    "href": "phd_projects/entries/CotterKalise_Fieldgames.html#project-description",
    "title": "A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games",
    "section": "",
    "text": "Colin Cotter is Professor of Computational Mathematics at Imperial College. He has relevant interests in optimal transport applied to the semigeostrophic equations and to weather forecast verification, and in design, analysis and implementation of finite element methods.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed variational methods for mean field games and control which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\nStarting from the fluid dynamics formulation of the Monge-Kantorovich mass transfer problem proposed by Benamou and Brenier, it is now well-understood that a wide class of problems including Optimal Transport (OT), Schroedinger Bridges (SB), Mean Field Control (MFC), and Variational Mean Field Games (MFG), can be seen as the solution of a PDE-constrained optimization problem where a convex cost if constrained to a continuity equation. While the numerical approximation of these problems has been extensively studied over the last decade due to their importance in statistical machine learning, computational methods are often developed for a particular problem of interest and fail to identified the underlying unifying structure.\nIn this project we will develop a unified solver for OT, SB, MFC and MFG, so that each instance arises after a suitable assignment of costs and constraints. The solver will be based on convex optimization methods (primal-dual algorithms), preconditioning, and structure-preserving discretizations of the continuity equation.\n\n\n\nSoftware deliverables will include: - A unified OT/SB/MFC/MFG solver which is not currently available. This will be built around Firedrake, building upon the work of Natale and Todeschi.\nNatale, Andrea, and Gabriele Todeschi. “A mixed finite element discretization of dynamical optimal transport.” Journal of Scientific Computing 91, no. 2 (2022): 38."
  },
  {
    "objectID": "phd_projects/entries/Boulle_Koopman.html",
    "href": "phd_projects/entries/Boulle_Koopman.html",
    "title": "Koopman operator learning of nonlinear PDEs",
    "section": "",
    "text": "The aim of this project is to learn nonlinear partial differential equations (PDEs) from data using techniques based on Koopman operators and dynamic mode decomposition.\n\n\nPDE learning, which involves discovering the underlying partial differential equations governing complex physical systems from data, has become an increasingly active research area in scientific machine learning, primarily relying on neural network approximation algorithms. These methods leverage the representational flexibility of neural networks to approximate complex functions and have shown promising results in various applications [1]. However, extracting physical insights from these learned models and establishing rigorous theoretical guarantees remains challenging due to the highly nonlinear nature of neural networks. A major open problem in this field is the development of tools and theories for nonlinear PDEs.\nOn the other hand, Koopman operator theory presents an alternative approach to analysing nonlinear dynamical systems. The Koopman operator provides a global, linear representation of a nonlinear system by acting on an infinite-dimensional space of observables. This linearization makes it possible to analyse nonlinear systems using tools traditionally reserved for linear systems, such as spectral analysis and operator theory techniques [2,3]. The Koopman framework has shown significant potential in applications such as control theory, fluid dynamics, and signal processing.\nHowever, the Koopman operator framework has been developed predominantly for discrete-time dynamical systems and ordinary differential equations (ODEs). Its extension to continuous-time systems, and in particular to nonlinear PDEs, remains an open and underexplored area of research. The challenge lies in bridging the gap between the theoretical foundation of Koopman operators and their practical application to the highly complex dynamics of PDEs. Addressing this gap will unlock new possibilities for analysing, controlling, and predicting the behaviour of nonlinear PDE systems through a data-driven, operator-theoretic lens.\n[1] N. Boulle and A. Townsend, “A Mathematical Guide to Operator Learning”, https://arxiv.org/abs/2312.14688. [2] M. J. Colbrook, “The Multiverse of Dynamic Mode Decomposition Algorithms”, https://arxiv.org/abs/2312.00137. [3] S. L. Brunton, M. Budisic, E. Kaiser, J. N. Kutz, “Modern Koopman Theory for Dynamical Systems”, https://doi.org/10.1137/21M1401243=\n\n\n\nThis project aims to achieve the following objectives: 1. Generalizing the Koopman operator framework to infinitely large system of ODEs and nonlinear PDEs 2. Developing the first theory for nonlinear PDE learning based on Koopman operators 3. Implementing new algorithms for approximating Koopman operators and applying them to fluid dynamics problems\n\n\n\nA large component of this project will involve the implementation of new data-driven algorithms for learning and computing spectral properties associated with PDEs. The resulting software will be made publicly available in the form of a package with a detailed documentation to facilitate its adoption among the scientific machine learning community."
  },
  {
    "objectID": "phd_projects/entries/Boulle_Koopman.html#project-description",
    "href": "phd_projects/entries/Boulle_Koopman.html#project-description",
    "title": "Koopman operator learning of nonlinear PDEs",
    "section": "",
    "text": "The aim of this project is to learn nonlinear partial differential equations (PDEs) from data using techniques based on Koopman operators and dynamic mode decomposition.\n\n\nPDE learning, which involves discovering the underlying partial differential equations governing complex physical systems from data, has become an increasingly active research area in scientific machine learning, primarily relying on neural network approximation algorithms. These methods leverage the representational flexibility of neural networks to approximate complex functions and have shown promising results in various applications [1]. However, extracting physical insights from these learned models and establishing rigorous theoretical guarantees remains challenging due to the highly nonlinear nature of neural networks. A major open problem in this field is the development of tools and theories for nonlinear PDEs.\nOn the other hand, Koopman operator theory presents an alternative approach to analysing nonlinear dynamical systems. The Koopman operator provides a global, linear representation of a nonlinear system by acting on an infinite-dimensional space of observables. This linearization makes it possible to analyse nonlinear systems using tools traditionally reserved for linear systems, such as spectral analysis and operator theory techniques [2,3]. The Koopman framework has shown significant potential in applications such as control theory, fluid dynamics, and signal processing.\nHowever, the Koopman operator framework has been developed predominantly for discrete-time dynamical systems and ordinary differential equations (ODEs). Its extension to continuous-time systems, and in particular to nonlinear PDEs, remains an open and underexplored area of research. The challenge lies in bridging the gap between the theoretical foundation of Koopman operators and their practical application to the highly complex dynamics of PDEs. Addressing this gap will unlock new possibilities for analysing, controlling, and predicting the behaviour of nonlinear PDE systems through a data-driven, operator-theoretic lens.\n[1] N. Boulle and A. Townsend, “A Mathematical Guide to Operator Learning”, https://arxiv.org/abs/2312.14688. [2] M. J. Colbrook, “The Multiverse of Dynamic Mode Decomposition Algorithms”, https://arxiv.org/abs/2312.00137. [3] S. L. Brunton, M. Budisic, E. Kaiser, J. N. Kutz, “Modern Koopman Theory for Dynamical Systems”, https://doi.org/10.1137/21M1401243=\n\n\n\nThis project aims to achieve the following objectives: 1. Generalizing the Koopman operator framework to infinitely large system of ODEs and nonlinear PDEs 2. Developing the first theory for nonlinear PDE learning based on Koopman operators 3. Implementing new algorithms for approximating Koopman operators and applying them to fluid dynamics problems\n\n\n\nA large component of this project will involve the implementation of new data-driven algorithms for learning and computing spectral properties associated with PDEs. The resulting software will be made publicly available in the form of a package with a detailed documentation to facilitate its adoption among the scientific machine learning community."
  },
  {
    "objectID": "phd_projects/entries/pavliotis_kalise.html",
    "href": "phd_projects/entries/pavliotis_kalise.html",
    "title": "Optimal control methods for agent-based models",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed optimal control methods for high-dimensional problems which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\n• Agent-based models (ABM), often described in terms of systems of interacting diffusions (multidimensional stochastic differential equations) arise in many applications, including mathematical biology, e.g. models for chemotaxis, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. Such systems often exhibit interesting collective behaviour as a result of the interaction between agents. Several different macroscopic configurations of the ABM are possible, for different choices of the parameters in the system, such as the interaction strength. The transition between different macroscopic configurations can have dramatic effects on the behaviour of the ABM. The goal of the proposed project is to develop efficient control methodologies for steering the ABM dynamics towards desired macroscopic configurations. Different approaches, such as optimal control for PDEs and data driven approaches based on model predictive control will be developed.\n\n\n\nSoftware deliverables will include:\n\nAn ABM simulator for trajectory generation. To date, there’s no universal standard/benchmarks for ABM simulation, despite a generic structure based on the interaction forces in the system. We will develop a trajectory simulator which is fundamental for the study of data-driven methods and for optimisation and control purposes. Such simulator will provide sufficient freedom for prescribing interaction forces, thus being useful for different applications including opinion dynamics, swarm robotics, and pedestrian motion, among others. The simulator will be constructed using state-of-the-art methods for accurate and fast approximation of large-scale SDEs.\nAn optimal control toolbox for ABM. There are several optimal control solvers readily available for traditional control engineering applications (robotics, power electronics), however, none of them scale to high-dimensional settings which are natural in ABMs. Moreover, they do not exploit model structure and eventually resort to treating the optimal control problem as a large-scale, model-free, nonlinear optimization problem. Instead, we will develop a toolbox for control of ABMs based on adjoint calculus, making extensive use of the particular model structure of ABMs and circumventing calls to external nonlinear optimization solvers."
  },
  {
    "objectID": "phd_projects/entries/pavliotis_kalise.html#project-description",
    "href": "phd_projects/entries/pavliotis_kalise.html#project-description",
    "title": "Optimal control methods for agent-based models",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\nDante Kalise is Reader in Optimisation and Control at the Department of Mathematics at Imperial College. His main research interests involve scientific computation and machine learning, optimal control, high-dimensional approximation and agent-based models across scales. He has developed optimal control methods for high-dimensional problems which constitute the basis of this project, and developed applications in swarm robotics, pedestrian motion, and global optimization.\n\n\n\n• Agent-based models (ABM), often described in terms of systems of interacting diffusions (multidimensional stochastic differential equations) arise in many applications, including mathematical biology, e.g. models for chemotaxis, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. Such systems often exhibit interesting collective behaviour as a result of the interaction between agents. Several different macroscopic configurations of the ABM are possible, for different choices of the parameters in the system, such as the interaction strength. The transition between different macroscopic configurations can have dramatic effects on the behaviour of the ABM. The goal of the proposed project is to develop efficient control methodologies for steering the ABM dynamics towards desired macroscopic configurations. Different approaches, such as optimal control for PDEs and data driven approaches based on model predictive control will be developed.\n\n\n\nSoftware deliverables will include:\n\nAn ABM simulator for trajectory generation. To date, there’s no universal standard/benchmarks for ABM simulation, despite a generic structure based on the interaction forces in the system. We will develop a trajectory simulator which is fundamental for the study of data-driven methods and for optimisation and control purposes. Such simulator will provide sufficient freedom for prescribing interaction forces, thus being useful for different applications including opinion dynamics, swarm robotics, and pedestrian motion, among others. The simulator will be constructed using state-of-the-art methods for accurate and fast approximation of large-scale SDEs.\nAn optimal control toolbox for ABM. There are several optimal control solvers readily available for traditional control engineering applications (robotics, power electronics), however, none of them scale to high-dimensional settings which are natural in ABMs. Moreover, they do not exploit model structure and eventually resort to treating the optimal control problem as a large-scale, model-free, nonlinear optimization problem. Instead, we will develop a toolbox for control of ABMs based on adjoint calculus, making extensive use of the particular model structure of ABMs and circumventing calls to external nonlinear optimization solvers."
  },
  {
    "objectID": "phd_projects/entries/Pearce_bacterialbiofilms.html",
    "href": "phd_projects/entries/Pearce_bacterialbiofilms.html",
    "title": "Data-driven multi-scale modelling of bacterial biofilms",
    "section": "",
    "text": "Bacterial biofilms are communities of bacterial cells embedded in extracellular matrix, and are the most common form of bacterial life. Numerous software packages exist for performing cell-based simulation of bacterial biofilms and synthetic bacterial communities (e.g. iDynoMiCS, BSim, gro). However, such software does not typically allow for flexibility in the properties of extracellular matrix proteins, which we have found in previous work to provide both structural integrity (Hartmann et al., Nat. Phys., 2019; Pearce et al., Phys. Rev. Lett. 2019) and protection from antibiotics (Böhning et al., Nat. Comm., 2023) in bacterial biofilms. There is therefore a crucial gap in the software landscape, which limits our ability to understand and test fundamentally how bacterial communities respond to antibiotic treatment in a range of conditions. Recent experimental data generated by our collaborators has revealed the structure, chemical properties and spatial arrangement of the molecules that make up the extracellular matrix in biofilms, providing an opportunity to close this gap through data-driven coarse-grained models and codes for biomolecular dynamics within bacterial biofilms.\n\n\n\nIn this project, we will develop a computational workflow to take in molecular-level detail about matrix proteins and output coarse-grained models of bacterial biofilms that account for matrix properties. We will realise the following objectives:\n\nDevelop a workflow to take in experimental data on the molecular structure and properties of matrix proteins and output coarse-grained molecular properties for use in cell-based simulations, validated where possible by mesoscopic experimental data such as phase diagrams.\nPerform simulations of bacterial biofilms including such extracellular matrix proteins and explore their effect on antitbiotic treatment, validated by data on molecular organisation and antibiotic transport in biofilms.\nIntegrate the workflow and code into new and/or existing software so that molecular data can be quickly converted into new coarse-grained particles in cell-based models.\n\n\n\n\n\nSoftware to convert molecular data to coarse-grained models.\nSoftware to include the microscopic properties of matrix proteins in cell-based models."
  },
  {
    "objectID": "phd_projects/entries/Pearce_bacterialbiofilms.html#project-description",
    "href": "phd_projects/entries/Pearce_bacterialbiofilms.html#project-description",
    "title": "Data-driven multi-scale modelling of bacterial biofilms",
    "section": "",
    "text": "Bacterial biofilms are communities of bacterial cells embedded in extracellular matrix, and are the most common form of bacterial life. Numerous software packages exist for performing cell-based simulation of bacterial biofilms and synthetic bacterial communities (e.g. iDynoMiCS, BSim, gro). However, such software does not typically allow for flexibility in the properties of extracellular matrix proteins, which we have found in previous work to provide both structural integrity (Hartmann et al., Nat. Phys., 2019; Pearce et al., Phys. Rev. Lett. 2019) and protection from antibiotics (Böhning et al., Nat. Comm., 2023) in bacterial biofilms. There is therefore a crucial gap in the software landscape, which limits our ability to understand and test fundamentally how bacterial communities respond to antibiotic treatment in a range of conditions. Recent experimental data generated by our collaborators has revealed the structure, chemical properties and spatial arrangement of the molecules that make up the extracellular matrix in biofilms, providing an opportunity to close this gap through data-driven coarse-grained models and codes for biomolecular dynamics within bacterial biofilms.\n\n\n\nIn this project, we will develop a computational workflow to take in molecular-level detail about matrix proteins and output coarse-grained models of bacterial biofilms that account for matrix properties. We will realise the following objectives:\n\nDevelop a workflow to take in experimental data on the molecular structure and properties of matrix proteins and output coarse-grained molecular properties for use in cell-based simulations, validated where possible by mesoscopic experimental data such as phase diagrams.\nPerform simulations of bacterial biofilms including such extracellular matrix proteins and explore their effect on antitbiotic treatment, validated by data on molecular organisation and antibiotic transport in biofilms.\nIntegrate the workflow and code into new and/or existing software so that molecular data can be quickly converted into new coarse-grained particles in cell-based models.\n\n\n\n\n\nSoftware to convert molecular data to coarse-grained models.\nSoftware to include the microscopic properties of matrix proteins in cell-based models."
  },
  {
    "objectID": "partner_workshop.html",
    "href": "partner_workshop.html",
    "title": "Inaugural Partner Workshop",
    "section": "",
    "text": "Our first partner workshop is taking place 25 April 2025 at Imperial College London.\nLocation: Imperial South Kensington Campus, Skempton 64 64A & 64B"
  },
  {
    "objectID": "partner_workshop.html#schedule",
    "href": "partner_workshop.html#schedule",
    "title": "Inaugural Partner Workshop",
    "section": "Schedule",
    "text": "Schedule\n\n09:30 – 10:15 Networking\n10:15 - 10:30 CCMI Intro, Timo Betcke & Colin Cotter\n10:30 - 10:45 GAMS, Michael Bussieck\n10:45 – 11:00 FICO, Tristan Gally\n11:00 – 11:15 DataSparq, Jeremy Bradley\n11:15 – 11:35 SLB, Joshua Knowles\n11:35 – 11:50 McLaren, Julien Hoessler\n11:50 – 12:05 Graphcore, Andrew Fitzgibbon\n12:05 – 12:20 DELL, Paul Brook\n12:30 - 13:30 Lunch\n13:30 - 14:20 Poster Session\n14:20 – 14:30 RFI, Mark Basham\n14:30 – 14:40 Crick, James Briscoe\n14:40 – 14:50 ATI, Martin O’Reilly\n14:50 – 15:00 BASF, Rosona Eldred\n15:00 – 15:10 Gurobi, Lindsay Montanari\n15:10 – 15:20 SURF, Marco Verdicchio\n15:20 – 15:30 Enthought, Alexandre Chabot-Leclerc\n15:30 – 15:40 Rapiscan, William Thompson\n15:40 – 16:00 Summary, Marta Betcke/Ruth Misener"
  },
  {
    "objectID": "events/events.html",
    "href": "events/events.html",
    "title": "Events",
    "section": "",
    "text": "Networking with UKAEA\n                10 September 2024\n                \n                Fusion research is a complex multidisciplinary problem, ranging across fundamental science, engineering, computational modelling, and the data sciences. In this network event we discuss future collaborations between the CCMI CDT and UKAEA on the interface of exciting computational modelling problems, large-scale data science and AI, and cutting edge research software engineering.\n            \n                \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/nobel_prizes.html",
    "href": "blog/posts/nobel_prizes.html",
    "title": "Nobel Prizes in Chemistry and Physics for AI research",
    "section": "",
    "text": "London is the world center in AI research. This has been impressively demonstrated this year by the award of two Nobel Prizes to members of the UCL Community. Professor Geoffrey Hinton, founder of the Gatsby Computational Neuroscience Unit at UCL, was awarded the Physics Nobel Prize alongside John J Hopfield for breakthroughs in AI research.\nSir Demis Hassabis the Co-Founder and CEO of Google Deepmind, who completed his PhD at UCL and was a postdoc in the Gatsby Neuroscience Unit, was awarded the Nobel Prize in Chemistry along with John Jumper and Professor David Baker for their work on computational protein design and protein structure prediction.\nRead the UCL News Article for the full story of these remarkable researchers."
  },
  {
    "objectID": "training/thesis.html",
    "href": "training/thesis.html",
    "title": "Research thesis",
    "section": "",
    "text": "The four year research thesis will lead to a PhD degree from either UCL or Imperial College.\nAs part of the interview process students choose an available research project either at UCL or at Imperial College.\nThesis topics include the UCL Departments of Mathematics, Statistics, and Computer Science, and the Imperial Departments of Mathematics and Computing.\nFor a continuously updated selection of potential thesis topics please browse our collection of PhD projects."
  },
  {
    "objectID": "training/software_journey.html",
    "href": "training/software_journey.html",
    "title": "The Software Journey",
    "section": "",
    "text": "The software journey has been designed to give every CCMI student training to become a world class research software engineer. Students will collaborate on a diverse range of open-source software projects, some suggested by students themselves, others by academics or industrial partners. Students will learn how to focus ideas, develop minimum viable products, pitch ideas to stakeholders, and collaborate as teams. This will be supported by a dedicated termly software week with software sprints, progress talks, external seminars delivered by partners, and other activities.\nThe software journey is an ongoing activity throughout the whole four year training programme. Every CDT student is expected to actively contribute to the software journey and develop their skills as professional research software engineers."
  },
  {
    "objectID": "training/training.html",
    "href": "training/training.html",
    "title": "The CCMI Training Programme",
    "section": "",
    "text": "The CCMI graduate training programme rests on three foundations.\n\nA four year research thesis on the interface of computational sciences, research software engineering, and data sciences.\nCollaborative interface working groups, which are week long block events that combine topical overviews, research discussions, software exploration, and discovery of new ideas.\nA software journey that involves active collaboration in open-source projects, close collaboration with external partners, regular software weeks, sprint sessions, industry talks on software engineering, and many other activities."
  },
  {
    "objectID": "training/working_groups.html",
    "href": "training/working_groups.html",
    "title": "Collaborative interface working groups (CIWGs)",
    "section": "",
    "text": "Collaborative interface working groups (CIWGs) deliver the core topical training programme that every CCCMI student goes through.\nThey are organised in the form of week long cohort workships that cover an introduction to research topics, research and software landscape overviews, exploration of current research challenges, and a discovery phase for future trends and new directions.\n\n\n\nStructure of collaborative interface working groups\n\n\nIn contrast to traditional lectures these working groups are centered around learning how to be an effective researcher and will contain significant student led components.\nCIWGs are delivered across the first two years in the following topic areas:\n\nModern programming models for scientific computing\nSoftware engineering fundamentals\nData intensive computations\nPDE Discretisations\nDeep learning in theory and practice\nResponsible computational modelling\nHigh dimensional problems and optimisation"
  },
  {
    "objectID": "blog/blog_list.html",
    "href": "blog/blog_list.html",
    "title": "CCMI Blog",
    "section": "",
    "text": "Posts\n\n\n        \n            \n            \n                Nobel Prizes in Chemistry and Physics for AI research\n                \n                The 2024 nobel prizes in Physics and Chemistry were given out to breakthrough research in AI from members of the UCL community.\n            \n                \n        \n\n        \n            \n            \n                New homepage.\n                \n                Our new homepage is now live.\n            \n                \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/blogpost1.html",
    "href": "blog/posts/blogpost1.html",
    "title": "New homepage.",
    "section": "",
    "text": "Our brand-new homepage is now live. There may still be minor issues and features are continuously being added."
  },
  {
    "objectID": "application_procedure.html",
    "href": "application_procedure.html",
    "title": "Application Procedure",
    "section": "",
    "text": "The application procedure consists of the following four steps.\n1.) The online application. We ask you to submit an application to our online submission system. Based on your application we will make a first sift of the candidates and invite shortlisted candidates for interview.\n2.) The Interview. For the interview we will ask you to prepare a short presentation and answer a mixture of questions covering technical background and your motivation to join the CDT. The interview questions will be communicated to applicants beforehand so that they have sufficient time to prepare for the technical and non-technical part.\n3.) The project matching. Once you pass the interview we aim to match you with a supervisor. We will prepare an online info session about the different research areas of the CDT and you have opportunities to ask questions and discuss project choices. Your project will decide wether you will be enrolled at UCL or Imperial College. Every student can submit three preferred project choices. We will circulate your information after the interview stage with the corresponding supervisors and you can also discuss the projects directly with them. If both, you and the supervisor come together for one of your preferred projects you are matched and you proceed to the offer stage. If none of your three preferred projects leads to a match we will suggest another project for you based on your background. Should you accept you will proceed to the offer stage. If no match with a project/supervisor is possible you will not receive an offer.\n4.) The offer stage. You will be asked to submit a pro-forma application to either UCL or Imperial depending on your project match. This is only to make a formal document check about information you have submitted beforehand and to receive a formal offer from your chosen university on successful document check. The offer may be conditional on language test and the outcome of your current degree if you haven’t finished your previous studies at the time of application to the CDT."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n\t\tEPSRC Centre for Doctoral Training \n\t\t\n\t\t in \n\t\tCollaborative Computational Modelling at the Interface \n\t\t\t\n\t\t\n\t",
    "section": "",
    "text": "Blog Posts\n\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                    \n                        \n                                        \n                                New homepage.\n                            \n                            Our new homepage is now live.\n                        \n                    \n                \n                    \n                \n            \n                \n                    \n                    \n                        \n                                        \n                                Nobel Prizes in Chemistry and Physics for AI research\n                            \n                            The 2024 nobel prizes in Physics and Chemistry were given out to breakthrough research in AI from members of the UCL community.\n                        \n                    \n                \n                    \n                \n            \n\n            \n                \n                Previous\n            \n            \n                \n                Next\n            \n        \n    \n    \n\nNo matching items\n\n\n\n\n    \n        \n        \n            CCMI offers a fully funded 4 year PhD programme at either UCL or Imperial College\n        \n    \n    \n        \n        \n            We offer a bespoke cohort training programme consisting of dedicated block workshops, software weeks, summer schools, and many other activities\n        \n    \n    \n        \n        \n            The first intake starts in Sept. 2025 (call for applications to start in autumn 2024)\n        \n    \n    \n        \n        \n            We will provide research opportunities across computational mathematics, scientific algorithms, high-performance computing, research software engineering, data sciences, and applications\n        \n    \n    \n        \n        \n            We have external partners ranging from innovative SMEs to world-leading HPC labs\n        \n    \n    \n\n\nNo matching items\n\n\n\nEvents\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                        \n                            \n                                \n                                    Networking with UKAEA \n                                10 September 2024\n                                    \n                                A networking opportunity to discuss collaborative research projects between UKAEA and the CCMI CDT.\n                            \n                        \n                    \n                    \n                \n            \n\n        \n\n        \n            \n            Previous\n        \n        \n            \n            Next\n        \n    \n\n\n\nNo matching items\n\n\n\n\n\nEvents\n\n    \n        \n            \n            \n            \n        \n\n        \n\n            \n            \n                \n                    \n                        \n                            \n                                \n                                    Networking with UKAEA \n                                10 September 2024\n                                    \n                                A networking opportunity to discuss collaborative research projects between UKAEA and the CCMI CDT.\n                            \n                        \n                    \n                    \n                \n            \n\n        \n\n        \n            \n            Previous\n        \n        \n            \n            Next\n        \n    \n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCCMI is a collaboration between University College London and Imperial College London to offer a novel and innovative PhD training programme at the interface of computational modelling, data sciences, and research software engineering.\n\n\n\n\n\n\n\nStudents will join world-class research groups across UCL and Imperial, and will benefit from a bespoke training programme around research software engineering and its applications in the computational and data sciences.\n\n\n\n\n\n Previous\n\n\n Next\n\n\n\n\n\n\nKEY FACTS\n\n\n\n    \n        \n        \n            CCMI offers a fully funded 4 year PhD programme at either UCL or Imperial College\n        \n    \n    \n        \n        \n            We offer a bespoke cohort training programme consisting of dedicated block workshops, software weeks, summer schools, and many other activities\n        \n    \n    \n        \n        \n            The first intake starts in Sept. 2025 (call for applications to start in autumn 2024)\n        \n    \n    \n        \n        \n            We will provide research opportunities across computational mathematics, scientific algorithms, high-performance computing, research software engineering, data sciences, and applications\n        \n    \n    \n        \n        \n            We have external partners ranging from innovative SMEs to world-leading HPC labs\n        \n    \n    \n\n\nNo matching items\n\n\n\n\n\n\n\n\nInterested?\n\n\n Sign Up Now"
  },
  {
    "objectID": "phd_projects/phd_project_list.html",
    "href": "phd_projects/phd_project_list.html",
    "title": "PhD Projects",
    "section": "",
    "text": "The following projects are advertised for 2025 entry into the CDT.\n \n\n\n        \n            \n            \n                Optimal control methods for agent-based models\n                Imperial Mathematics\n                \n                Principal advisor: Prof Greg Pavliotis and Dr Dante Kalise\n            \n                \n        \n\n        \n            \n            \n                Differentiable probabilistic deep learning with generative denoising diffusion models\n                UCL MSSL\n                \n                Principal advisor: Prof Jason McEwen\n            \n                \n        \n\n        \n            \n            \n                UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection\n                UCL Advanced Research Computing\n                \n                Principal advisor: Christian Gutschow\n            \n                \n        \n\n        \n            \n            \n                Deep learning with symmetry\n                Imperial Mathematics\n                \n                Principal advisor: Prof Jeroen Lamb\n            \n                \n        \n\n        \n            \n            \n                Aligned diffusion models for machine learning\n                Imperial Mathematics\n                \n                Principal advisor: Dr Felipe Tobar\n            \n                \n        \n\n        \n            \n            \n                High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials\n                UCL Mechanical Engineering\n                \n                Principal advisor: Dr Reza Haqshenas\n            \n                \n        \n\n        \n            \n            \n                Large-scale high-performance solver for therapeutic ultrasound applications\n                UCL UCL Division of Surgery and Interventional Science\n                \n                Principal advisor: Dr Pierre Gélat\n            \n                \n        \n\n        \n            \n            \n                Backpropagation through rough differential equations\n                Imperial Mathematics & Imperial-X\n                \n                Principal advisor: Dr Christopher Salvi\n            \n                \n        \n\n        \n            \n            \n                Optimization methods for high-dimensional reinforcement learning\n                Imperial Mathematics\n                \n                Principal advisor: Dr Yufei Zhang\n            \n                \n        \n\n        \n            \n            \n                Data-driven preconditioning for parallel-in-time PDE solvers\n                Imperial Mathematics\n                \n                Principal advisor: Prof Colin Cotter\n            \n                \n        \n\n        \n            \n            \n                AI code generation and numerical codes\n                UCL ARC\n                \n                Principal advisor: Prof James Hetherington\n            \n                \n        \n\n        \n            \n            \n                Model correlation and the version control tree\n                UCL ARC\n                \n                Principal advisor: Prof James Hetherington\n            \n                \n        \n\n        \n            \n            \n                Data-driven multi-scale modelling of bacterial biofilms\n                UCL Mathematics\n                \n                Principal advisor: Dr. Philip Pearce\n            \n                \n        \n\n        \n            \n            \n                Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)\n                UCL Medical Physics and Biomedical Engineering\n                \n                Principal advisor: Prof Ben Cox\n            \n                \n        \n\n        \n            \n            \n                Koopman operator learning of nonlinear PDEs\n                Imperial Mathematics\n                \n                Principal advisor: Dr.Nicholas Boulle\n            \n                \n        \n\n        \n            \n            \n                A Unified Solver for Optimal Transport, Schroedinger Bridges, and Variational Mean Field Games\n                Imperial Mathematics\n                \n                Principal advisor: Prof Colin Cotter and Dr. Dante Kalise\n            \n                \n        \n\n        \n            \n            \n                A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems\n                Imperial Mathematics\n                \n                Principal advisor: Dr.Cristopher Salvi\n            \n                \n        \n\n        \n            \n            \n                Machine Learning for Low-Cost Data Assimilation\n                UCL Computer Science\n                \n                Principal advisor: Prof Marc Deisenroth\n            \n                \n        \n\n        \n            \n            \n                Optimal transport for probabilistic machine learning\n                Imperial Mathematics\n                \n                Principal advisor: Dr Felipe Tobar\n            \n                \n        \n\n        \n            \n            \n                Fast Multipole Methods on modern architectures\n                UCL Maths and Advanced Research Computing Centre\n                \n                Principal advisor: Prof Timo Betcke\n            \n                \n        \n\n        \n            \n            \n                Inference of gene regulatory networks from single cell data\n                Imperial Mathematics\n                \n                Principal advisor: Prof Vahid Shahrezaei and Dr. Philipp Thomas \n            \n                \n        \n\n        \n            \n            \n                Re-griding the TROVE nuclear motion program\n                UCL Physics and Astronomy\n                \n                Principal advisor: Prof Jonathan Tennyson\n            \n                \n        \n\n        \n            \n            \n                Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems\n                UCL Advanced Research Computing Centre\n                \n                Principal advisor: Dr Matt Graham\n            \n                \n        \n\n        \n            \n            \n                Agent-based simulation of tissue self-organization\n                Imperial Mathematics\n                \n                Principal advisor: Dr.Omer Karin\n            \n                \n        \n\n        \n            \n            \n                Robust Bayesian Inference at Scale\n                UCL Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                Efficient Computation of Transition States in Molecular Dynamics\n                Imperial Computing\n                \n                Principal advisor: Dr. Panos Parpas\n            \n                \n        \n\n        \n            \n            \n                Time Reversal Imaging and Learned Physics with Neural ODEs\n                UCL Medical Physics and Biomedical Engineering\n                \n                Principal advisor: Prof Ben Cox\n            \n                \n        \n\n        \n            \n            \n                Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems\n                UCL Chemistry and Advanced Research Computing\n                \n                Principal advisor: Prof Peter V. Coveney\n            \n                \n        \n\n        \n            \n            \n                Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach\n                UCL Mathematics\n                \n                Principal advisor: Prof Hao Ni\n            \n                \n        \n\n        \n            \n            \n                All-at-once deep learning methods for nonlinear PDE based inverse problems\n                UCL Computer Science\n                \n                Principal advisor: Prof Marta Betcke\n            \n                \n        \n\n        \n            \n            \n                Developing Quasi-Geostrophic Coupled Ocean–Atmosphere Model\n                Imperial Mathematics\n                \n                Principal advisor: Prof Pavel Berloff\n            \n                \n        \n\n        \n            \n            \n                Improving impaired hearing through sound reconstruction from neural activity patterns\n                UCL Computer Science\n                \n                Principal advisor: Prof Martin Benning\n            \n                \n        \n\n        \n            \n            \n                Inference and inverse problems for stochastic interacting particle systems\n                Imperial Mathemetics\n                \n                Principal advisor: Prof Greg A. Pavliotis\n            \n                \n        \n\n        \n            \n            \n                HemeLB at the exascale and beyond: Implementing and studying whole human scale cardiovascular hemodynamics on the world’s most powerful supercomputers\n                UCL Chemistry and Advanced Research Computing\n                \n                Principal advisor: Prof Peter V. Coveney\n            \n                \n        \n\n        \n            \n            \n                PDE-driven/data-driven hybrid modelling for data assimilation\n                Imperial Mathematics\n                \n                Principal advisor: Prof Colin Cotter\n            \n                \n        \n\n        \n            \n            \n                Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models\n                UCL Statistics\n                \n                Principal advisor: Prof Alexandros Beskos\n            \n                \n        \n\n        \n            \n            \n                Simulation-based Inference for Expensive Scientific Simulators\n                UCL Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                A computational framework for heterogeneous coupling in large scale computations\n                UCL Mathematics\n                \n                Principal advisor: Prof Erik Burman\n            \n                \n        \n\n        \n            \n            \n                A machine learning-informed computational model of cancer-immune interactions\n                Imperial Mathematics\n                \n                Principal advisor: Dr. Barbara Bravi\n            \n                \n        \n\n        \n            \n            \n                Automated Bayesian inference in stochastic differential equation models\n                UCL Advanced Research Computing Centre\n                \n                Principal advisor: Dr Matt Graham\n            \n                \n        \n\n        \n            \n            \n                Numerical methods for PDEs in fractal domains\n                UCL Mathematics\n                \n                Principal advisor: Prof David Hewett\n            \n                \n        \n\n        \n            \n            \n                Algorithms and software for differentiable programming at the ML/PDE/DAE interface\n                Imperial Computing and Mathematics\n                \n                Principal advisor: Prof Ruth Misener and Prof David Ham\n            \n                \n        \n\n        \n            \n            \n                A Unified Form Language for PDEs in non-divergence form\n                UCL Mathematics\n                \n                Principal advisor: Dr Max Jensen\n            \n                \n        \n\n        \n            \n            \n                Transfer Learning for Monte Carlo\n                UCL Statistical Science\n                \n                Principal advisor: Dr.Francois-Xavier Briol\n            \n                \n        \n\n        \n            \n            \n                Drift detection in graph streams and its applications in healthcare\n                UCL Computer Science and Advanced Research Computing Centre\n                \n                Principal advisor: Dr. Yevgeniya Kovalchuk\n            \n                \n        \n\n        \n            \n            \n                PARALIFT – A PARAllel framework for LIFTed training of neural networks\n                UCL Computer Science\n                \n                Principal advisor: Prof Martin Benning\n            \n                \n        \n\n        \n            \n            \n                Computational solution of inverse problems using large datasets of low rank\n                UCL Mathematics\n                \n                Principal advisor: Prof Erik Burman\n            \n                \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "phd_projects/entries/mcewen_probabilistic.html",
    "href": "phd_projects/entries/mcewen_probabilistic.html",
    "title": "Differentiable probabilistic deep learning with generative denoising diffusion models",
    "section": "",
    "text": "Generative AI models for images, such as denoising diffusion models (e.g. Stable Diffusion), have recently demonstrated remarkable performance (Romback et al. 2022; https://arxiv.org/abs/2112.10752). Such generative models can be adapted to solve scientific inverse problems, such as recovering maps of the dark matter of the Universe. However, current approaches typically recover a single prediction, e.g. recover a single image. For robust scientific studies, however, single estimates are not sufficient and a principled statistical assessment is critical in order to quantify uncertainties. Embedding denoising diffusion models in a principled statistical framework for solving inverse problems remains a topical open problem in the field. A number of approximate solutions have been proposed (e.g. Chung et al 2023; https://arxiv.org/abs/2209.14687).\nMcEwen and collaborators have recently developed the proximal nested sampling framework (Cai et al. 2022; https://arxiv.org/abs/2106.03646) for principled statistical inference for high-dimensional inverse imaging problems with convex likelihoods (initial code available at https://github.com/astro-informatics/proxnest). Not only is the correct underlying posterior distribution targeted but the framework also supports computation of the marginal likelihood for principled Bayesian model comparison. Recently, the framework has been extended to support deep learned data-driven priors based on simple denoisers (McEwen et al. 2023; https://arxiv.org/abs/2307.00056), although not denoising diffusion models.\n\n\n\nIn this project we will develop a principled statistical framework to sample the posterior distribution of scientific inverse imaging problems that integrates the generative power of denoising diffusion models. This will be achieved by integrating denoising diffusion models into the proximal nested sampling framework. The resulting framework is expected to result in superior reconstruction performance due to the power of generative diffusion models, targets the correct underlying posterior distribution and also allows for Bayesian model comparison to assess different data-driven priors. The framework will be extended beyond convex likelihoods to handle general non-linear models by leveraging automatic differentiation and gradient-based likelihood constraints. Automatic differentiation will also be exploited to accelerate inference. While the focus will be mostly on theoretical methodological and code developments, the methods developed will be demonstrated on a number of inverse imaging problems in a range of fields.\n\n\n\nThe main deliverable with be an open-source code implementing the framework developed. Development will involve differentiable programming, generative denoising diffusion models, and Markov chain Monte Carlo (MCMC) techniques. A number of articles will be prepared as the research progresses, targeting the main deep learning venues (e.g. ICLR, ICML, NeurIPS)."
  },
  {
    "objectID": "phd_projects/entries/mcewen_probabilistic.html#project-description",
    "href": "phd_projects/entries/mcewen_probabilistic.html#project-description",
    "title": "Differentiable probabilistic deep learning with generative denoising diffusion models",
    "section": "",
    "text": "Generative AI models for images, such as denoising diffusion models (e.g. Stable Diffusion), have recently demonstrated remarkable performance (Romback et al. 2022; https://arxiv.org/abs/2112.10752). Such generative models can be adapted to solve scientific inverse problems, such as recovering maps of the dark matter of the Universe. However, current approaches typically recover a single prediction, e.g. recover a single image. For robust scientific studies, however, single estimates are not sufficient and a principled statistical assessment is critical in order to quantify uncertainties. Embedding denoising diffusion models in a principled statistical framework for solving inverse problems remains a topical open problem in the field. A number of approximate solutions have been proposed (e.g. Chung et al 2023; https://arxiv.org/abs/2209.14687).\nMcEwen and collaborators have recently developed the proximal nested sampling framework (Cai et al. 2022; https://arxiv.org/abs/2106.03646) for principled statistical inference for high-dimensional inverse imaging problems with convex likelihoods (initial code available at https://github.com/astro-informatics/proxnest). Not only is the correct underlying posterior distribution targeted but the framework also supports computation of the marginal likelihood for principled Bayesian model comparison. Recently, the framework has been extended to support deep learned data-driven priors based on simple denoisers (McEwen et al. 2023; https://arxiv.org/abs/2307.00056), although not denoising diffusion models.\n\n\n\nIn this project we will develop a principled statistical framework to sample the posterior distribution of scientific inverse imaging problems that integrates the generative power of denoising diffusion models. This will be achieved by integrating denoising diffusion models into the proximal nested sampling framework. The resulting framework is expected to result in superior reconstruction performance due to the power of generative diffusion models, targets the correct underlying posterior distribution and also allows for Bayesian model comparison to assess different data-driven priors. The framework will be extended beyond convex likelihoods to handle general non-linear models by leveraging automatic differentiation and gradient-based likelihood constraints. Automatic differentiation will also be exploited to accelerate inference. While the focus will be mostly on theoretical methodological and code developments, the methods developed will be demonstrated on a number of inverse imaging problems in a range of fields.\n\n\n\nThe main deliverable with be an open-source code implementing the framework developed. Development will involve differentiable programming, generative denoising diffusion models, and Markov chain Monte Carlo (MCMC) techniques. A number of articles will be prepared as the research progresses, targeting the main deep learning venues (e.g. ICLR, ICML, NeurIPS)."
  },
  {
    "objectID": "phd_projects/entries/Cox_kwave.html",
    "href": "phd_projects/entries/Cox_kwave.html",
    "title": "Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)",
    "section": "",
    "text": "Pseudospectral time domain models of wave propagation, in which wave equations are solved using spectral methods for computing spatial gradients and corrected-finite-difference schemes for the temporal integration, are increasingly widely used in acoustics, eg. in therapeutic and diagnostic biomedical ultrasound (imaging and therapy), and in large-scale underwater acoustics. This has been driven largely by the success of our software k-Wave, a toolbox for modelling linear and nonlinear wave propagation, written initially in Matlab but with alternative implementations now available. There are currently &gt;16,000 registered k-Wave users from &gt;70 countries who have downloaded at least one version of k-Wave, with &gt;4000 users downloading the latest release (a proxy for active users). This makes k-Wave one of the most widely used open-source tools in acoustics. The two main journal articles describing k-Wave have been cited over 2500 times. Recently, we began working on a new version of the software, k-Wave 2.0, which involves re-engineering the code base to leverage object orientated programming and differentiable functions to facilitate its use in deep learning and coupled physics problems. In addition, we are taking the opportunity to extend the algorithms underlying k-Wave so that the inputs (sources, sensors, medium properties, boundary conditions) can be defined arbitrarily in space and are not dependent on the underlying grid used for the computations. Our vision is for k-Wave 2.0 to be largely grid-independent, as far as the user is concerned. (This is guided in part by the philosophy that drives k-Wave development, that of lowering the barrier to entry for potential users as much as possible.) In the current version of k-Wave it is already possible to use ‘off-grid’ sources and sensors. This has been achieved by exploiting the fact that the bandlimited interpolant implicit in the numerical scheme is known to a high degree of accuracy [REF]. Furthermore, we have shown that reflecting boundary conditions can, in a restricted set of cases, be implemented by exploiting the symmetries in discrete sine and cosine transforms to automatically compute image source, but this approach is limited to planar boundaries and short duration simulations.\n\n\n\nThe two main limitations of k-Wave are (1) the staircasing effect that heterogeneous medium properties are subject to, because they have to be defined at the grid points and are undefined in-between, and (2) the fact that only boundary condition currently implemented is an absorbing boundary condition (perfectly matched layer) mimicking free-space propagation. The objectives of this project are to develop and code numerical schemes that overcome these limitations.\nFirst, we propose to tackle the problem of defining the medium properties independently of the grid by rearranging the governing equations so that terms containing heterogeneous medium properties are reformulated as equivalent scattering terms, and exploiting and extending the off-grid approach currently used to implement sources and sensors. A preliminary simulation with a toy model and a sound speed heterogeneity gives us confidence this approach will work, but its extension to other heterogeneities, eg. density, absorption, nonlinearity parameter has not been explored.\nSecond, we aim to implement arbitrary boundary conditions by computing, at each time-step, the distributed image source that would be required to mimic the effect of the boundary. This strategy, which will also employ the off-grid machinery, extends the conventional idea of an image source in a planar reflector to that of an arbitrarily-shaped boundary. Preliminary work has demonstrated this idea in the simple scenario of a perfectly-reflecting circular boundary, but the extent to which it can be extended to impedance boundaries with arbitrary geometries remains to be seen.\nFor both these areas of research, as well as the question of what the optimal algorithms and numerical schemes are, there remain questions as to how they are best implemented in k-Wave 2.0, both from the perspective of computational efficiency as well as the user experience.\n\n\n\nThe algorithms and code developed as part of the project will be directly contributing to k-Wave 2.0. k-Wave 2.0 is being developed within a GitHub repository with the help of UCL’s Advanced Research Computing group, and following a rigorous software development protocol."
  },
  {
    "objectID": "phd_projects/entries/Cox_kwave.html#project-description",
    "href": "phd_projects/entries/Cox_kwave.html#project-description",
    "title": "Grid-independent pseudospectral models of broadband acoustic wave propagation (k-Wave 2.0)",
    "section": "",
    "text": "Pseudospectral time domain models of wave propagation, in which wave equations are solved using spectral methods for computing spatial gradients and corrected-finite-difference schemes for the temporal integration, are increasingly widely used in acoustics, eg. in therapeutic and diagnostic biomedical ultrasound (imaging and therapy), and in large-scale underwater acoustics. This has been driven largely by the success of our software k-Wave, a toolbox for modelling linear and nonlinear wave propagation, written initially in Matlab but with alternative implementations now available. There are currently &gt;16,000 registered k-Wave users from &gt;70 countries who have downloaded at least one version of k-Wave, with &gt;4000 users downloading the latest release (a proxy for active users). This makes k-Wave one of the most widely used open-source tools in acoustics. The two main journal articles describing k-Wave have been cited over 2500 times. Recently, we began working on a new version of the software, k-Wave 2.0, which involves re-engineering the code base to leverage object orientated programming and differentiable functions to facilitate its use in deep learning and coupled physics problems. In addition, we are taking the opportunity to extend the algorithms underlying k-Wave so that the inputs (sources, sensors, medium properties, boundary conditions) can be defined arbitrarily in space and are not dependent on the underlying grid used for the computations. Our vision is for k-Wave 2.0 to be largely grid-independent, as far as the user is concerned. (This is guided in part by the philosophy that drives k-Wave development, that of lowering the barrier to entry for potential users as much as possible.) In the current version of k-Wave it is already possible to use ‘off-grid’ sources and sensors. This has been achieved by exploiting the fact that the bandlimited interpolant implicit in the numerical scheme is known to a high degree of accuracy [REF]. Furthermore, we have shown that reflecting boundary conditions can, in a restricted set of cases, be implemented by exploiting the symmetries in discrete sine and cosine transforms to automatically compute image source, but this approach is limited to planar boundaries and short duration simulations.\n\n\n\nThe two main limitations of k-Wave are (1) the staircasing effect that heterogeneous medium properties are subject to, because they have to be defined at the grid points and are undefined in-between, and (2) the fact that only boundary condition currently implemented is an absorbing boundary condition (perfectly matched layer) mimicking free-space propagation. The objectives of this project are to develop and code numerical schemes that overcome these limitations.\nFirst, we propose to tackle the problem of defining the medium properties independently of the grid by rearranging the governing equations so that terms containing heterogeneous medium properties are reformulated as equivalent scattering terms, and exploiting and extending the off-grid approach currently used to implement sources and sensors. A preliminary simulation with a toy model and a sound speed heterogeneity gives us confidence this approach will work, but its extension to other heterogeneities, eg. density, absorption, nonlinearity parameter has not been explored.\nSecond, we aim to implement arbitrary boundary conditions by computing, at each time-step, the distributed image source that would be required to mimic the effect of the boundary. This strategy, which will also employ the off-grid machinery, extends the conventional idea of an image source in a planar reflector to that of an arbitrarily-shaped boundary. Preliminary work has demonstrated this idea in the simple scenario of a perfectly-reflecting circular boundary, but the extent to which it can be extended to impedance boundaries with arbitrary geometries remains to be seen.\nFor both these areas of research, as well as the question of what the optimal algorithms and numerical schemes are, there remain questions as to how they are best implemented in k-Wave 2.0, both from the perspective of computational efficiency as well as the user experience.\n\n\n\nThe algorithms and code developed as part of the project will be directly contributing to k-Wave 2.0. k-Wave 2.0 is being developed within a GitHub repository with the help of UCL’s Advanced Research Computing group, and following a rigorous software development protocol."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html",
    "href": "phd_projects/entries/gutschow.html",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "",
    "text": "The Large Hadron Collider (LHC) has generated an immense amount of data, which, when combined with advanced Standard Model (SM) predictions, presents new opportunities for discovering physics beyond the SM. Current analysis frameworks, such as Rivet, allow for data-theory comparisons, but incorporating higher-order calculations (NLO, NNLO) into these comparisons remains challenging. Additionally, there is potential to enhance traditional statistical analyses with machine learning-based anomaly detection to identify mismodelling or uncover hints of new physics. This project aims to improve both the affordability and accuracy of SM calculations and apply them to large-scale metadata analysis of collider data."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#existing-background-work",
    "href": "phd_projects/entries/gutschow.html#existing-background-work",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "",
    "text": "The Large Hadron Collider (LHC) has generated an immense amount of data, which, when combined with advanced Standard Model (SM) predictions, presents new opportunities for discovering physics beyond the SM. Current analysis frameworks, such as Rivet, allow for data-theory comparisons, but incorporating higher-order calculations (NLO, NNLO) into these comparisons remains challenging. Additionally, there is potential to enhance traditional statistical analyses with machine learning-based anomaly detection to identify mismodelling or uncover hints of new physics. This project aims to improve both the affordability and accuracy of SM calculations and apply them to large-scale metadata analysis of collider data."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/gutschow.html#main-objectives-of-the-project",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\n\nOptimising SM Calculations: The project seeks to make state-of-the-art SM predictions computationally affordable and accessible for large-scale collider data analysis. This will be achieved through performance profiling of existing tools and integrating higher-order calculations into Monte Carlo (MC) particle-level predictions using reweighting techniques.\nComprehensive Metadata Analysis: The student will perform large-scale metadata analysis using Rivet, comparing experimental data from LHC experiments like ATLAS and CMS with optimised SM predictions. P-value distributions will be generated to quantify the level of agreement between theory and data across multiple observables.\nAnomaly Detection Framework: Machine learning algorithms will be applied to p-value distributions to detect anomalies in data-theory comparisons. This will enhance sensitivity in identifying potential Monte Carlo mismodelling or uncovering signals of new physics."
  },
  {
    "objectID": "phd_projects/entries/gutschow.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/gutschow.html#details-of-softwaredata-deliverables",
    "title": "UCL - Scalable Collider Data Analysis with Optimised Predictions and Machine-Learning Anomaly Detection",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\n\nPerformance-Optimised Prediction Tools: The student will profile existing SM tools (e.g. Sherpa, Herwig) to identify and address computational bottlenecks, optimising them for large-scale collider data comparisons.\nIntegration of Higher-Order Calculations: Incorporation of NLO and NNLO corrections into MC particle-level predictions through reweighting techniques, making these sophisticated calculations accessible for widespread use.\nData-Theory Comparison Framework: A Rivet-based framework for generating detailed p-value distributions, which will provide a robust statistical foundation for comparing experimental data with SM predictions.\nMachine Learning-Based Anomaly Detection: Development of a machine learning-enhanced system to detect inconsistencies in data-theory comparisons, using unsupervised learning techniques to identify potential new physics signals or MC mismodelling."
  },
  {
    "objectID": "phd_projects/entries/Salvi_Gillespie.html",
    "href": "phd_projects/entries/Salvi_Gillespie.html",
    "title": "A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems",
    "section": "",
    "text": "The Gillespie algorithm is arguably one of the most well-known models for simulating chemical reactions, essentially acting as an Euler-Maruyama scheme applied to a jump process with a discrete state space. At each step, a random time is drawn from an exponential distribution (reflecting Poisson arrival events), and the discrete state is updated based on state-dependent transition probabilities. This project will focus on developing a rigorous mathematical theory and corresponding algorithm in JAX to calibrate parameterised and path-wise solutions of the Gillespie algorithm to data using automatic differentiation. By leveraging tools from rough path theory and stochastic automatic differentiation, we aim to compute exact gradients, which would improve on the approximate gradient methods currently in use. The work could have far-reaching implications for the fields of chemical kinetics, synthetic biology, and computational biology.\n\n\nThe principal supervisor’s group has extensive experience in both rough path theory and deep learning, particularly in deriving exact gradients for stochastic systems. For example, the work on “Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals” has laid the foundation for differentiating through stochastic and event-driven systems with discontinuities. Within the wider research community, a recent relevant contribution is the paper “A Differentiable Gillespie Algorithm for Simulating Chemical Kinetics, Parameter Estimation, and Designing Synthetic Biological Circuits.” However, this approach only provides approximate solutions and does not compute path-wise gradients, making it a target for improvement. Other works on stochastic automatic differentiation, such as “Automatic Differentiation of Programs with Discrete Randomness,” will also inform this project by providing a framework to handle the challenge of differentiating through inherently discrete random variables.\n\n\n\n\nDevelop higher order auto-differentiable solvers to efficiently compute path-wise solutions of the Gillespie algorithm, with a focus on computing exact path-wise gradients.\nStudy convergence/error and stability analysis of the proposed algorithms.\nCalibrate the algorithm to real-world data, showcasing its application in chemical kinetics and synthetic biology.\n\n\n\n\nThe project will deliver a JAX-based implementation of a differentiable Gillespie algorithm that computes exact gradients through discrete stochastic systems. This will involve: 1. The development of custom gradient routines that handle discrete random variables and jump processes. 2. A path-wise algorithm capable of calibrating chemical reaction networks to empirical data using automatic differentiation. 3. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. 4. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/Salvi_Gillespie.html#project-description",
    "href": "phd_projects/entries/Salvi_Gillespie.html#project-description",
    "title": "A differentiable Gillespie algorithm for exact gradients through discrete stochastic systems",
    "section": "",
    "text": "The Gillespie algorithm is arguably one of the most well-known models for simulating chemical reactions, essentially acting as an Euler-Maruyama scheme applied to a jump process with a discrete state space. At each step, a random time is drawn from an exponential distribution (reflecting Poisson arrival events), and the discrete state is updated based on state-dependent transition probabilities. This project will focus on developing a rigorous mathematical theory and corresponding algorithm in JAX to calibrate parameterised and path-wise solutions of the Gillespie algorithm to data using automatic differentiation. By leveraging tools from rough path theory and stochastic automatic differentiation, we aim to compute exact gradients, which would improve on the approximate gradient methods currently in use. The work could have far-reaching implications for the fields of chemical kinetics, synthetic biology, and computational biology.\n\n\nThe principal supervisor’s group has extensive experience in both rough path theory and deep learning, particularly in deriving exact gradients for stochastic systems. For example, the work on “Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals” has laid the foundation for differentiating through stochastic and event-driven systems with discontinuities. Within the wider research community, a recent relevant contribution is the paper “A Differentiable Gillespie Algorithm for Simulating Chemical Kinetics, Parameter Estimation, and Designing Synthetic Biological Circuits.” However, this approach only provides approximate solutions and does not compute path-wise gradients, making it a target for improvement. Other works on stochastic automatic differentiation, such as “Automatic Differentiation of Programs with Discrete Randomness,” will also inform this project by providing a framework to handle the challenge of differentiating through inherently discrete random variables.\n\n\n\n\nDevelop higher order auto-differentiable solvers to efficiently compute path-wise solutions of the Gillespie algorithm, with a focus on computing exact path-wise gradients.\nStudy convergence/error and stability analysis of the proposed algorithms.\nCalibrate the algorithm to real-world data, showcasing its application in chemical kinetics and synthetic biology.\n\n\n\n\nThe project will deliver a JAX-based implementation of a differentiable Gillespie algorithm that computes exact gradients through discrete stochastic systems. This will involve: 1. The development of custom gradient routines that handle discrete random variables and jump processes. 2. A path-wise algorithm capable of calibrating chemical reaction networks to empirical data using automatic differentiation. 3. Extensive test cases and benchmarking to validate the accuracy and efficiency of the algorithm. 4. Open-source release of the software, complete with documentation and tutorials."
  },
  {
    "objectID": "phd_projects/entries/Tobar_machinelearning.html",
    "href": "phd_projects/entries/Tobar_machinelearning.html",
    "title": "Optimal transport for probabilistic machine learning",
    "section": "",
    "text": "During the last decade, optimal transport (OT) has penetrated the core technical aspects of machine learning (ML). In particular, OT’s ability to define meaningful distances among generative models has allowed to design better, ad hoc, learning strategies for models defined over complex data structures.\nWe have developed OT methods, including Wasserstein-inspired distances and novel types of barycentres, for Gaussian processes, time series analysis, Bayesian model selection, outlier detection, trajectory tracking, natural language processing, and clustering of distributions.\n\n\n\nTo design and validate learning strategies and architectures for probabilistic generative models using concepts and resources from OT. Likewise, to explore the use and benefits that ML methods bring to the computation of OT. The project includes both theoretical and applied (computational) aspects.\n\nTo explore the state of the art in the interface between computational OT and probabilistic machine learning\nTo identify which aspects of learning strategies, or model architectures, can be enhanced via OT\nTo devise directions in which ML can improve OT computation (e.g., speed and robustness)\nTo provide theoretical guarantees for the developed solutions\nTo produce experimental validation of the proposed methodologies for general applied subjects (e.g., climate, astronomy, audio, social sciences, health)\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML communities\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing particular applications to scientific or social challenges."
  },
  {
    "objectID": "phd_projects/entries/Tobar_machinelearning.html#project-description",
    "href": "phd_projects/entries/Tobar_machinelearning.html#project-description",
    "title": "Optimal transport for probabilistic machine learning",
    "section": "",
    "text": "During the last decade, optimal transport (OT) has penetrated the core technical aspects of machine learning (ML). In particular, OT’s ability to define meaningful distances among generative models has allowed to design better, ad hoc, learning strategies for models defined over complex data structures.\nWe have developed OT methods, including Wasserstein-inspired distances and novel types of barycentres, for Gaussian processes, time series analysis, Bayesian model selection, outlier detection, trajectory tracking, natural language processing, and clustering of distributions.\n\n\n\nTo design and validate learning strategies and architectures for probabilistic generative models using concepts and resources from OT. Likewise, to explore the use and benefits that ML methods bring to the computation of OT. The project includes both theoretical and applied (computational) aspects.\n\nTo explore the state of the art in the interface between computational OT and probabilistic machine learning\nTo identify which aspects of learning strategies, or model architectures, can be enhanced via OT\nTo devise directions in which ML can improve OT computation (e.g., speed and robustness)\nTo provide theoretical guarantees for the developed solutions\nTo produce experimental validation of the proposed methodologies for general applied subjects (e.g., climate, astronomy, audio, social sciences, health)\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML communities\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing particular applications to scientific or social challenges."
  },
  {
    "objectID": "phd_projects/entries/ThomasShahrezaei_inference.html",
    "href": "phd_projects/entries/ThomasShahrezaei_inference.html",
    "title": "Inference of gene regulatory networks from single cell data",
    "section": "",
    "text": "Interpretation of single-cell transcriptomics data is a tremendous mathematical challenge. Existing statistical modelling tools often lack interpretability and can introduce ambiguity introducing unquantified errors that feed into downstream analyses. By inferring mechanistic mathematical models of stochastic gene expression and gene regulatory networks, the project aims to utilise distributional information to enhance the interpretability of predictions and shed light on the cellular processes underlying transcriptional regulation.\n\n\n\nOur major innovation is model-driven leveraging distributional information of transcriptomic signatures hidden behind averages to learn about transcriptional regulation in living cells. Such distributional information exists across cells, genes and modalities but is often aggregated, averaged, or ignored by current tools. We will utilise mechanistic stochastic models as interpretable generative models for single genes to produce an atlas of burst kinetics parameters across cell types, tissues, organisms and align differentiation trajectories. Extending our model to multiple genes allows us to leverage multi-modal distributional information to infer gene regulatory networks.\n\n\n\nOur model-driven approach will provide easy-to-use computational tools capable of Bayesian parameter estimation and model selection. The tools utilise amortised inference that efficiently scales to genomic data and will be used to create single-cell atlases of transcriptional regulation across several organisms. The availability of these computational pipelines is expected to enhance downstream analyses in applications, such as normalisation and data integration, providing lasting benefits to the field of single-cell transcriptomics. Overall, the project aims to significantly improve the interpretability, reliability, and computational efficiency of transcriptomic analyses, boosting our understanding of cellular processes and their disruption in disease.\n\n\n\nModelling capture efficiency of single cell RNA-sequencing data improves inference of transcriptome-wide burst kinetics W Tang, ACS Jorgensen, S Marguerat, P Thomas, V Shahrezaei Bioinformatics, Volume 39, Issue 7, July 2023, btad395\nGlobal transcription regulation revealed from dynamical correlations in time-resolved single-cell RNA-sequencing D Volteras, V Shahrezaei, P Thomas bioRxiv, 2023.10. 24.563709"
  },
  {
    "objectID": "phd_projects/entries/ThomasShahrezaei_inference.html#project-description",
    "href": "phd_projects/entries/ThomasShahrezaei_inference.html#project-description",
    "title": "Inference of gene regulatory networks from single cell data",
    "section": "",
    "text": "Interpretation of single-cell transcriptomics data is a tremendous mathematical challenge. Existing statistical modelling tools often lack interpretability and can introduce ambiguity introducing unquantified errors that feed into downstream analyses. By inferring mechanistic mathematical models of stochastic gene expression and gene regulatory networks, the project aims to utilise distributional information to enhance the interpretability of predictions and shed light on the cellular processes underlying transcriptional regulation.\n\n\n\nOur major innovation is model-driven leveraging distributional information of transcriptomic signatures hidden behind averages to learn about transcriptional regulation in living cells. Such distributional information exists across cells, genes and modalities but is often aggregated, averaged, or ignored by current tools. We will utilise mechanistic stochastic models as interpretable generative models for single genes to produce an atlas of burst kinetics parameters across cell types, tissues, organisms and align differentiation trajectories. Extending our model to multiple genes allows us to leverage multi-modal distributional information to infer gene regulatory networks.\n\n\n\nOur model-driven approach will provide easy-to-use computational tools capable of Bayesian parameter estimation and model selection. The tools utilise amortised inference that efficiently scales to genomic data and will be used to create single-cell atlases of transcriptional regulation across several organisms. The availability of these computational pipelines is expected to enhance downstream analyses in applications, such as normalisation and data integration, providing lasting benefits to the field of single-cell transcriptomics. Overall, the project aims to significantly improve the interpretability, reliability, and computational efficiency of transcriptomic analyses, boosting our understanding of cellular processes and their disruption in disease.\n\n\n\nModelling capture efficiency of single cell RNA-sequencing data improves inference of transcriptome-wide burst kinetics W Tang, ACS Jorgensen, S Marguerat, P Thomas, V Shahrezaei Bioinformatics, Volume 39, Issue 7, July 2023, btad395\nGlobal transcription regulation revealed from dynamical correlations in time-resolved single-cell RNA-sequencing D Volteras, V Shahrezaei, P Thomas bioRxiv, 2023.10. 24.563709"
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "",
    "text": "Bayesian methods for calibration, that is fitting the parameters of a model to data, often require a large number of model runs to adequately explore the model parameter space. This makes them infeasible to apply to computationally intensive simulations. A common approach in this setting is to fit a statistical emulator to a limited number of model evaluations and use this as a proxy for the full model in an inference algorithm (Kennedy and O’Hagan, 2001; Rasmussen, 2003).\nA key issue in such approaches is the choice of points in the parameter space at which to evaluate the model at to form the data used to fit the emulator. Use of Bayesian experimental design approaches, where the parameter values are chosen to minimize the expected future uncertainty about the model output at each step (Kandasamy, Schneider and Póczos, 2015; Sinsbeck and Nowak, 2017; Acerbi, 2018; Oliveira, Ott, and Ramos, 2021), offer a principled approach for maximising the information we get about a model from a limited number of evaluations, and can be extended to evaluate batches of points at each iteration (Järvenpää et al., 2021) improving the opportunity for parallelisation.\nEmulator-based calibration methods often treat the model as a monolithic black box, however, in reality complex simulators typically have tuneable variables allowing a trade off between numerical accuracy and computational cost. Multi-level Monte Carlo methods (Henrich, 2001; Giles, 2015) offer one approach for exploiting this trade off, combining simulations at multiple level of fidelity to reduce computational cost while controlling error. A recent related idea in the field of probabilistic numerics, is to statistically model how numerical error varies with fidelity (Teymur et al., 2021) and use this to probabilistically extrapolate the behaviour at high fidelity.\n\n\nThe aim of this project will be to develop emulator-driven calibration approaches which employ a batched Bayesian experimental design strategy to adaptively select both the parameters to simulate the model with and variables controlling model fidelity, with an objective of maximising information gain from the simulations within a limited computational budget. This will require creating joint statistical models of the model output, its numerical error and computational cost, and devising utility functions to optimize over which balance between information gain and computational cost.\n\n\n\nThe key output of the project will be a suite of open-source software tools allowing emulator-based calibration of expensive simulator models with adaptive control of both parameters simulations are run at and variables controlling model fidelity. A central aim will be to ensure the tools developed are suitable to be used at scale on high performance computing systems, for example allowing interacting with job schedulers to asynchronously dispatch simulations, using batched sequential design strategies that allow setting off multiple runs in parallel and minimizing communication overheads by performing as much processing in-situ on nodes as possible. Where possible the aim will be to build on (and contribute back to) existing open-source tools and packages.\nThe proposed supervisory team has worked on developing and implementing related methodology in a fusion-energy modelling context in collaboration with UKAEA as part of the uncertainty quantification efforts of the ExCALIBUR fusion use-case project, NEPTUNE, and this project would build upon this existing foundation, with prototype implementations in the packages calibr and emul"
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html#project-description",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html#project-description",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "",
    "text": "Bayesian methods for calibration, that is fitting the parameters of a model to data, often require a large number of model runs to adequately explore the model parameter space. This makes them infeasible to apply to computationally intensive simulations. A common approach in this setting is to fit a statistical emulator to a limited number of model evaluations and use this as a proxy for the full model in an inference algorithm (Kennedy and O’Hagan, 2001; Rasmussen, 2003).\nA key issue in such approaches is the choice of points in the parameter space at which to evaluate the model at to form the data used to fit the emulator. Use of Bayesian experimental design approaches, where the parameter values are chosen to minimize the expected future uncertainty about the model output at each step (Kandasamy, Schneider and Póczos, 2015; Sinsbeck and Nowak, 2017; Acerbi, 2018; Oliveira, Ott, and Ramos, 2021), offer a principled approach for maximising the information we get about a model from a limited number of evaluations, and can be extended to evaluate batches of points at each iteration (Järvenpää et al., 2021) improving the opportunity for parallelisation.\nEmulator-based calibration methods often treat the model as a monolithic black box, however, in reality complex simulators typically have tuneable variables allowing a trade off between numerical accuracy and computational cost. Multi-level Monte Carlo methods (Henrich, 2001; Giles, 2015) offer one approach for exploiting this trade off, combining simulations at multiple level of fidelity to reduce computational cost while controlling error. A recent related idea in the field of probabilistic numerics, is to statistically model how numerical error varies with fidelity (Teymur et al., 2021) and use this to probabilistically extrapolate the behaviour at high fidelity.\n\n\nThe aim of this project will be to develop emulator-driven calibration approaches which employ a batched Bayesian experimental design strategy to adaptively select both the parameters to simulate the model with and variables controlling model fidelity, with an objective of maximising information gain from the simulations within a limited computational budget. This will require creating joint statistical models of the model output, its numerical error and computational cost, and devising utility functions to optimize over which balance between information gain and computational cost.\n\n\n\nThe key output of the project will be a suite of open-source software tools allowing emulator-based calibration of expensive simulator models with adaptive control of both parameters simulations are run at and variables controlling model fidelity. A central aim will be to ensure the tools developed are suitable to be used at scale on high performance computing systems, for example allowing interacting with job schedulers to asynchronously dispatch simulations, using batched sequential design strategies that allow setting off multiple runs in parallel and minimizing communication overheads by performing as much processing in-situ on nodes as possible. Where possible the aim will be to build on (and contribute back to) existing open-source tools and packages.\nThe proposed supervisory team has worked on developing and implementing related methodology in a fusion-energy modelling context in collaboration with UKAEA as part of the uncertainty quantification efforts of the ExCALIBUR fusion use-case project, NEPTUNE, and this project would build upon this existing foundation, with prototype implementations in the packages calibr and emul"
  },
  {
    "objectID": "phd_projects/entries/Graham_Automatedbayesian.html#references",
    "href": "phd_projects/entries/Graham_Automatedbayesian.html#references",
    "title": "Accelerated multi-level Bayesian calibration of expensive simulations on high performance computing systems",
    "section": "References",
    "text": "References\n\nKennedy, M. C. and O’Hagan, A. (2001). “Bayesian calibration of computer models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3): 425–464. MR1858398. doi: https://doi.org/10.1111/1467-9868.00294.\nHeinrich, S. (2001). Multilevel Monte Carlo methods. In Large-Scale Scientific Computing: Third International Conference, LSSC 2001 (pp. 58-67). Springer Berlin Heidelberg.\nRasmussen, C. E. (2003). “Gaussian Processes to Speed up Hybrid Monte Carlo for Expensive Bayesian Integrals.” Bayesian Statistics 7, 651–659. MR2003529.\nKandasamy, K., Schneider, J., and Póczos, B. (2015). “Bayesian active learning for posterior estimation.” In International Joint Conference on Artificial Intelligence, 3605–3611.\nGiles, M. B. (2015). Multilevel Monte Carlo methods. Acta numerica, 24, 259-328.\nSinsbeck, M. and Nowak, W. (2017). “Sequential Design of Computer Experiments for the Solution of Bayesian Inverse Problems.” SIAM/ASA Journal on Uncertainty Quantification, 5(1): 640–664. MR3679325. doi: https://doi.org/10.1137/15M1047659.\nAcerbi, L. (2018). “Variational Bayesian Monte Carlo.” In Advances in Neural Information Processing Systems 31 , 8223–8233.\nJärvenpää, M., Gutmann, M. U., Vehtari, A., & Marttinen, P. (2021). Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations. Bayesian Analysis. 16(1), 147-178.\nOliveira, R., Ott, L., & Ramos, F. (2021). “No-regret approximate inference via Bayesian optimisation”. In Uncertainty in Artificial Intelligence (pp. 2082-2092). PMLR.\nTeymur, O., Foley, C., Breen, P., Karvonen, T., & Oates, C. J. (2021). Black box probabilistic numerics. Advances in Neural Information Processing Systems, 34, 23452-23464."
  },
  {
    "objectID": "phd_projects/entries/Briol_Bayesianinference.html",
    "href": "phd_projects/entries/Briol_Bayesianinference.html",
    "title": "Robust Bayesian Inference at Scale",
    "section": "",
    "text": "### Existing background work\nRobustness refers to the ability of a model to perform well on unseen data, or data that is different from the data it was trained on. It is an ever-evolving challenge for practitioners of statistical and machine learning methods, who need to deal with large, complex, and un-curated data sets and build tools that are reliable in uncontrolled environments. It is also particularly important in critical applications, such as medical diagnosis or weather prediction, where a lack of robustness can have a severe impact. In this project, we will focus on the robustness of Bayesian machine learning. In Bayesian inference, we start with a prior belief about a quantity of interest, and then update this belief based on our model of the world and new evidence in the form of data. This allows us to formally describe our uncertainty and make reliable predictions. However, a crucial assumption is that the model can truly represent the data-generating process. When this assumption is violated, the model is called misspecified, and our predictions become unreliable. To remedy this issue, a wide range of approaches have been proposed to make Bayesian methods more robust, including modifications of standard likelihoods and generalised Bayesian approaches.\n\n\nThe main aim of this project will be to develop novel robust alternative to existing probabilistic machine learning algorithms which are used across the physical sciences, ranging from statistical emulators such as Gaussian processes to algorithms for large-scale filtering such as the Kalman filter. The focus will be on carefully selecting the generalised Bayesian update rules to ensure these algorithms have the same, or even lower, computational complexity than their existing counter-parts based on Bayesian updates, and that they can be efficiently implemented on modern scientific computing hardware and infrastructure.\n\n\n\nThe main development of software will be a package for robust Bayesian methods in Python. The aim will be to make the algorithms developed as part of this project easily available to the scientific community, and to make it straightforward for algorithms to be rigorously and fairly compared against existing competitors."
  },
  {
    "objectID": "phd_projects/entries/Briol_Bayesianinference.html#project-description",
    "href": "phd_projects/entries/Briol_Bayesianinference.html#project-description",
    "title": "Robust Bayesian Inference at Scale",
    "section": "",
    "text": "### Existing background work\nRobustness refers to the ability of a model to perform well on unseen data, or data that is different from the data it was trained on. It is an ever-evolving challenge for practitioners of statistical and machine learning methods, who need to deal with large, complex, and un-curated data sets and build tools that are reliable in uncontrolled environments. It is also particularly important in critical applications, such as medical diagnosis or weather prediction, where a lack of robustness can have a severe impact. In this project, we will focus on the robustness of Bayesian machine learning. In Bayesian inference, we start with a prior belief about a quantity of interest, and then update this belief based on our model of the world and new evidence in the form of data. This allows us to formally describe our uncertainty and make reliable predictions. However, a crucial assumption is that the model can truly represent the data-generating process. When this assumption is violated, the model is called misspecified, and our predictions become unreliable. To remedy this issue, a wide range of approaches have been proposed to make Bayesian methods more robust, including modifications of standard likelihoods and generalised Bayesian approaches.\n\n\nThe main aim of this project will be to develop novel robust alternative to existing probabilistic machine learning algorithms which are used across the physical sciences, ranging from statistical emulators such as Gaussian processes to algorithms for large-scale filtering such as the Kalman filter. The focus will be on carefully selecting the generalised Bayesian update rules to ensure these algorithms have the same, or even lower, computational complexity than their existing counter-parts based on Bayesian updates, and that they can be efficiently implemented on modern scientific computing hardware and infrastructure.\n\n\n\nThe main development of software will be a package for robust Bayesian methods in Python. The aim will be to make the algorithms developed as part of this project easily available to the scientific community, and to make it straightforward for algorithms to be rigorously and fairly compared against existing competitors."
  },
  {
    "objectID": "phd_projects/entries/haqshenas.html",
    "href": "phd_projects/entries/haqshenas.html",
    "title": "High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney, as well as osteoid osteoma. OptimUS featured as part of an international software benchmarking exercise for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic institutions worldwide as well as a pedagogical tool for undergraduate and postgraduate students. The current BEM formulations in OptimUS assume that domains exhibit fluid-like properties. This gives us accurate prediction of ultrasound propagation in soft tissues (e.g. the liver and kidney). However, hard tissues like bones and many dense cancer tumours demonstrate solid-like (elastic) behaviour. This creates a gap in accurately modelling ultrasound propagation in domains with a mix of soft and hard tissues and underscores the need for BEM formulations that represent the physics more accurately.\nOptimUS leverages the BEMpp kernel developed by Prof Betcke in UCL. Prof Chaillat (from ENSTA, Paris, France) is an expert in modelling wave propagation in elastic domains using BEM. She is the main developer of COFFEE, which is a code for solving elastic BEM in separated domains. The supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art medical imaging and computational facilities (high-performance computing workstations as well as the UCL Research Computing Platforms Service) required for the successful delivery of this project.\n\n\n\nThis project aims to develop an efficient BEM formulation for solving ultrasound wave propagation in mixed domains of soft and hard tissues (i.e., both fluid and elastic domains). The BEM approach uses Green’s functions, which are inherently complex in elastic problems. Also, the equations become ill-conditioned at high frequencies, substantially deteriorating the performance of the solver. To address these challenges, a novel formulation based on the Helmholtz transform will be introduced, building on the team’s previous works in solving the Helmholtz and Maxwell equations. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from medical images (ultrasound/MRI/CT scans). The successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as shear wave elastography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its adoption in clinical settings, where it could be used for personalised treatment, ultimately improving patient outcomes.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of the new BEM formulation for solving ultrasound waves in complex viscous and elastic domains. The student will be actively involved in software development using Python and employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in clinics."
  },
  {
    "objectID": "phd_projects/entries/haqshenas.html#project-description",
    "href": "phd_projects/entries/haqshenas.html#project-description",
    "title": "High-performance solver for ultrasound propagation in mixed domains with complex fluid and solid materials",
    "section": "",
    "text": "The supervisory team, together with Dr van ’t Wout, has been developing the open-source Python library OptimUS for solving acoustic wave propagation in an unbounded medium in 3D with piecewise homogeneous domains. OptimUS solves the Helmholtz equation using the boundary element method (BEM). It has been used to simulate biomedical ultrasound clinical scenarios relating to treating cancers of the liver and kidney, as well as osteoid osteoma. OptimUS featured as part of an international software benchmarking exercise for ultrasound transcranial neuromodulation, overseen by the International Transcranial Ultrasonic Stimulation Safety and Standards. The OptimUS library is being used in biomedical ultrasound research in academic institutions worldwide as well as a pedagogical tool for undergraduate and postgraduate students. The current BEM formulations in OptimUS assume that domains exhibit fluid-like properties. This gives us accurate prediction of ultrasound propagation in soft tissues (e.g. the liver and kidney). However, hard tissues like bones and many dense cancer tumours demonstrate solid-like (elastic) behaviour. This creates a gap in accurately modelling ultrasound propagation in domains with a mix of soft and hard tissues and underscores the need for BEM formulations that represent the physics more accurately.\nOptimUS leverages the BEMpp kernel developed by Prof Betcke in UCL. Prof Chaillat (from ENSTA, Paris, France) is an expert in modelling wave propagation in elastic domains using BEM. She is the main developer of COFFEE, which is a code for solving elastic BEM in separated domains. The supervisory team and the external partners will offer academic mentorship, educational support and access to the state-of-the-art medical imaging and computational facilities (high-performance computing workstations as well as the UCL Research Computing Platforms Service) required for the successful delivery of this project.\n\n\n\nThis project aims to develop an efficient BEM formulation for solving ultrasound wave propagation in mixed domains of soft and hard tissues (i.e., both fluid and elastic domains). The BEM approach uses Green’s functions, which are inherently complex in elastic problems. Also, the equations become ill-conditioned at high frequencies, substantially deteriorating the performance of the solver. To address these challenges, a novel formulation based on the Helmholtz transform will be introduced, building on the team’s previous works in solving the Helmholtz and Maxwell equations. The new formulation will be implemented in the OptimUS software, validated against analytical solutions for canonical geometries, and tested in simulations of ultrasound wave propagation through the human body using anatomical models derived from medical images (ultrasound/MRI/CT scans). The successful completion of this project will have significant implications for the field of biomedical ultrasound, particularly in treatment planning for procedures like focused ultrasound therapy as well as imaging applications such as shear wave elastography. By improving the accuracy and efficiency of simulations, this work will enhance OptimUS and facilitate its adoption in clinical settings, where it could be used for personalised treatment, ultimately improving patient outcomes.\n\n\n\nThe project involves implementing, testing and optimising the computational performance of the new BEM formulation for solving ultrasound waves in complex viscous and elastic domains. The student will be actively involved in software development using Python and employing best practices in software carpentry including version control with Git and automated testing to ensure code reliability and reproducibility. The outcome of this project will impact the field of biomedical ultrasound, specifically for treatment planning, and facilitate the uptake of OptimUS in clinics."
  },
  {
    "objectID": "phd_projects/entries/Coveney_dynamicalsystems.html",
    "href": "phd_projects/entries/Coveney_dynamicalsystems.html",
    "title": "Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems",
    "section": "",
    "text": "Floating point pathologies in the simulation of chaotic dynamics on digital computers have been uncovered in the case of some remarkably simple chaotic maps and ordinary differential equations. In particular, the generalised Bernoulli map and the Lorenz 96 system exhibit certain behaviours where the numerical solutions generate incorrect results and related behaviour which is not understood. [1,2]\nA fundamental aspect of chaotic dynamics is the presence of unstable periodic orbits (UPOs), the enumeration of which provides the skeleton of chaos. However, the floating point numbers are unable to exactly identify the UPOs, with the result that substantial numbers of these orbits are missed. Moreover, the period of these orbits grows exponentially with the dimension of the differential equations underpinning them. Given that the statistical properties of these chaotic systems are determined by the spectrum of their UPOs, those properties are compromised by their absence.\n\n\n\nIn this research, the detailed way in which UPOs are excluded will be investigated theoretically as well as numerically in order to understand why, for higher dimensional versions of the Lorenz 96 system (N ~ 500), half, single and double precision floating point numbers produce closely similar statistical properties of the system.\nAn additional line of investigation will be undertaken with a view to implementing these dynamical systems on analogue computers, where no such floating point pathologies should arise. In principle, analogue solutions should be a much closer approximation to the true continuum behaviour of these systems and will throw further light on the floating point pathologies encountered on digital computers. It may well be necessary to extend the analysis to handle analogue systems incorporating noise.\n\n\n\nWe shall need to develop fast and highly efficient methods for identifying unstable periodic orbits to assess how many of them are missed using floating point numbers. A major problem in addressing difficulties caused by floats at present is that all conventional computers have IEEE floating point numbers on them; new numbering systems have not yet been deployed in any wide ranging manner on such devices. The most promising route to overcome these floating point pathologies is stochastic rounding, but while it can overcome some of these pathologies, there remains the intractability of computing the very large period orbits. The possibility of programming analogue computers is also relevant in order to test the accuracy of digital solutions.\n\nB. M. Boghosian, P. V. Coveney H. Wang, “A New Pathology in the Simulation of Chaotic Dynamical Systems on Digital Computers”, Advanced Theory and Simulations, 1900125 (2019), DOI:10.1002/adts.201900125\nM. Klöwer, P. V. Coveney, E. A. Paxton, T. N. Palmer, “Periodic orbits in chaotic systems simulated at low precision”, Nature Scientific Reports (2023) DOI: 10.1038/s41598-023-37004-4"
  },
  {
    "objectID": "phd_projects/entries/Coveney_dynamicalsystems.html#project-description",
    "href": "phd_projects/entries/Coveney_dynamicalsystems.html#project-description",
    "title": "Quantifying and eliminating floating point pathologies in the simulation of chaotic dynamical systems",
    "section": "",
    "text": "Floating point pathologies in the simulation of chaotic dynamics on digital computers have been uncovered in the case of some remarkably simple chaotic maps and ordinary differential equations. In particular, the generalised Bernoulli map and the Lorenz 96 system exhibit certain behaviours where the numerical solutions generate incorrect results and related behaviour which is not understood. [1,2]\nA fundamental aspect of chaotic dynamics is the presence of unstable periodic orbits (UPOs), the enumeration of which provides the skeleton of chaos. However, the floating point numbers are unable to exactly identify the UPOs, with the result that substantial numbers of these orbits are missed. Moreover, the period of these orbits grows exponentially with the dimension of the differential equations underpinning them. Given that the statistical properties of these chaotic systems are determined by the spectrum of their UPOs, those properties are compromised by their absence.\n\n\n\nIn this research, the detailed way in which UPOs are excluded will be investigated theoretically as well as numerically in order to understand why, for higher dimensional versions of the Lorenz 96 system (N ~ 500), half, single and double precision floating point numbers produce closely similar statistical properties of the system.\nAn additional line of investigation will be undertaken with a view to implementing these dynamical systems on analogue computers, where no such floating point pathologies should arise. In principle, analogue solutions should be a much closer approximation to the true continuum behaviour of these systems and will throw further light on the floating point pathologies encountered on digital computers. It may well be necessary to extend the analysis to handle analogue systems incorporating noise.\n\n\n\nWe shall need to develop fast and highly efficient methods for identifying unstable periodic orbits to assess how many of them are missed using floating point numbers. A major problem in addressing difficulties caused by floats at present is that all conventional computers have IEEE floating point numbers on them; new numbering systems have not yet been deployed in any wide ranging manner on such devices. The most promising route to overcome these floating point pathologies is stochastic rounding, but while it can overcome some of these pathologies, there remains the intractability of computing the very large period orbits. The possibility of programming analogue computers is also relevant in order to test the accuracy of digital solutions.\n\nB. M. Boghosian, P. V. Coveney H. Wang, “A New Pathology in the Simulation of Chaotic Dynamical Systems on Digital Computers”, Advanced Theory and Simulations, 1900125 (2019), DOI:10.1002/adts.201900125\nM. Klöwer, P. V. Coveney, E. A. Paxton, T. N. Palmer, “Periodic orbits in chaotic systems simulated at low precision”, Nature Scientific Reports (2023) DOI: 10.1038/s41598-023-37004-4"
  },
  {
    "objectID": "phd_projects/entries/Ni_StochasticPDE.html",
    "href": "phd_projects/entries/Ni_StochasticPDE.html",
    "title": "Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach",
    "section": "",
    "text": "Stochastic partial differential equations (SPDEs) are power mathematical tools to model random spatiotemporal dynamics, with broader applications ranging from weather forecasting to fluid dynamics. Machine learning (ML)-based approaches are emerging as it can be used to effectively handle the curse of dimensionality of traditional numerical methods. It can lead to the high-performing SPDE solver with significant computational acceleration. Moreover, the generalization of physics- informed neural networks from PDEs to SPDEs may provide a novel family of neural networks for analysing noisy spatiotemporal data, which can be used to discover the hidden physics law and predict the future data evolution. Most of ML algorithms for learning SPDEs fall into the supervised learning category, where the input and output pairs are “observable data” consisted of driving noise and the corresponding solution trajectories. These data are simulated by high quality numerical solver. One important aspect of improving ML algorithms is to design neural networks to better approximate the SPDE solution map [1 – 3]. Among them, our proposed Deep latent regularity network incorporates Regularity structure, a groundbreaking work on SPDEs by the Fields medallist Martin Hairer, to design neural networks. This model demonstrates superior performance in terms of accuracy and inferences time on various SPDEs such as the stochastic 2D Navier-Stokes equation.\n\n\n\nIn this project, we address the challenge of learning solutions to SPDEs using only observable solution trajectory samples. This approach is crucial for realistically modeling spatio-temporal dynamics in real-world applications, like fluid dynamics, where the driving noise is often unobservable. Our primary goal is to develop an unsupervised learning method for solving parabolic stochastic PDEs. To this end, we explore the generalization of Weak Adversarial Networks (WANs) [4, 5] —originally an unsupervised method for learning PDE solutions inspired by their weak solution—to the case of SPDEs. To achieve the superior accuracy and efficiency, we will design the suitable physics-informed neural networks (such as DLR net [3] or neural SPDEs [2]), which are capable of effectively approximate the primal network for SPDE solution and the dual network for the weak solution.\nThe main objectives of the projects are two folds:\n\nto design the unsupervised learning methodology for solving parabolic SPDEs based on WANs and establish the theoretical foundations for the proposed networks;\no validate the effectiveness of the proposed model on a number of SPDE examples by benchmarking with the conventional SPDE solvers and state-of-the-art ML models.\n\n\n\n\nThe expected deliverables are outlined as follows:\n\nData: Simulate the numerical solutions of the SPDE examples, such as the dynamic model ϕ_1^4 in [1] and the stochastic 2D Navier-Stokes equation [2].\nCodes: Implement the python toolbox using PyTorch for the proposed ML methods to solve parabolic SPDEs and conduct numerical experiments for model comparison.\n\nReference\n[1]. Zhang, D., Guo, L. and Karniadakis, G.E., 2020. Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. SIAM Journal on Scientific Computing, 42(2), pp.A639-A665.\n[2]. Salvi, C., Lemercier, M. and Gerasimovics, A., 2022. Neural stochastic PDEs: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35, pp.1333-1344.\n[3]. Gong, S., Hu, P., Meng, Q., Wang, Y., Zhu, R., Chen, B., Ma, Z., Ni, H. and Liu, T.Y., 2023, June. Deep latent regularity network for modeling stochastic partial differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7740-7747).\n[4]. Zang, Y., Bao, G., Ye, X. and Zhou, H., 2020. Weak adversarial networks for high-dimensional partial differential equations. Journal of Computational Physics, 411, p.109409.\n[5]. Oliva, P.V., Wu, Y., He, C. and Ni, H., 2022. Towards fast weak adversarial training to solve high dimensional parabolic partial differential equations using XNODE-WAN. Journal of Computational Physics, 463, p.111233.\n[6]. Chevyrev, Ilya, Andris Gerasimovics, and Hendrik Weber. “Feature engineering with regularity structures.” arXiv preprint arXiv:2108.05879 (2021)."
  },
  {
    "objectID": "phd_projects/entries/Ni_StochasticPDE.html#project-description",
    "href": "phd_projects/entries/Ni_StochasticPDE.html#project-description",
    "title": "Accelerating parabolic Stochastic PDE solver via a weak adversarial network approach",
    "section": "",
    "text": "Stochastic partial differential equations (SPDEs) are power mathematical tools to model random spatiotemporal dynamics, with broader applications ranging from weather forecasting to fluid dynamics. Machine learning (ML)-based approaches are emerging as it can be used to effectively handle the curse of dimensionality of traditional numerical methods. It can lead to the high-performing SPDE solver with significant computational acceleration. Moreover, the generalization of physics- informed neural networks from PDEs to SPDEs may provide a novel family of neural networks for analysing noisy spatiotemporal data, which can be used to discover the hidden physics law and predict the future data evolution. Most of ML algorithms for learning SPDEs fall into the supervised learning category, where the input and output pairs are “observable data” consisted of driving noise and the corresponding solution trajectories. These data are simulated by high quality numerical solver. One important aspect of improving ML algorithms is to design neural networks to better approximate the SPDE solution map [1 – 3]. Among them, our proposed Deep latent regularity network incorporates Regularity structure, a groundbreaking work on SPDEs by the Fields medallist Martin Hairer, to design neural networks. This model demonstrates superior performance in terms of accuracy and inferences time on various SPDEs such as the stochastic 2D Navier-Stokes equation.\n\n\n\nIn this project, we address the challenge of learning solutions to SPDEs using only observable solution trajectory samples. This approach is crucial for realistically modeling spatio-temporal dynamics in real-world applications, like fluid dynamics, where the driving noise is often unobservable. Our primary goal is to develop an unsupervised learning method for solving parabolic stochastic PDEs. To this end, we explore the generalization of Weak Adversarial Networks (WANs) [4, 5] —originally an unsupervised method for learning PDE solutions inspired by their weak solution—to the case of SPDEs. To achieve the superior accuracy and efficiency, we will design the suitable physics-informed neural networks (such as DLR net [3] or neural SPDEs [2]), which are capable of effectively approximate the primal network for SPDE solution and the dual network for the weak solution.\nThe main objectives of the projects are two folds:\n\nto design the unsupervised learning methodology for solving parabolic SPDEs based on WANs and establish the theoretical foundations for the proposed networks;\no validate the effectiveness of the proposed model on a number of SPDE examples by benchmarking with the conventional SPDE solvers and state-of-the-art ML models.\n\n\n\n\nThe expected deliverables are outlined as follows:\n\nData: Simulate the numerical solutions of the SPDE examples, such as the dynamic model ϕ_1^4 in [1] and the stochastic 2D Navier-Stokes equation [2].\nCodes: Implement the python toolbox using PyTorch for the proposed ML methods to solve parabolic SPDEs and conduct numerical experiments for model comparison.\n\nReference\n[1]. Zhang, D., Guo, L. and Karniadakis, G.E., 2020. Learning in modal space: Solving time-dependent stochastic PDEs using physics-informed neural networks. SIAM Journal on Scientific Computing, 42(2), pp.A639-A665.\n[2]. Salvi, C., Lemercier, M. and Gerasimovics, A., 2022. Neural stochastic PDEs: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35, pp.1333-1344.\n[3]. Gong, S., Hu, P., Meng, Q., Wang, Y., Zhu, R., Chen, B., Ma, Z., Ni, H. and Liu, T.Y., 2023, June. Deep latent regularity network for modeling stochastic partial differential equations. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7740-7747).\n[4]. Zang, Y., Bao, G., Ye, X. and Zhou, H., 2020. Weak adversarial networks for high-dimensional partial differential equations. Journal of Computational Physics, 411, p.109409.\n[5]. Oliva, P.V., Wu, Y., He, C. and Ni, H., 2022. Towards fast weak adversarial training to solve high dimensional parabolic partial differential equations using XNODE-WAN. Journal of Computational Physics, 463, p.111233.\n[6]. Chevyrev, Ilya, Andris Gerasimovics, and Hendrik Weber. “Feature engineering with regularity structures.” arXiv preprint arXiv:2108.05879 (2021)."
  },
  {
    "objectID": "phd_projects/entries/lamb_deep_learning copy.html",
    "href": "phd_projects/entries/lamb_deep_learning copy.html",
    "title": "Deep learning with symmetry",
    "section": "",
    "text": "Project Description\nThis project will study the equivariance of deep learning. The large majority of equivariant deep learning approaches is ad-hoc, and the direction of the PhD project is to exploit the algebraic theory of invariant and equivariant functions in a systematic way to develop a versatile and general approach to learning in the presence of symmetry. A few important tasks that can be achieved with such general theory are learning symmetry group orbits (e.g. recognizing objects, irrespective of their position or orientation), learning symmetry coordinates (e.g. the position and orientation of objects), and learning symmetries and near-symmetries (invariances and near-invariances).\n\n\nExisting background work\nEquivariant deep learning is a subfield of deep learning that deals explicitly with symmetry in datasets and models. One of the most well-known and successful example is Convolutional Neural Networks (CNNs), used for the translation invariant machine learning of images. Equivariance is considered by many leading machine learners as one of the essential ingredients, explaining the remarkable leap in efficiency of deep learning models [1]. It is also an important ingredient in physics informed modelling. Our group has a long track record of understanding symmetry in dynamical systems and we are now translating this to machine learning.\n[1] M. Bronstein et al. Geometric deep learning. https://geometricdeeplearning.com\n\n\nMain objectives of the project\nThe project will develop a systematic approach to develop algorithms that merge (computer) algebra with optimisation strategies in deep learning settings. There are many areas of applications where equivariant deep learning is the intrinsic objective. The PhD project aims to develop algorithms and software to demonstrate the effectiveness of the mathematical methodology in a concrete applied setting.\n\n\nDetails of Software/Data Deliverables\nThe project will provide open source software enabling the implementation of equivariant learning strategies compatible with widely used deep learning frameworks."
  },
  {
    "objectID": "phd_projects/entries/Benning_Hearing.html",
    "href": "phd_projects/entries/Benning_Hearing.html",
    "title": "Improving impaired hearing through sound reconstruction from neural activity patterns",
    "section": "",
    "text": "This PhD project centres on the intersection of computer science and hearing research, with a primary focus on unravelling the complexities of the auditory system and its implications for perception and hearing loss. While decades of hearing research have yielded many valuable insights, the difference between normal and impaired perception relies on intricacies of the auditory system that remain poorly understood. Traditionally, the field has relied on simplifying assumptions, which limit the applicability of research findings to real-world scenarios. In [1], authors from the co-supervisor’s lab have studied auditory processing deficits in individuals with hearing loss using brain recordings, revealing that hearing loss distorts the low-dimensional neural encoding of speech, primarily affecting spectral processing and cross-frequency interactions, leading to hypersensitivity to background noise even after hearing aid amplification, and highlights the potential of deep neural networks for central brain structure research. In [2, 3], authors of the primary supervisor’s lab have introduced a novel regularisation framework for inverting deep neural networks that utilises auxiliary variables and tailored Bregman distances to lift the network parameter space into higher dimensions.\n\n\n\nThis project aims to leverage modern deep learning techniques to formulate and address the forward problem of mapping sounds onto neural activity as well as the inverse problem of reconstructing sounds from neural activity, a pivotal aspect in hearing research. By inverting neural networks to reconstruct sounds from simulated auditory nerve activity and real brain activity after various forms of cochlear damage, this project seeks to answer crucial questions about the degradation of acoustic information after hearing loss. Furthermore, it explores the extent to which information loss can be compensated when the inverse problem in the presence of cochlear damage is solved with neural activity that is recorded from an undamaged cochlea.\nThe project aims at combining and extending the described background research, with the primary goal of developing and implementing invertible deep neural networks that are expressive enough to convert sounds into neural activity, for which the inverse problem can be solved in a stable fashion, and that can be applied to real-world data.\nThis research has the potential to contribute significantly to our understanding of auditory processing, with implications for the design of assistive listening technologies. The project benefits from a substantial database of neural recordings from the inferior colliculus, a central hub in the auditory pathway, for its initial phases.\n\n\n\nHigh-quality software development is at the core of the proposed PhD project. Coding developments will include but are not limited to: - Software for Auditory Processing: Development of a comprehensive software suite, including user-friendly, well-documented programming codes for implementing and training invertible neural network models. - Integration with Existing Frameworks: Ensuring compatibility and ease of integration with established Python libraries like PyTorch or JAX, enhancing the utility of the developed software in various research and practical applications. - Open Access to Software: Making all developed software tools publicly available, ensuring they are accessible and well-documented for use by the wider research community. - Publication of Research Findings: Releasing open access publications detailing the research findings, methodologies, and applications of the developed software, contributing to the advancement of the field.\nThe applicant should have a background in Computer Science, Mathematics, or a related subject. The ideal applicant has programming experience in Python and particularly with advanced automatic differentiation and deep learning libraries such as PyTorch or JAX. A strong background in inverse problems is desirable but not mandatory.\nReferences - [1] Shievanie Sabesan, Andreas Fragner, Ciaran Bench, Fotios Drakopoulos, Nicholas A Lesica (2023) Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss. eLife 12. doi: 10.7554/eLife.85108 - [2] Xiaoyu Wang, and Martin Benning. “Lifted bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [3] Xiaoyu Wang, and Martin Benning (2023) A lifted Bregman formulation for the inversion of deep neural networks. Front. Appl. Math. Stat. 9:1176850. doi: 10.3389/fams.2023.1176850"
  },
  {
    "objectID": "phd_projects/entries/Benning_Hearing.html#project-description",
    "href": "phd_projects/entries/Benning_Hearing.html#project-description",
    "title": "Improving impaired hearing through sound reconstruction from neural activity patterns",
    "section": "",
    "text": "This PhD project centres on the intersection of computer science and hearing research, with a primary focus on unravelling the complexities of the auditory system and its implications for perception and hearing loss. While decades of hearing research have yielded many valuable insights, the difference between normal and impaired perception relies on intricacies of the auditory system that remain poorly understood. Traditionally, the field has relied on simplifying assumptions, which limit the applicability of research findings to real-world scenarios. In [1], authors from the co-supervisor’s lab have studied auditory processing deficits in individuals with hearing loss using brain recordings, revealing that hearing loss distorts the low-dimensional neural encoding of speech, primarily affecting spectral processing and cross-frequency interactions, leading to hypersensitivity to background noise even after hearing aid amplification, and highlights the potential of deep neural networks for central brain structure research. In [2, 3], authors of the primary supervisor’s lab have introduced a novel regularisation framework for inverting deep neural networks that utilises auxiliary variables and tailored Bregman distances to lift the network parameter space into higher dimensions.\n\n\n\nThis project aims to leverage modern deep learning techniques to formulate and address the forward problem of mapping sounds onto neural activity as well as the inverse problem of reconstructing sounds from neural activity, a pivotal aspect in hearing research. By inverting neural networks to reconstruct sounds from simulated auditory nerve activity and real brain activity after various forms of cochlear damage, this project seeks to answer crucial questions about the degradation of acoustic information after hearing loss. Furthermore, it explores the extent to which information loss can be compensated when the inverse problem in the presence of cochlear damage is solved with neural activity that is recorded from an undamaged cochlea.\nThe project aims at combining and extending the described background research, with the primary goal of developing and implementing invertible deep neural networks that are expressive enough to convert sounds into neural activity, for which the inverse problem can be solved in a stable fashion, and that can be applied to real-world data.\nThis research has the potential to contribute significantly to our understanding of auditory processing, with implications for the design of assistive listening technologies. The project benefits from a substantial database of neural recordings from the inferior colliculus, a central hub in the auditory pathway, for its initial phases.\n\n\n\nHigh-quality software development is at the core of the proposed PhD project. Coding developments will include but are not limited to: - Software for Auditory Processing: Development of a comprehensive software suite, including user-friendly, well-documented programming codes for implementing and training invertible neural network models. - Integration with Existing Frameworks: Ensuring compatibility and ease of integration with established Python libraries like PyTorch or JAX, enhancing the utility of the developed software in various research and practical applications. - Open Access to Software: Making all developed software tools publicly available, ensuring they are accessible and well-documented for use by the wider research community. - Publication of Research Findings: Releasing open access publications detailing the research findings, methodologies, and applications of the developed software, contributing to the advancement of the field.\nThe applicant should have a background in Computer Science, Mathematics, or a related subject. The ideal applicant has programming experience in Python and particularly with advanced automatic differentiation and deep learning libraries such as PyTorch or JAX. A strong background in inverse problems is desirable but not mandatory.\nReferences - [1] Shievanie Sabesan, Andreas Fragner, Ciaran Bench, Fotios Drakopoulos, Nicholas A Lesica (2023) Large-scale electrophysiology and deep learning reveal distorted neural signal dynamics after hearing loss. eLife 12. doi: 10.7554/eLife.85108 - [2] Xiaoyu Wang, and Martin Benning. “Lifted bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [3] Xiaoyu Wang, and Martin Benning (2023) A lifted Bregman formulation for the inversion of deep neural networks. Front. Appl. Math. Stat. 9:1176850. doi: 10.3389/fams.2023.1176850"
  },
  {
    "objectID": "phd_projects/entries/Pavliotis_inference.html",
    "href": "phd_projects/entries/Pavliotis_inference.html",
    "title": "Inference and inverse problems for stochastic interacting particle systems",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\n\n\n\nStochastic interacting particle systems (SIPS) arise in many applications, including mathematical biology, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. In addition, many PDE models, such as the Keller-Segel model for bacterial chemotaxis and the Onsager model for liquid crystals, can be interpreted as the mean field limit of a system of interacting diffusions. Quite often, the interaction law of the SIPS is not known and has to be inferred from data. The objective of this project is to develop efficient and accurate inference methodologies for learning the interaction law of SIPS and of their mean field limit from data. We will consider both the parametric and the nonparametric inference problem, and we will develop and implement a variety of methodologies, including maximum likelihood-based techniques, stochastic gradient descent in continuous time, kernel methods and spectral theoretic methodologies. We will also use recently developed neural calibration techniques. In addition, we will also interpret the inference problem for SIPS as an inverse problem for the mean field PDE and then apply Bayesian methods for inverse problems. For this, the measurement model will be taken to be noisy measurements of the solution to the PDE, e.g. of the Keller-Segel or of the Onager model, and more generally of the nonlinear and nonlocal McKean-Vlasov mean field PDE.\n\n\n\n\nAn inference toolbox for SIPS and their mean field limit, including efficient SDE solvers for the SIPS. All of the methods that will be studied and developed (MLE, SGDCT, neural calibration) will be included in the toolbox that we will develop.\nSoftware for implementing Bayesian inverse problem methodologies to the mean field nonlinear, nonlocal PDEs."
  },
  {
    "objectID": "phd_projects/entries/Pavliotis_inference.html#project-description",
    "href": "phd_projects/entries/Pavliotis_inference.html#project-description",
    "title": "Inference and inverse problems for stochastic interacting particle systems",
    "section": "",
    "text": "Greg Pavliotis is professor of Applied Mathematics at the department of Mathematics at Imperial College. His main research interests include statistical mechanics, multiscale systems, inference for stochastic processes and data-driven methods in applied mathematics. He has studied phase transitions for agent-based models (ABM) and interacting particle systems (IPS) and he has developed efficient inference methodologies for IPS and their mean field limit. He has also developed data-driven approaches to the study of ABMs.\n\n\n\nStochastic interacting particle systems (SIPS) arise in many applications, including mathematical biology, epidemiology, and the social sciences, for example in models for opinion formation, pedestrian dynamics and macroeconomics. In addition, many PDE models, such as the Keller-Segel model for bacterial chemotaxis and the Onsager model for liquid crystals, can be interpreted as the mean field limit of a system of interacting diffusions. Quite often, the interaction law of the SIPS is not known and has to be inferred from data. The objective of this project is to develop efficient and accurate inference methodologies for learning the interaction law of SIPS and of their mean field limit from data. We will consider both the parametric and the nonparametric inference problem, and we will develop and implement a variety of methodologies, including maximum likelihood-based techniques, stochastic gradient descent in continuous time, kernel methods and spectral theoretic methodologies. We will also use recently developed neural calibration techniques. In addition, we will also interpret the inference problem for SIPS as an inverse problem for the mean field PDE and then apply Bayesian methods for inverse problems. For this, the measurement model will be taken to be noisy measurements of the solution to the PDE, e.g. of the Keller-Segel or of the Onager model, and more generally of the nonlinear and nonlocal McKean-Vlasov mean field PDE.\n\n\n\n\nAn inference toolbox for SIPS and their mean field limit, including efficient SDE solvers for the SIPS. All of the methods that will be studied and developed (MLE, SGDCT, neural calibration) will be included in the toolbox that we will develop.\nSoftware for implementing Bayesian inverse problem methodologies to the mean field nonlinear, nonlocal PDEs."
  },
  {
    "objectID": "phd_projects/entries/Cotter_hybridmodelling.html",
    "href": "phd_projects/entries/Cotter_hybridmodelling.html",
    "title": "PDE-driven/data-driven hybrid modelling for data assimilation",
    "section": "",
    "text": "Data assimilation is the process of taking measurements from a system that is evolving in time (like the Earth’s weather, or water levels on a river network, etc) and using them to update knowledge about the current state of that system, so that a model can be used to produce future predictions of it. The goal of this project is to develop, analyse and implement new methods for performing data assimilation when the model has errors in it (due to e.g. unresolved processes below the gridscale). In our approach, we will use a stochastic formulation for these errors, which must itself be learned from data, either from precomputed high resolution simulation, or updated online during the data assimilation process."
  },
  {
    "objectID": "phd_projects/entries/Cotter_hybridmodelling.html#project-description",
    "href": "phd_projects/entries/Cotter_hybridmodelling.html#project-description",
    "title": "PDE-driven/data-driven hybrid modelling for data assimilation",
    "section": "",
    "text": "Data assimilation is the process of taking measurements from a system that is evolving in time (like the Earth’s weather, or water levels on a river network, etc) and using them to update knowledge about the current state of that system, so that a model can be used to produce future predictions of it. The goal of this project is to develop, analyse and implement new methods for performing data assimilation when the model has errors in it (due to e.g. unresolved processes below the gridscale). In our approach, we will use a stochastic formulation for these errors, which must itself be learned from data, either from precomputed high resolution simulation, or updated online during the data assimilation process."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "",
    "text": "Professor Alexandros Beskos has made fundamental research contributions regarding:\n\nthe development of computationally intensive Sequential Monte-Carlo (SMC) algorithms that can overcome the curse-of-dimensionality that characterises standard approaches [1];\nthe extension of the applicability of SMC algorithms on high-dimensional models, across a spectrum of application fields, including: physics-driven Data Assimilation problems in atmospheric sciences [6]; whole-genome applications in biomedicine [5]; multivariate Graphical models in finance [3]. The above research has been carried out in collaboration with expert colleagues, in the designated fields (Professor Dan Crisan, Dr Nikolas Kantas at Imperial College, UK; Professor Ajay Jasra at the Chinese University of Hong Kong in Shenzhen, China; Professor Maria de Iorio at the National University of Singapore; Prof Stephan Beck at the Cancer Institute, UCL)."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#project-description",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#project-description",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "",
    "text": "Professor Alexandros Beskos has made fundamental research contributions regarding:\n\nthe development of computationally intensive Sequential Monte-Carlo (SMC) algorithms that can overcome the curse-of-dimensionality that characterises standard approaches [1];\nthe extension of the applicability of SMC algorithms on high-dimensional models, across a spectrum of application fields, including: physics-driven Data Assimilation problems in atmospheric sciences [6]; whole-genome applications in biomedicine [5]; multivariate Graphical models in finance [3]. The above research has been carried out in collaboration with expert colleagues, in the designated fields (Professor Dan Crisan, Dr Nikolas Kantas at Imperial College, UK; Professor Ajay Jasra at the Chinese University of Hong Kong in Shenzhen, China; Professor Maria de Iorio at the National University of Singapore; Prof Stephan Beck at the Cancer Institute, UCL)."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-atmospheric-sciences",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-atmospheric-sciences",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of atmospheric sciences:",
    "text": "Direction of atmospheric sciences:\nSMC methods – in particular Particle Filters (PFs) – have only very recently started being attempted and used in high-dimensional Data Assimilation applications. In contrast to ensemble Kalman Filters (EnKFs) which are the particle-based algorithms typically used in Data Assimilation and are based on Gaussian approximations, PFs make use of the correct model and can recover strong non-linearities characterising dynamical systems, especially at high resolutions [7]. However, standard PF algorithms are known to suffer from the curse-of-dimensionality.\nRecent research has focused on the development of a new generation of PFs that can be effective for Data Assimilation applications in high dimensions. Such PFs are termed “Localised Particle Filters” (LPFs). LPFs can overcome the curse-of-dimensionality by replacing a single high-dimensional global importance sampling step – across the whole domain of the signal and for all set of arriving observations – with a number of ‘local’ importance sampling steps (at a lower dimension) that only make use of the subset of data that are informative for chosen subdomains. The localised approach is based on the simple remark that observations obtained at a given position of the domain of the signal contain minimal information for parts of the domain which are far from such a position.\nSuch methodological advances have led to LPFs very recently been tested in operational settings in Numerical Weather Forecasting (NWF) [8], where states can be of dimension of O(10^9) or higher."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-biomedicine",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-biomedicine",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of biomedicine:",
    "text": "Direction of biomedicine:\nThere is a need for the development of statistical models for genome-wide epigenetics data and of the accompanying computational methods for their calibration. Such computations can involve the use of state-of-the-art scalable SMC algorithms and High-Performance Computing (HPC), as datasizes can be of order of 10^8 or more. Appropriate models recently proposed in the literature include, e.g., change-point dynamics [5] that track DNA methylation patterns jointly over cases/controls, across the whole genome, and can identify particular positions along the genome where cases and controls have different patterns.\nFull Bayesian inference for genome-wide models can provide much more information for underlying patterns than competing methods. So far Biologists have relied on out-of-shelf approaches when the cost for producing the data in the lab is enormous, so maximised extraction of information from available data is of high significance. In general, there appears to be lack of expertise in the development of suitable statistical models in this field and in corresponding scalable computational methods for fitting these models."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-finance",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#direction-of-finance",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Direction of finance:",
    "text": "Direction of finance:\nHigh-dimensional applications, involving dynamical models, are abundant in finance, see e.g. [9]. Recent advances in generic computational SMC methodology have yet to be fully appreciated by researchers focused on applications in this area."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#main-objectives-of-the-project",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#main-objectives-of-the-project",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Main objectives of the project",
    "text": "Main objectives of the project\nThe main objectives of the project will depend on the particular application domain of interest, and can be summarised as follows:\n\nInvestigate state-of-the-art SMC algorithms. Such a study is important given several new methods proposed in recent research in the field of Data Assimilation (see e.g. [4]) or in a general context in (see e.g. [2]).\nExplore key strengths/weaknesses of SMC algorithms and contrast approaches against alternative biased methods (e.g. EnKFs in atmospheric sciences).\nStudy performance of advanced SMC methods across a number of benchmark model scenarios.\nDevelop novel SMC algorithms which make use of expert methodology available in the SMC community, but which has yet to be fully used by researchers focused on particular application domains. Algorithmic tools already shown to greatly improve performance of SMC involve e.g.: tempering accompanied by jittering of particles to better disperse the latter across space; adaptation techniques; data-driven improved proposals for particles. Such procedures aim to generate particles within a Monte-Carlo framework which represent effectively the hidden signal. This latter signal can be modelled, e.g., via PDE/SPDE dynamics in atmospheric sciences, change point models in genome-wide applications in biomedicine, and time-evolving Graphical models in finance."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#details-of-softwaredata-deliverables",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "Details of Software/Data deliverables",
    "text": "Details of Software/Data deliverables\nThere is an apparent lack of software for implementation of advanced SMC methods in all three areas of application highlighted above (and beyond). Provision of such software will be a key output delivered by the PhD project, given the chosen application field. Indicatively, in the field of atmospheric sciences, the comprehensive review work in [4] provides a package in GitHub (https://github.com/thiery-lab/data-assimilation) but the purpose of this software seems to be the reproducibility of the results in the accompanying paper rather than a general-purpose software. In the case of genome-wide applications in biomedicine, Professor Alexandros Beskos has been involved in the development of the package in https://github.com/ucl-medical-genomics/hygeia, but this is still at its infancy. Thus, the proposed project will provide an opportunity for a trainee in Computational Modelling to develop a software that will cover a gap in an important application field – selected based on the applicant’s interests. As such, the produced software can have a large impact. By their very nature, SMC algorithms require state-of-the-art parallelisation and High-Performance Computing (HPC) implementations. Computations will be carried out within CPU or GPU architectures, depending on the problem at hand, or on the Cloud."
  },
  {
    "objectID": "phd_projects/entries/Beskos_bayesiancalibration.html#references",
    "href": "phd_projects/entries/Beskos_bayesiancalibration.html#references",
    "title": "Advanced Computational Methods for Bayesian Calibration of High-Dimensional Dynamical Models",
    "section": "References",
    "text": "References\n[1] Beskos, A., Crisan D., Jasra, A. (2014). On the stability of sequential Monte Carlo methods in high dimensions. Annals of Applied Probability, 24, 1396-1445.\n[2] Finke, A., Thiery, A. (2023). Conditional sequential Monte Carlo in high dimensions. Annals of Statistics 51, 437-463.\n[3] Franzolini, B., Beskos, A., De Iorio, M., Poklewski Koziell, W., Grzeszkiewicz, K. (2024). Change point detection in dynamic Gaussian graphical models: the impact of COVID-19 pandemic on the US stock market. Annals of Applied Statistics 18, 555-584.\n[4] Graham, M., Thiery, A. (2019). A scalable optimal-transport based local particle filter. arXiv preprint arXiv:1906.00507."
  },
  {
    "objectID": "phd_projects/entries/Briol_simulation.html",
    "href": "phd_projects/entries/Briol_simulation.html",
    "title": "Simulation-based Inference for Expensive Scientific Simulators",
    "section": "",
    "text": "In many domains of science and engineering, statistical inference is challenging due to the unavailability of the likelihood associated to the scientific model of interest. Instead, it is often possible to simulate from these models, and to use these simulations to perform approximate inference. One challenge in this context is that these procedures typically rely on a large numbers of model simulations, which is a challenge when the simulator is computationally costly to run. Examples of such models include tsunami models based on non-linear shallow water equations, which take several minutes per run, or large eddy simulations of wind farms, which can take tens of hours per run. In these settings, statistical inference can be either extremely computationally demanding, or sometimes simply unfeasible.\n\n\n\nThe main aim of this project will be to develop novel simulation-based inference algorithms which are particularly suitable for inference with expensive scientific simulators. Different directions will be explored, including combining multi-fidelity, cost-aware and amortised methods. These should lead to novel algorithms which require far fewer expensive simulations, or where each simulation is cheaper than for existing methods.\n\n\n\nThe main development of software will be implementations in Python of these algorithms. The aim will be to contribute this code to existing software packages for simulation-based inference, including the pytorch-based package “sbi” and the package “BayesFlow”."
  },
  {
    "objectID": "phd_projects/entries/Briol_simulation.html#project-description",
    "href": "phd_projects/entries/Briol_simulation.html#project-description",
    "title": "Simulation-based Inference for Expensive Scientific Simulators",
    "section": "",
    "text": "In many domains of science and engineering, statistical inference is challenging due to the unavailability of the likelihood associated to the scientific model of interest. Instead, it is often possible to simulate from these models, and to use these simulations to perform approximate inference. One challenge in this context is that these procedures typically rely on a large numbers of model simulations, which is a challenge when the simulator is computationally costly to run. Examples of such models include tsunami models based on non-linear shallow water equations, which take several minutes per run, or large eddy simulations of wind farms, which can take tens of hours per run. In these settings, statistical inference can be either extremely computationally demanding, or sometimes simply unfeasible.\n\n\n\nThe main aim of this project will be to develop novel simulation-based inference algorithms which are particularly suitable for inference with expensive scientific simulators. Different directions will be explored, including combining multi-fidelity, cost-aware and amortised methods. These should lead to novel algorithms which require far fewer expensive simulations, or where each simulation is cheaper than for existing methods.\n\n\n\nThe main development of software will be implementations in Python of these algorithms. The aim will be to contribute this code to existing software packages for simulation-based inference, including the pytorch-based package “sbi” and the package “BayesFlow”."
  },
  {
    "objectID": "phd_projects/entries/Tobar_diffusion_models.html",
    "href": "phd_projects/entries/Tobar_diffusion_models.html",
    "title": "Aligned diffusion models for machine learning",
    "section": "",
    "text": "Diffusion models (DM) are the state of the art on generative modelling, their ability to learn (implicit) probabilistic models over complex structured datasets is unparalleled and it has been validated on images and audio in a number of applications. The generality and wide applicability of DMs motivates a variety of research directions including fairness, reinforcement-learning-based enhancement, prevention of mode collapse, AI alignment, and accelerated computation. Alignment, in particular, is a much-desired feature in DMs as they are currently being deployed for use by the general public, where they might deal with sensitive data and critical decision making.\nWe have developed fine-tuning techniques for DM with the principal aim of aligning the samples generated by the DM to human criteria. Our contributions have been tested on novel DM samplers that follow objectives that are difficult to describe by a standard discriminant function. This includes producing samples that are aesthetic, incompressible, or that do not include violent content (e.g., explicit images).\n\n\n\nTo design and validate strategies to align diffusion models with human criteria. The project comprises both theoretical and computational aspects.\n\nTo explore the state of the art in DMs and the techniques currently used for guidance and finetuning.\nTo understand current alignment techniques using, e.g., guidance and reinforcement learning\nTo identify which tools in the ML literature and related resources from computational mathematics, optimisation, statistics, and probability, can be used to propose control (alignment) loops for sampling in DMs\nTo design an experiment, and an experimental setup, to validate the proposed alignment techniques in applications involving fairness, social sciences, health, or general generative modelling\nTo analyse, both from theoretical and practical perspectives, the developed alignment techniques so as to provide convincing evidence of alignment in DMs\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML community\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing applications to scientific or social challenges. Describe coding and data developments during the project. See above."
  },
  {
    "objectID": "phd_projects/entries/Tobar_diffusion_models.html#project-description",
    "href": "phd_projects/entries/Tobar_diffusion_models.html#project-description",
    "title": "Aligned diffusion models for machine learning",
    "section": "",
    "text": "Diffusion models (DM) are the state of the art on generative modelling, their ability to learn (implicit) probabilistic models over complex structured datasets is unparalleled and it has been validated on images and audio in a number of applications. The generality and wide applicability of DMs motivates a variety of research directions including fairness, reinforcement-learning-based enhancement, prevention of mode collapse, AI alignment, and accelerated computation. Alignment, in particular, is a much-desired feature in DMs as they are currently being deployed for use by the general public, where they might deal with sensitive data and critical decision making.\nWe have developed fine-tuning techniques for DM with the principal aim of aligning the samples generated by the DM to human criteria. Our contributions have been tested on novel DM samplers that follow objectives that are difficult to describe by a standard discriminant function. This includes producing samples that are aesthetic, incompressible, or that do not include violent content (e.g., explicit images).\n\n\n\nTo design and validate strategies to align diffusion models with human criteria. The project comprises both theoretical and computational aspects.\n\nTo explore the state of the art in DMs and the techniques currently used for guidance and finetuning.\nTo understand current alignment techniques using, e.g., guidance and reinforcement learning\nTo identify which tools in the ML literature and related resources from computational mathematics, optimisation, statistics, and probability, can be used to propose control (alignment) loops for sampling in DMs\nTo design an experiment, and an experimental setup, to validate the proposed alignment techniques in applications involving fairness, social sciences, health, or general generative modelling\nTo analyse, both from theoretical and practical perspectives, the developed alignment techniques so as to provide convincing evidence of alignment in DMs\nTo ensure availability and dissemination of the project contributions in the form of free (open source) software which is compatible with other toolbox of the ML community\n\n\n\n\nThe conceptual contributions of the project are expected to be complemented with reproducible experimental validation. This includes i) open-source software to be used by the scientific community, ii) a public repository hosting the developed software, iii) reproducible examples showing applications to scientific or social challenges. Describe coding and data developments during the project. See above."
  },
  {
    "objectID": "phd_projects/entries/Graham_multilevelbayesian.html",
    "href": "phd_projects/entries/Graham_multilevelbayesian.html",
    "title": "Automated Bayesian inference in stochastic differential equation models",
    "section": "",
    "text": "Diffusion processes specified by systems of stochastic differential equations are used to model phenomena in a wide range of settings. Inferring the posterior distribution on the unknown parameters of such models given, potentially partial and noisy, observations of the process is often a computationally demanding task, with even forward simulation of the model typically intractable, necessitating use of numerical integration schemes. A common approach in this setting is to use data augmentation, whereby the paths of the time discretised diffusion process are jointly inferred with the model parameters. A plethora of approximate inference schemes have been proposed in this setting, with key challenges from a computational statistics standpoint being the high-dimension of the resulting latent space when a fine time-discretization is used, and the complex, typically non-linear, dependencies between the latent variables, resulting in a challenging to approximate posterior distribution geometry. The appropriate choice of numerical integration scheme will often also be model dependent, with properties such as hypoellipticity whereby the diffusion process includes a mix of rough and smooth components, requiring careful treatment to ensure the time-discretized process retains key properties of the underlying continuous time system.\nA promising recently proposed inference approach in this setting is to consider the joint configurations of the time discretized diffusion process and model parameters which are consistent with the observations as forming an implicitly defined manifold in the latent space, with a constrained Hamiltonian Monte Carlo algorithm then used to sample from the resulting manifold supported joint posterior distribution (Graham, Thiery and Beskos, 2022). In contrast with other approaches proposed in the literature, this methodology can be applied alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. In particular, full flexibility is available in the choice of numerical integration scheme, allowing straightforward use of higher-order integrators without a tractable transition density such as those proposed in a hypo-elliptic setting (Iguchi, Beskos and Graham, 2023).\n\nGraham, M. M., Thiery, A. H., & Beskos, A. (2022). Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4), 1229-1256.\nIguchi, Y., Beskos, A., & Graham, M. (2023). Parameter Inference for Degenerate Diffusion Processes. arXiv preprint arXiv:2307.16485.\n\n\n\n\nWhile there has been much work on designing increasingly efficient, but also complex, approximate inference and numerical integration schemes for diffusion processes, there is lack of corresponding robust general-purpose software implementations which abstract details of the inference algorithm and numerical discretization away from practitioners. Probabilistic programming frameworks such as BUGS, Stan, PyMC and Turing, which combine domain specific languages to specify probabilistic generative models, with general purpose inference algorithms that can be used to estimate the unknown variables in the model, have been very successful in encouraging adoption of Bayesian methodology and are widely used in practice in a range of fields. However, while it is possible to use existing frameworks to perform inference in diffusion models, typically this will still require the user to manually implement the time discretisation of the process. Further, the default inference algorithms used in these frameworks will typically scale poorly to high-dimensional and complex posteriors characteristic of diffusion models.\nIn this project we propose to produce a dedicated open-source software framework for performing inference in stochastic differential equation models. This will combine a simple interface for practitioners to specify their model of interest, automatic detection of key system properties such as hypoellipticity and selection of appropriate discretization schemes, and general purpose implementations of a range of numerical integration schemes and inference algorithms. As well developing the underlying software framework, there will also be the opportunity to develop novel statistical algorithms for performing inference in diffusion models to integrate in to the framework and to design automated approaches for tuning and adapting the control parameters of existing methodology, to minimize the need for user-intervention."
  },
  {
    "objectID": "phd_projects/entries/Graham_multilevelbayesian.html#project-description",
    "href": "phd_projects/entries/Graham_multilevelbayesian.html#project-description",
    "title": "Automated Bayesian inference in stochastic differential equation models",
    "section": "",
    "text": "Diffusion processes specified by systems of stochastic differential equations are used to model phenomena in a wide range of settings. Inferring the posterior distribution on the unknown parameters of such models given, potentially partial and noisy, observations of the process is often a computationally demanding task, with even forward simulation of the model typically intractable, necessitating use of numerical integration schemes. A common approach in this setting is to use data augmentation, whereby the paths of the time discretised diffusion process are jointly inferred with the model parameters. A plethora of approximate inference schemes have been proposed in this setting, with key challenges from a computational statistics standpoint being the high-dimension of the resulting latent space when a fine time-discretization is used, and the complex, typically non-linear, dependencies between the latent variables, resulting in a challenging to approximate posterior distribution geometry. The appropriate choice of numerical integration scheme will often also be model dependent, with properties such as hypoellipticity whereby the diffusion process includes a mix of rough and smooth components, requiring careful treatment to ensure the time-discretized process retains key properties of the underlying continuous time system.\nA promising recently proposed inference approach in this setting is to consider the joint configurations of the time discretized diffusion process and model parameters which are consistent with the observations as forming an implicitly defined manifold in the latent space, with a constrained Hamiltonian Monte Carlo algorithm then used to sample from the resulting manifold supported joint posterior distribution (Graham, Thiery and Beskos, 2022). In contrast with other approaches proposed in the literature, this methodology can be applied alike in a range of settings, including: elliptic or hypo-elliptic systems; observations with or without noise; linear or non-linear observation operators. In particular, full flexibility is available in the choice of numerical integration scheme, allowing straightforward use of higher-order integrators without a tractable transition density such as those proposed in a hypo-elliptic setting (Iguchi, Beskos and Graham, 2023).\n\nGraham, M. M., Thiery, A. H., & Beskos, A. (2022). Manifold Markov chain Monte Carlo methods for Bayesian inference in diffusion models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4), 1229-1256.\nIguchi, Y., Beskos, A., & Graham, M. (2023). Parameter Inference for Degenerate Diffusion Processes. arXiv preprint arXiv:2307.16485.\n\n\n\n\nWhile there has been much work on designing increasingly efficient, but also complex, approximate inference and numerical integration schemes for diffusion processes, there is lack of corresponding robust general-purpose software implementations which abstract details of the inference algorithm and numerical discretization away from practitioners. Probabilistic programming frameworks such as BUGS, Stan, PyMC and Turing, which combine domain specific languages to specify probabilistic generative models, with general purpose inference algorithms that can be used to estimate the unknown variables in the model, have been very successful in encouraging adoption of Bayesian methodology and are widely used in practice in a range of fields. However, while it is possible to use existing frameworks to perform inference in diffusion models, typically this will still require the user to manually implement the time discretisation of the process. Further, the default inference algorithms used in these frameworks will typically scale poorly to high-dimensional and complex posteriors characteristic of diffusion models.\nIn this project we propose to produce a dedicated open-source software framework for performing inference in stochastic differential equation models. This will combine a simple interface for practitioners to specify their model of interest, automatic detection of key system properties such as hypoellipticity and selection of appropriate discretization schemes, and general purpose implementations of a range of numerical integration schemes and inference algorithms. As well developing the underlying software framework, there will also be the opportunity to develop novel statistical algorithms for performing inference in diffusion models to integrate in to the framework and to design automated approaches for tuning and adapting the control parameters of existing methodology, to minimize the need for user-intervention."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "",
    "text": "The researcher on this project will develop new modelling capabilities in the differentiable programming paradigm that will enable gradient-based optimisation to be employed in complex systems whose models combine partial differential equations (PDEs), differential-algebraic equations (DAEs), and machine learning (ML).\nAcross physics-based and data-driven modelling, the need to optimise the modelled system for an objective is pervasive. Optimisation objectives can include the fit to observed data, minimisation of resource consumption such as energy or materials, or performance criteria such as speed or weight. Optimising these objectives requires the entire mathematical model to be differentiated with respect to its inputs. In many circumstances, the only tractable way to do this is using reverse mode algorithmic differentiation, known in the ML community as backpropagation. In short, the model code is differentiated and run backwards.\nThe most realistic and challenging model systems combine components represented in different ways. A complex, energy-intensive and polluting industrial process might contain mechanical components modelled by DAEs, fluid processes described by PDEs and have components for which no model is known but for which a neural network can be trained from data. Optimising the design of such systems has enormous environmental and economic benefits. This is the big prize.\nTraditional algorithmic differentiation tools started with a model written in a language such as Fortran or C and differentiated at the level of the primitive options. The much more modern approach is to design a programming model to be differentiated. This enables much more efficient differentiation of higher level mathematical structures. This approach was termed “differentiable programming” in the ML community (Meijer, ESEC/FSE 2018), but similar approaches exist in PDE-based modelling (Farrell et al. 2013) and for DAE models (`-Hart 2011).\nDifferentiable programming treats computational models as the mathematical composition of differentiable components, and applies the chain rule to differentiate across multiple components. By matching the mathematical and software abstractions of differentiable programming tools in ML, PDEs, and DAEs, differentiable models of the most complex coupled systems will be made possible. Initial work in this field has coupled ML and PDEs (Bouziani & Ham, 2023) and ML and DAEs (Ceccon et al., 2022). This project will build the bridge between PDEs and DAEs using the software tools in those works and hence complete the triangle, delivering seamless differentiable modelling and optimisation between all three domains."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#project-description",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#project-description",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "",
    "text": "The researcher on this project will develop new modelling capabilities in the differentiable programming paradigm that will enable gradient-based optimisation to be employed in complex systems whose models combine partial differential equations (PDEs), differential-algebraic equations (DAEs), and machine learning (ML).\nAcross physics-based and data-driven modelling, the need to optimise the modelled system for an objective is pervasive. Optimisation objectives can include the fit to observed data, minimisation of resource consumption such as energy or materials, or performance criteria such as speed or weight. Optimising these objectives requires the entire mathematical model to be differentiated with respect to its inputs. In many circumstances, the only tractable way to do this is using reverse mode algorithmic differentiation, known in the ML community as backpropagation. In short, the model code is differentiated and run backwards.\nThe most realistic and challenging model systems combine components represented in different ways. A complex, energy-intensive and polluting industrial process might contain mechanical components modelled by DAEs, fluid processes described by PDEs and have components for which no model is known but for which a neural network can be trained from data. Optimising the design of such systems has enormous environmental and economic benefits. This is the big prize.\nTraditional algorithmic differentiation tools started with a model written in a language such as Fortran or C and differentiated at the level of the primitive options. The much more modern approach is to design a programming model to be differentiated. This enables much more efficient differentiation of higher level mathematical structures. This approach was termed “differentiable programming” in the ML community (Meijer, ESEC/FSE 2018), but similar approaches exist in PDE-based modelling (Farrell et al. 2013) and for DAE models (`-Hart 2011).\nDifferentiable programming treats computational models as the mathematical composition of differentiable components, and applies the chain rule to differentiate across multiple components. By matching the mathematical and software abstractions of differentiable programming tools in ML, PDEs, and DAEs, differentiable models of the most complex coupled systems will be made possible. Initial work in this field has coupled ML and PDEs (Bouziani & Ham, 2023) and ML and DAEs (Ceccon et al., 2022). This project will build the bridge between PDEs and DAEs using the software tools in those works and hence complete the triangle, delivering seamless differentiable modelling and optimisation between all three domains."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#justification-of-how-it-fits-the-scope-of-the-programme",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#justification-of-how-it-fits-the-scope-of-the-programme",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "Justification of how it fits the scope of the programme",
    "text": "Justification of how it fits the scope of the programme\nThis is a software-centred project spanning physics-based and data-driven modelling approaches. It is at the core of the CDT scope."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#details-of-softwaredata-deliverables",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#details-of-softwaredata-deliverables",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "Details of Software/Data Deliverables",
    "text": "Details of Software/Data Deliverables\nThe new simulation capability will be delivered as extensions to the Firedrake (https://www.firedrakeproject.org/), OMLT project (https://github.com/cog-imperial/OMLT), Pyadjoint (https://pyadjoint.org/) and/or Pyomo (http://www.pyomo.org/)."
  },
  {
    "objectID": "phd_projects/entries/MisenerHam_DAEinterface.html#references",
    "href": "phd_projects/entries/MisenerHam_DAEinterface.html#references",
    "title": "Algorithms and software for differentiable programming at the ML/PDE/DAE interface",
    "section": "References",
    "text": "References\nBouziani, Nacime and Ham, David A. Physics-driven machine learning models coupling PyTorch and Firedrake ICLR 2023 Workshop on Physics for Machine Learning, 2023 https://doi.org/10.48550/arXiv.2303.06871 Francesco Ceccon, Jordan Jalving, Joshua Haddad, Alexander Thebelt, Calvin Tsay, Carl D Laird, and Ruth Misener. 2022. OMLT: optimization & machine learning toolkit. J. Mach. Learn. Res. 23, 1, Article 349 P. E. Farrell, D. A. Ham, S. W. Funke, and M. E. Rognes, Automated Derivation of the Adjoint of High-Level Transient Finite Element Programs, SIAM Journal on Scientific Computing 2013 35:4, C369-C393 Hart, William E., Jean-Paul Watson, and David L. Woodruff. “Pyomo: modeling and solving mathematical programs in Python.” Mathematical Programming Computation 3(3) (2011): 219-260. Meijer, Erik. “Behind every great deep learning framework is an even greater programming languages concept (keynote).” Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2018."
  },
  {
    "objectID": "phd_projects/entries/Briol_Montecarlo.html",
    "href": "phd_projects/entries/Briol_Montecarlo.html",
    "title": "Transfer Learning for Monte Carlo",
    "section": "",
    "text": "The problem of computing quantities of interest taking the form of intractable expectations is widespread in computational statistics, machine learning, and more broadly in the computational sciences. In practice we often need to tackle several integrals which are similar; e.g. related through time or space. However, the vast majority of existing research focuses on refining approximations of a single integral and completely ignores the fact that these are part of a set of related integrals. This key piece of information, if used appropriately, could allow algorithms to share computation across integration tasks, leading to much more accurate estimates. Although a small number of such methods have been proposed, their use is currently very limited due to their high computational cost and the lack of widely available software.\n\n\n\nThe objectives of this project are two-fold: 1) Developing novel algorithms for the transfer of information across integration tasks. Most existing algorithms have a cost which increases cubically with the number of tasks and/or with the number of samples per task. This means that they are not worthwhile in all but the most challenging integration tasks. In this project, the student will explore approximation methods to reduce this from cubic to linear cost. This work will build on the literatures on Monte Carlo methods, kernel methods and Gaussian processes. 2) Developing software for the widespread use of transfer learning for numerical integration. The ProbNum package in Python already includes a basic algorithm to do this, but there is very little support in terms of hyper parameter optimisation and/or advice for practitioners on how to use the methods. Many other methods are also missing. The second objective of this project will therefore be the development of this package and implementation of any novel algorithms developed in the first part of the project. The broader aim will be to make these algorithms broadly available to allow users beyond statistics/machine learning to make use of these methods.\n\n\n\nThe PhD student will contribute to the development of the “ProbNum” python package; see https://probnum.readthedocs.io/en/latest/. Specifically, they will focus on the implementation of existing, and novel, methods for numerical integration, with a specific focus on methods based on transfer learning. This will be done in collaboration with colleagues and other PhD students at the University of Tuebingen, which are the leads for this package."
  },
  {
    "objectID": "phd_projects/entries/Briol_Montecarlo.html#project-description",
    "href": "phd_projects/entries/Briol_Montecarlo.html#project-description",
    "title": "Transfer Learning for Monte Carlo",
    "section": "",
    "text": "The problem of computing quantities of interest taking the form of intractable expectations is widespread in computational statistics, machine learning, and more broadly in the computational sciences. In practice we often need to tackle several integrals which are similar; e.g. related through time or space. However, the vast majority of existing research focuses on refining approximations of a single integral and completely ignores the fact that these are part of a set of related integrals. This key piece of information, if used appropriately, could allow algorithms to share computation across integration tasks, leading to much more accurate estimates. Although a small number of such methods have been proposed, their use is currently very limited due to their high computational cost and the lack of widely available software.\n\n\n\nThe objectives of this project are two-fold: 1) Developing novel algorithms for the transfer of information across integration tasks. Most existing algorithms have a cost which increases cubically with the number of tasks and/or with the number of samples per task. This means that they are not worthwhile in all but the most challenging integration tasks. In this project, the student will explore approximation methods to reduce this from cubic to linear cost. This work will build on the literatures on Monte Carlo methods, kernel methods and Gaussian processes. 2) Developing software for the widespread use of transfer learning for numerical integration. The ProbNum package in Python already includes a basic algorithm to do this, but there is very little support in terms of hyper parameter optimisation and/or advice for practitioners on how to use the methods. Many other methods are also missing. The second objective of this project will therefore be the development of this package and implementation of any novel algorithms developed in the first part of the project. The broader aim will be to make these algorithms broadly available to allow users beyond statistics/machine learning to make use of these methods.\n\n\n\nThe PhD student will contribute to the development of the “ProbNum” python package; see https://probnum.readthedocs.io/en/latest/. Specifically, they will focus on the implementation of existing, and novel, methods for numerical integration, with a specific focus on methods based on transfer learning. This will be done in collaboration with colleagues and other PhD students at the University of Tuebingen, which are the leads for this package."
  },
  {
    "objectID": "phd_projects/entries/cotter_parallel_time.html",
    "href": "phd_projects/entries/cotter_parallel_time.html",
    "title": "Data-driven preconditioning for parallel-in-time PDE solvers",
    "section": "",
    "text": "In our group we are developing solvers for parallel-in-time simulation based on a technique called ParaDiag. The goal of parallel-in-time methods is to solve time-dependent PDEs using a parallel computer with computation across several time steps at once in addition to the usual parallelism across spatial subdomains, to yield additional parallelism so that we can get the solution to bigger problems more quickly. ParaDiag is a computational linear algebra trick for solving the implicit system coupling together several time steps at once. For problems with waves and/or transport (such as those arising in weather and climate, or fusion plasma simulation) the big challenge is that ParaDiag requires the solution of systems that look like backward Euler applied to the original system but with a complex valued timestep! This complex valued aspect causes trouble for standard iterative solvers, and here we plan to use an iteration consisting of solving the linear system restricted independently on each of a set of overlapping patches, inspired by Graham et al (2020) which solved similar problems in the context of frequency domain wave equations. From that work it is evident that the patch systems must have an (approximate) absorbing boundary condition for the solver to converge well. For nondispersive wave equations and pure advection problems it is clear what these boundary conditions should be, but for nonlinear problems the theory is incomplete. In this project we propose to use a data-driven approach, taking solutions of the backward Euler problem, and restricting them to patches as input data, before fitting parameters using an appropriate model (such as a neural network, random features model, etc.).\n\nGraham, Ivan G., Euan A. Spence, and Jun Zou. “Domain decomposition with local impedance conditions for the Helmholtz equation with absorption.” SIAM Journal on Numerical Analysis 58, no. 5 (2020): 2515-2543.\n\n\n\nThe history of the ParaDiag framework is surveyed in Gander et al (2020); we are pursuing the ParaDiag II approach as it is described in that article, particularly focussing on achieving practical performance for nonlinear PDEs. Funded by the UK ExCALIBUR project led by UKRI, the Met Office, UKAEA and STFC, we have developed a software library, asQ (Hope-Collins et al (2024)) for investigating ParaDiag algorithms and benchmarking them on high performance supercomputers. This has involved specific test cases relevant to weather and climate, fusion simulation and computational geology, but the scope is broad. asQ is built using Firedrake (firedrakeproject.org) which has a PyTorch interface for seamless integration of neural networks (Bouziani, Ham and Farsi (2024)).\n\nGander, Martin J., Jun Liu, Shu-Lin Wu, Xiaoqiang Yue, and Tao Zhou. “Paradiag: Parallel-in-time algorithms based on the diagonalization technique.” arXiv preprint arXiv:2005.09158 (2020).\nHope-Collins, Joshua, Abdalaziz Hamdan, Werner Bauer, Lawrence Mitchell, and Colin Cotter. “asQ: parallel-in-time finite element simulations using ParaDiag for geoscientific models and beyond.” arXiv preprint arXiv:2409.18792 (2024).\nBouziani, Nacime, David A. Ham, and Ado Farsi. “Differentiable programming across the PDE and Machine Learning barrier.” arXiv preprint arXiv:2409.06085 (2024).\n\n\n\n\nThe objectives are: * To develop a data-driven approach to absorbing boundary conditions for ParaDiag problems * To build up the capability through dispersive wave equations, coupled wave-transport problems, and the problems arising from ParaDiag applied to specific nonlinear PDEs\n\n\n\n\nimplemented preconditioners for asQ using petsc4py integrated with Firedrake\ntuned parameters for example problems\neverything developed open source on Github"
  },
  {
    "objectID": "phd_projects/entries/cotter_parallel_time.html#project-description",
    "href": "phd_projects/entries/cotter_parallel_time.html#project-description",
    "title": "Data-driven preconditioning for parallel-in-time PDE solvers",
    "section": "",
    "text": "In our group we are developing solvers for parallel-in-time simulation based on a technique called ParaDiag. The goal of parallel-in-time methods is to solve time-dependent PDEs using a parallel computer with computation across several time steps at once in addition to the usual parallelism across spatial subdomains, to yield additional parallelism so that we can get the solution to bigger problems more quickly. ParaDiag is a computational linear algebra trick for solving the implicit system coupling together several time steps at once. For problems with waves and/or transport (such as those arising in weather and climate, or fusion plasma simulation) the big challenge is that ParaDiag requires the solution of systems that look like backward Euler applied to the original system but with a complex valued timestep! This complex valued aspect causes trouble for standard iterative solvers, and here we plan to use an iteration consisting of solving the linear system restricted independently on each of a set of overlapping patches, inspired by Graham et al (2020) which solved similar problems in the context of frequency domain wave equations. From that work it is evident that the patch systems must have an (approximate) absorbing boundary condition for the solver to converge well. For nondispersive wave equations and pure advection problems it is clear what these boundary conditions should be, but for nonlinear problems the theory is incomplete. In this project we propose to use a data-driven approach, taking solutions of the backward Euler problem, and restricting them to patches as input data, before fitting parameters using an appropriate model (such as a neural network, random features model, etc.).\n\nGraham, Ivan G., Euan A. Spence, and Jun Zou. “Domain decomposition with local impedance conditions for the Helmholtz equation with absorption.” SIAM Journal on Numerical Analysis 58, no. 5 (2020): 2515-2543.\n\n\n\nThe history of the ParaDiag framework is surveyed in Gander et al (2020); we are pursuing the ParaDiag II approach as it is described in that article, particularly focussing on achieving practical performance for nonlinear PDEs. Funded by the UK ExCALIBUR project led by UKRI, the Met Office, UKAEA and STFC, we have developed a software library, asQ (Hope-Collins et al (2024)) for investigating ParaDiag algorithms and benchmarking them on high performance supercomputers. This has involved specific test cases relevant to weather and climate, fusion simulation and computational geology, but the scope is broad. asQ is built using Firedrake (firedrakeproject.org) which has a PyTorch interface for seamless integration of neural networks (Bouziani, Ham and Farsi (2024)).\n\nGander, Martin J., Jun Liu, Shu-Lin Wu, Xiaoqiang Yue, and Tao Zhou. “Paradiag: Parallel-in-time algorithms based on the diagonalization technique.” arXiv preprint arXiv:2005.09158 (2020).\nHope-Collins, Joshua, Abdalaziz Hamdan, Werner Bauer, Lawrence Mitchell, and Colin Cotter. “asQ: parallel-in-time finite element simulations using ParaDiag for geoscientific models and beyond.” arXiv preprint arXiv:2409.18792 (2024).\nBouziani, Nacime, David A. Ham, and Ado Farsi. “Differentiable programming across the PDE and Machine Learning barrier.” arXiv preprint arXiv:2409.06085 (2024).\n\n\n\n\nThe objectives are: * To develop a data-driven approach to absorbing boundary conditions for ParaDiag problems * To build up the capability through dispersive wave equations, coupled wave-transport problems, and the problems arising from ParaDiag applied to specific nonlinear PDEs\n\n\n\n\nimplemented preconditioners for asQ using petsc4py integrated with Firedrake\ntuned parameters for example problems\neverything developed open source on Github"
  },
  {
    "objectID": "phd_projects/entries/Benning_Paralift.html",
    "href": "phd_projects/entries/Benning_Paralift.html",
    "title": "PARALIFT – A PARAllel framework for LIFTed training of neural networks",
    "section": "",
    "text": "This project is founded on innovative advancements in distributed neural network training. The cornerstone of this work, as proposed in [1], is a framework for the distributed training of deep neural networks innovatively replaces the traditional neural network consistency constraint with quadratic penalisations of that constraint. These quadratic penalisations have the advantage that they remove the need for back-propagation and pave the way for distributed optimisation algorithms that can distribute the training of individual layers across different workers. At the same time, the framework requires the introduction of auxiliary variables, which lifts the parameter search space into a higher dimensional space. This framework has subsequently been augmented by constraining the auxiliary variables to convex sets [2], and its quadratic penalties have been replaced with more bespoke penalty functions [3, 4]. In [4], the penalty functions have been replaced with tailored Bregman distance functions that, in combination with suitable optimisation algorithms, bypass the requirement of differentiating activation functions in the process. Building on this foundation, the lifted Bregman framework has been effectively applied to address the inverse problem in neural networks [5]. The objective here is to deduce the input of a pre-trained network from its output. In [6], the proposed framework has also been shown to be an effective tool for the computation of source condition elements, which are important quantities in the context of inverse problems. Furthermore, the method therein has been adapted to efficiently compute optimal sampling patterns for magnetic resonance imaging.\n\n\n\nWhile the background work has showcased the versatility of the quadratic penalisation and lifted Bregman methods in smaller-scale applications, their full potential in distributed neural network training remains largely untapped. This project aims to tap this potential by democratising access to the distributed lifted Bregman framework for the broader research community. The main objective is to develop a user-friendly software toolbox, designed to simplify the implementation of deep neural network training for large-scale datasets using the lifted Bregman framework. This toolbox will be as straightforward to use as existing toolboxes like PyTorch or JAX. A key advantage of this new toolbox is its capacity for massive parallelisation, enabling the distribution of parameter updates across various workers for individual layers, while concurrently avoiding the need for differentiating non-differentiable operations.\nThe specific main objectives of this PhD project include (but are not limited to): - Developing a Comprehensive Software Toolbox: Creating an efficient and effective toolbox for implementing the lifted Bregman framework, tailored to both ease of use and high performance. - Advancing Medical Imaging Techniques: Applying this innovative framework to medical imaging applications, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), for large-scale datasets. - Enhancing Variational Regularisations: Focusing on the training of variational regularisations with optimal source condition elements as a novel use-case. - Inverting Deep Neural Networks: Employing the lifted Bregman inversion framework for the challenging task of inverting deep neural networks, particularly in contexts involving large-scale datasets.\n\n\n\nThis project is designed to propel the use of the lifted Bregman framework from its current state with limited use cases to mass adoption. To do so, the project involves the development a Python library or Julia package (depending on the candidate’s prior programming experience and skillset) for distributed optimisation of the lifted Bregman objectives. The goal of this library or package is to make it as easy setting up complex use cases, e.g. the training of very deep transformer-based neural network architectures for large-scale datasets, as it currently is in popular automatic differentiation/deep learning frameworks such as PyTorch and JAX, but to efficiently utilise distributed training routines tailored to the lifted Bregman framework without users having to implement any of this functionality manually. In particular, the library or package will - allow users to set up a wide range of different and versatile architectures and optimisation problems. - support popular training techniques, such as batch normalisation. - feature a variety of different distributed, non-smooth optimisation routines, including distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - utilise distributed hardware architectures with little to no user input to enable both data and layer parallelism.\nThis project aims to elevate the lifted Bregman framework from its current state of limited use-cases to widespread use in the research community. Central to this objective is the development of either a Python library or a Julia package, depending on the candidate’s programming expertise and skills. This tool will facilitate distributed optimisation of lifted Bregman objectives, making the setup of complex scenarios as effortless as using established automatic differentiation and deep learning frameworks like PyTorch and JAX. However, it uniquely focuses on the efficient employment of distributed training routines specific to the lifted Bregman framework, all without requiring users having to engage with the intricate details of manual implementation. Key features of this library or package include (but are not limited to): - Versatile Architecture and Optimisation Setup: Enabling users to effortlessly configure a broad spectrum of deep neural network architectures, loss functions, constraints, and optimisation problems. - Support for Established Training Techniques: Incorporating widely used training methodologies, including batch normalisation, to ensure compatibility with current best practices. - Advanced Distributed Optimisation Routines: Offering an array of sophisticated distributed, non-smooth optimisation techniques, such as distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - Optimized Utilisation of Distributed Hardware: Designed to leverage distributed hardware architectures with minimal user intervention, thereby facilitating both data and layer parallelism for enhanced computational efficiency.\nReferences - [1] Miguel Carreira-Perpinan, and Weiran Wang. “Distributed optimization of deeply nested systems.” PMLR, 2014. - [2] Armin Askari, et al. “Lifted neural networks.” arXiv preprint arXiv:1805.01532 (2018). - [3] Fangda Gu, Armin Askari, and Laurent El Ghaoui. “Fenchel lifted networks: A Lagrange relaxation of neural network training.” PMLR, 2020. - [4] Xiaoyu Wang, and Martin Benning. “Lifted Bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [5] Xiaoyu Wang, and Martin Benning. “A lifted Bregman formulation for the inversion of deep neural networks.” Frontiers in Applied Mathematics and Statistics 9 (2023): 1176850. - [6] Martin Benning, Tatiana A. Bubba, Luca Ratti, and Danilo Riccio. “Trust your source: quantifying source condition elements for variational regularisation methods.” arXiv preprint arXiv:2303.00696 (2023)."
  },
  {
    "objectID": "phd_projects/entries/Benning_Paralift.html#project-description",
    "href": "phd_projects/entries/Benning_Paralift.html#project-description",
    "title": "PARALIFT – A PARAllel framework for LIFTed training of neural networks",
    "section": "",
    "text": "This project is founded on innovative advancements in distributed neural network training. The cornerstone of this work, as proposed in [1], is a framework for the distributed training of deep neural networks innovatively replaces the traditional neural network consistency constraint with quadratic penalisations of that constraint. These quadratic penalisations have the advantage that they remove the need for back-propagation and pave the way for distributed optimisation algorithms that can distribute the training of individual layers across different workers. At the same time, the framework requires the introduction of auxiliary variables, which lifts the parameter search space into a higher dimensional space. This framework has subsequently been augmented by constraining the auxiliary variables to convex sets [2], and its quadratic penalties have been replaced with more bespoke penalty functions [3, 4]. In [4], the penalty functions have been replaced with tailored Bregman distance functions that, in combination with suitable optimisation algorithms, bypass the requirement of differentiating activation functions in the process. Building on this foundation, the lifted Bregman framework has been effectively applied to address the inverse problem in neural networks [5]. The objective here is to deduce the input of a pre-trained network from its output. In [6], the proposed framework has also been shown to be an effective tool for the computation of source condition elements, which are important quantities in the context of inverse problems. Furthermore, the method therein has been adapted to efficiently compute optimal sampling patterns for magnetic resonance imaging.\n\n\n\nWhile the background work has showcased the versatility of the quadratic penalisation and lifted Bregman methods in smaller-scale applications, their full potential in distributed neural network training remains largely untapped. This project aims to tap this potential by democratising access to the distributed lifted Bregman framework for the broader research community. The main objective is to develop a user-friendly software toolbox, designed to simplify the implementation of deep neural network training for large-scale datasets using the lifted Bregman framework. This toolbox will be as straightforward to use as existing toolboxes like PyTorch or JAX. A key advantage of this new toolbox is its capacity for massive parallelisation, enabling the distribution of parameter updates across various workers for individual layers, while concurrently avoiding the need for differentiating non-differentiable operations.\nThe specific main objectives of this PhD project include (but are not limited to): - Developing a Comprehensive Software Toolbox: Creating an efficient and effective toolbox for implementing the lifted Bregman framework, tailored to both ease of use and high performance. - Advancing Medical Imaging Techniques: Applying this innovative framework to medical imaging applications, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), for large-scale datasets. - Enhancing Variational Regularisations: Focusing on the training of variational regularisations with optimal source condition elements as a novel use-case. - Inverting Deep Neural Networks: Employing the lifted Bregman inversion framework for the challenging task of inverting deep neural networks, particularly in contexts involving large-scale datasets.\n\n\n\nThis project is designed to propel the use of the lifted Bregman framework from its current state with limited use cases to mass adoption. To do so, the project involves the development a Python library or Julia package (depending on the candidate’s prior programming experience and skillset) for distributed optimisation of the lifted Bregman objectives. The goal of this library or package is to make it as easy setting up complex use cases, e.g. the training of very deep transformer-based neural network architectures for large-scale datasets, as it currently is in popular automatic differentiation/deep learning frameworks such as PyTorch and JAX, but to efficiently utilise distributed training routines tailored to the lifted Bregman framework without users having to implement any of this functionality manually. In particular, the library or package will - allow users to set up a wide range of different and versatile architectures and optimisation problems. - support popular training techniques, such as batch normalisation. - feature a variety of different distributed, non-smooth optimisation routines, including distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - utilise distributed hardware architectures with little to no user input to enable both data and layer parallelism.\nThis project aims to elevate the lifted Bregman framework from its current state of limited use-cases to widespread use in the research community. Central to this objective is the development of either a Python library or a Julia package, depending on the candidate’s programming expertise and skills. This tool will facilitate distributed optimisation of lifted Bregman objectives, making the setup of complex scenarios as effortless as using established automatic differentiation and deep learning frameworks like PyTorch and JAX. However, it uniquely focuses on the efficient employment of distributed training routines specific to the lifted Bregman framework, all without requiring users having to engage with the intricate details of manual implementation. Key features of this library or package include (but are not limited to): - Versatile Architecture and Optimisation Setup: Enabling users to effortlessly configure a broad spectrum of deep neural network architectures, loss functions, constraints, and optimisation problems. - Support for Established Training Techniques: Incorporating widely used training methodologies, including batch normalisation, to ensure compatibility with current best practices. - Advanced Distributed Optimisation Routines: Offering an array of sophisticated distributed, non-smooth optimisation techniques, such as distributed proximal gradient descent, proximal incremental aggregated gradient, and proximal stochastic averaged gradient. - Optimized Utilisation of Distributed Hardware: Designed to leverage distributed hardware architectures with minimal user intervention, thereby facilitating both data and layer parallelism for enhanced computational efficiency.\nReferences - [1] Miguel Carreira-Perpinan, and Weiran Wang. “Distributed optimization of deeply nested systems.” PMLR, 2014. - [2] Armin Askari, et al. “Lifted neural networks.” arXiv preprint arXiv:1805.01532 (2018). - [3] Fangda Gu, Armin Askari, and Laurent El Ghaoui. “Fenchel lifted networks: A Lagrange relaxation of neural network training.” PMLR, 2020. - [4] Xiaoyu Wang, and Martin Benning. “Lifted Bregman training of neural networks.” Journal of Machine Learning Research 24, no. 232 (2023): 1-51. - [5] Xiaoyu Wang, and Martin Benning. “A lifted Bregman formulation for the inversion of deep neural networks.” Frontiers in Applied Mathematics and Statistics 9 (2023): 1176850. - [6] Martin Benning, Tatiana A. Bubba, Luca Ratti, and Danilo Riccio. “Trust your source: quantifying source condition elements for variational regularisation methods.” arXiv preprint arXiv:2303.00696 (2023)."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Leadership Team",
    "section": "",
    "text": "Prof Timo Betcke is Centre Director. He is Professor of Computational Mathematics at UCL.\n                Homepage\n                Orcid\n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Colin Cotter is Co-Lead for the CDT. He is Professor of Computational Mathematics at Imperial College.\n                Homepage\n                Orcid\n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Marta Betcke is Co-Director for Research and Partnership. She is Professor of Scientific Computing at UCL.\n                Homepage\n                Orcid\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Ruth Misener is Co-Director for Research and Partnership. She is Professor in Computational Optimisation at Imperial College.\n                Homepage\n                Orcid\n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Serge Guillas is Co-Director for Training. He is the Met Office Joint Chair in Data Sciences for Weather and Climate at UCL.\n                Homepage\n                Orcid\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Dr Dante Kalise is Co-Director for Training. He is a Reader in Computational Optimisation and Control at Imperial College.\n                Homepage\n                Orcid\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Prof Hao Ni is Co-Director for Admissions. She is Professor of Mathematics at UCL and a Turing Fellow at the Alan Turing Institute.\n                Homepage\n                Orcid\n                \n                GitHub\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Dr Vahid Shahrezaei is Co-Director for Admissions. He is a Reader in Biomathematics at Imperial College.\n                Homepage\n                Orcid\n                \n            \n        \n    \n\n\n\n    \n        \n        \n        \n        \n            \n                Laura Beer is Centre Manager for the CCMI CDT. She is a Professional Research Investment & Strategy Manager at UCL ARC.\n                Homepage\n                Orcid\n                \n            \n        \n    \n\n\n\nNo matching items"
  }
]